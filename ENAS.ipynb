{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "\n",
    "    epoch_num = 100\n",
    "    retrain_epoch_num = 1\n",
    "    platform = \"cpu\"\n",
    "    # -----------\n",
    "    # child \n",
    "    # -----------\n",
    "    \n",
    "    # model\n",
    "    class_num = 10\n",
    "    child_num_layers = 6\n",
    "    child_out_channels = 32\n",
    "    child_num_op = 4\n",
    "    # training\n",
    "    child_data_path = './data/cifar-10-batches-py/'\n",
    "    child_num_valids = 5000\n",
    "    child_batch_size = 64\n",
    "    # cosine learning \n",
    "    child_lr_init = 0.05\n",
    "    child_lr_gamma = 0.1\n",
    "    child_lr_cos_lmin = 0.001\n",
    "    child_lr_cos_Tmax = 2\n",
    "    # weight decay \n",
    "    child_l2_reg = 1e-4\n",
    "\n",
    "    child_run_loss_every = 100\n",
    "    # validate shared parameters\n",
    "    child_valid_every_epochs = 1\n",
    "    \n",
    "\n",
    "    # -----------\n",
    "    # controller \n",
    "    # -----------\n",
    "    # model\n",
    "    ctrl_lstm_size = 64\n",
    "    ctrl_lstm_num_layers = 2\n",
    "    # child\n",
    "    # --- training\n",
    "    ctrl_train_step_num = 10 # number of training steps per epoch\n",
    "    ctrl_batch_size = 5 # number of samples per training step\n",
    "    ctrl_train_every_epochs = 2\n",
    "    # learning scheduler = exponential decaying\n",
    "    ctrl_lr_init = 0.0001\n",
    "    ctrl_lr_gamma = 0.1\n",
    "    # baseline - reduce high variance; exponential moving average\n",
    "    ctrl_baseline_decay = 0.999\n",
    "    # prevent from being permature of controller\n",
    "    # applied to logits\n",
    "    ctrl_temperature = 5\n",
    "    ctrl_tanh_constant = 2.5\n",
    "    # add entropy to reward\n",
    "    ctrl_entropy_weight = 0.0001\n",
    "    # enforce skip sparsity \n",
    "    # add skip penalty to loss\n",
    "    ctrl_skip_target = 0.4\n",
    "    ctrl_skip_weight = 0.8\n",
    "    # validate/test controller \n",
    "    ctrl_valid_every_epochs = 1\n",
    "    ctrl_eval_arc_num = 2\n",
    "    ctrl_final_arc_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import torch\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"\n",
    "    Read a batch\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def _read_data(file_list):\n",
    "    \"\"\"\n",
    "    Read a dataset and reshape it\n",
    "    args: \n",
    "        file_list: path of dataset\n",
    "    returns:\n",
    "        data_set: all data\n",
    "            format NCHW: x, 3072 -> x, 3, 32, 32\n",
    "        label_set: all labels of the data\n",
    "    \"\"\"\n",
    "    for i in range(len(file_list)):\n",
    "        file = file_list[i]\n",
    "        if DEBUG: print(file)\n",
    "        data_batch = unpickle(file)\n",
    "        data = data_batch[b'data']\n",
    "        data = data.astype('float32')   # change to \n",
    "        data = data.reshape((-1, 3, 32, 32))\n",
    "        labels = np.array(data_batch[b'labels']).astype('int')\n",
    "        if i == 0:\n",
    "            data_set = data\n",
    "            label_set = labels\n",
    "        else:\n",
    "            data_set = np.concatenate((data_set, data), axis = 0)\n",
    "            label_set = np.concatenate((label_set, labels), axis = 0)\n",
    "        # if DEBUG:\n",
    "        #     print(data.shape)\n",
    "        #     print(labels.shape)\n",
    "\n",
    "\n",
    "    return (data_set, label_set)\n",
    "\n",
    "def read_data(data_path, num_valids=5000):\n",
    "    \"\"\"\n",
    "    Read train/valid/test data sets\n",
    "    args: \n",
    "        num_valids: num of images in a valid set\n",
    "    returns:\n",
    "        images: N, C, H, W\n",
    "            in case of CIFAR-10, it consists of \n",
    "            \n",
    "            train: \n",
    "                no valid set: 50k\n",
    "                with valid set: 45k\n",
    "            test:\n",
    "                10k\n",
    "            valid\n",
    "                last 5k of training set\n",
    "        label_set: all labels of the datasets\n",
    "    \"\"\"\n",
    "    # create the file list\n",
    "    # data_path = 'F:/2 Work/0 Solo/data/cifar-10-python/'\n",
    "    train_files = []\n",
    "    for i in range(5):\n",
    "        # train_files.append('../data/cifar-10-python/data_batch_' + str(i+1))\n",
    "        train_files.append(data_path + 'data_batch_' + str(i+1))\n",
    "        # path:\n",
    "            # F:\\2 Work\\0 Solo invented by InvisibleForce\\pytorch\\data\\cifar-10-batches-py\n",
    "    test_files = []\n",
    "    test_files.append(data_path + 'test_batch')\n",
    "\n",
    "    # create image and label dict\n",
    "    images, labels = {}, {}\n",
    "    # read train set\n",
    "    images['train'], labels['train'] = _read_data(train_files)\n",
    "    # read valid set\n",
    "    if num_valids: # need\n",
    "        # valid set\n",
    "        images['valid'] = images['train'][-num_valids:] # last num_valids images\n",
    "        labels['valid'] = labels['train'][-num_valids:]\n",
    "        # train set = the remaining orginal train set\n",
    "        images['train'] = images['train'][:-num_valids] # last num_valids images\n",
    "        labels['train'] = labels['train'][:-num_valids]\n",
    "\n",
    "    # read test set\n",
    "    images['test'], labels['test'] = _read_data(test_files)\n",
    "\n",
    "\n",
    "    # normalize data\n",
    "    # 1. sub mean\n",
    "    # 2. divide std\n",
    "    # proc train set\n",
    "    images['train'] = normalize(images['train'])\n",
    "    # convert data to tensor\n",
    "    images['train'] = torch.from_numpy(images['train'])\n",
    "    # proc valid set\n",
    "    if num_valids:\n",
    "        images['valid'] = normalize(images['valid'])\n",
    "        images['valid'] = torch.from_numpy(images['valid'])\n",
    "    # proc test set\n",
    "    images['test'] = normalize(images['test'])\n",
    "    images['test'] = torch.from_numpy(images['test'])\n",
    "    # convert labels from np array to torch.tensor.long\n",
    "    labels['train'] = torch.from_numpy(labels['train'])\n",
    "    labels['train'] = labels['train'].long()\n",
    "    if num_valids:\n",
    "        labels['valid'] = torch.from_numpy(labels['valid'])\n",
    "        labels['valid'] = labels['valid'].long()\n",
    "    labels['test'] = torch.from_numpy(labels['test'])\n",
    "    labels['test'] = labels['test'].long()\n",
    "    return images, labels\n",
    "\n",
    "def normalize(dataset):\n",
    "    \"\"\"\n",
    "    1. sub mean\n",
    "    2. divide std\n",
    "    arg:\n",
    "        img: dataset, (N, C, H, W)\n",
    "    return: \n",
    "        img: normalized dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = np.transpose(dataset, [0, 2, 3, 1]) # NCHW -> NHWC\n",
    "    dataset = dataset / 255.0 # 0-255 -> 0-1\n",
    "    mean = np.mean(dataset, axis=(0, 1, 2), keepdims=True)\n",
    "    std = np.std(dataset, axis=(0, 1, 2), keepdims=True)\n",
    "    dataset = (dataset - mean) / std \n",
    "    dataset = np.transpose(dataset, [0, 3, 1, 2]) # NHWC -> NCHW\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def augment(batch):\n",
    "    \"\"\"\n",
    "    Processed on GPU\n",
    "    1 upsample: 32x32 -> 40x40\n",
    "    2 randomly crop: 40x40 -> 32x32\n",
    "    3 flip horizontally: left -> right\n",
    "    arg:\n",
    "        img: a batch of images, (N, C, H, W)\n",
    "    return: \n",
    "        img: augmented batch\n",
    "    \"\"\"\n",
    "    # convert batch from tensor to nparray\n",
    "    # batch = batch.data.numpy() # only supported by cpu devices\n",
    "    if DEBUG: print('augment', type(batch))\n",
    "    # parameters\n",
    "    N, C, H, W = batch.size()\n",
    "    # augment\n",
    "    # for i in range(N):\n",
    "    # img = batch[i, :, :, :]\n",
    "    # 1 upsample: 32x32 -> 40x40 (H, W)\n",
    "    batch = pad(batch, [[4, 4], [4, 4]])\n",
    "    # 2 randomly crop: 40x40 -> 32x32\n",
    "    batch = crop(batch, [H, W])\n",
    "    # 3 flip horizontally: left -> right\n",
    "    batch = flip_left_right(batch)\n",
    "    # store it back to batch\n",
    "    # batch[i, :, :, :] = img_flip\n",
    "\n",
    "    # batch = torch.from_numpy(batch) # only for cpu\n",
    "    if DEBUG: print('augment', type(batch))\n",
    "    return batch \n",
    "\n",
    "def pad(batch, pad_size):\n",
    "    \"\"\"\n",
    "    pad zeros to an img\n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "        pad_size: [[C_before, C_after], [H_top, H_bottom], [W_left, W_right]]\n",
    "    return: \n",
    "        img: a zero-padded img\n",
    "    \"\"\"\n",
    "    # params\n",
    "    N, C, H, W = batch.size()\n",
    "    H_zeros, W_zeros = pad_size\n",
    "    # zero padding\n",
    "    H_zp = H + H_zeros[0] + H_zeros[1]\n",
    "    W_zp = W + W_zeros[0] + W_zeros[1]\n",
    "    batch_zp = torch.zeros((N, C, H_zp, W_zp)).cuda()\n",
    "    # batch_zp = batch_zp\n",
    "    batch_zp[:, :, H_zeros[0] : H_zeros[0] + H, W_zeros[0] : W_zeros[0] + W] = batch\n",
    "    \n",
    "    return batch_zp\n",
    "\n",
    "def crop(batch, crop_size):\n",
    "    \"\"\"\n",
    "    randomly crop img to a smaller one\n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "        crop_size: [C_crop, H_crop, W_crop]\n",
    "    return: \n",
    "        img_crop: a cropped img\n",
    "    \"\"\"\n",
    "    # parameter\n",
    "    N, C, H, W = batch.size()\n",
    "    H_crop, W_crop = crop_size\n",
    "    H_diff = H - H_crop + 1\n",
    "    W_diff = W - W_crop + 1\n",
    "    # randomly sample the crop offset\n",
    "    H_offset = int(np.random.randint(0, H_diff, size=1))\n",
    "    W_offset = int(np.random.randint(0, W_diff, size=1))\n",
    "    # crop the img\n",
    "    batch_crop = batch[:, :, H_offset : H_offset + H_crop, W_offset : W_offset + W_crop]\n",
    "\n",
    "    return batch_crop\n",
    "\n",
    "def flip_left_right(batch):\n",
    "    \"\"\"\n",
    "    flip img from left to right. \n",
    "    CHW data format needs to flip axis=2 (i.e., W-axis) \n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "    return: \n",
    "        img_flip: a left-right-flipped img \n",
    "    \"\"\"\n",
    "    # img_flip = np.flip(img, axis=2)\n",
    "    # batch = torch.flip(batch, )\n",
    "    batch = batch.flip(3) # flip along w-axis\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def test_data_proc():\n",
    "    images, labels = read_data()\n",
    "    batch = images['train'][0:3]\n",
    "    print(batch[0, 0, 5, :])\n",
    "    print(torch.sum(batch))\n",
    "    batch_aug = augment(batch)\n",
    "    print(batch_aug[0, 0, 5, :])\n",
    "    print(torch.sum(batch_aug))\n",
    "    # diff = batch - batch_aug\n",
    "    # print(batch)\n",
    "    # print(batch_aug)\n",
    "    # print(diff)\n",
    "    print(batch.shape)\n",
    "    print(batch_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# installed\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "DEBUG = 0\n",
    "\n",
    "\n",
    "def global_avgpool(x):\n",
    "    \"\"\"\n",
    "    An operation used to reduce the H and W axis\n",
    "    x = [N, C, H, W] -> [N, C, 1, 1]\n",
    "    \"\"\"\n",
    "    H = x.size()[2]\n",
    "    W = x.size()[3]\n",
    "    x = torch.sum(x, dim=[2, 3])\n",
    "    x = x / (H * W)\n",
    "\n",
    "    return x\n",
    "\n",
    "# TODO: rename LayserOp as NodeOp\n",
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    An operation used by a nas layer\n",
    "    Args:\n",
    "        op: conv3, conv5, avgpool3, maxpool3\n",
    "        out_channels: = M, num of filters\n",
    "    \"\"\"\n",
    "    def __init__(self, op, out_channels):\n",
    "        super(Operation, self).__init__() \n",
    "        self.op = op \n",
    "        self.out_channels = out_channels\n",
    "        self.layers = self._build_layer()\n",
    "    \n",
    "    def _build_layer(self):\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # kernel\n",
    "        if self.op == 'conv3':\n",
    "            kernel = nn.Conv2d(\n",
    "            in_channels=self.out_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size = 3, \n",
    "            padding=(1,1), # (3-1)/2\n",
    "            stride=1)\n",
    "        elif self.op == 'conv5':\n",
    "            kernel = nn.Conv2d(\n",
    "                in_channels=self.out_channels, \n",
    "                out_channels=self.out_channels, \n",
    "                kernel_size = 5, \n",
    "                padding=(2,2), # (5-1)/2\n",
    "                stride=1)\n",
    "        elif self.op == 'avgpool3':\n",
    "            kernel = nn.AvgPool2d(\n",
    "                kernel_size=3, \n",
    "                padding=(1,1),\n",
    "                stride=1)\n",
    "        elif self.op == 'maxpool3':\n",
    "            kernel = nn.MaxPool2d(\n",
    "                kernel_size=3, \n",
    "                padding=(1,1),\n",
    "                stride=1)\n",
    "        layers.append(kernel)\n",
    "        # bn_out\n",
    "        if (self.op == 'conv3') or (self.op == 'conv5'):\n",
    "            layers.append(nn.ReLU(inplace = True))\n",
    "\n",
    "        layers = nn.Sequential(*layers)\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        return self.layers(x)\n",
    "        \n",
    "\n",
    "class Node(nn.Module):\n",
    "    def __init__(self, out_channels=24):\n",
    "        super(Node, self).__init__()\n",
    "        \"\"\"\n",
    "        Create a nas layer\n",
    "        \"\"\"\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.layers = self._build_nas_layer()\n",
    "        \n",
    "        self.bn_out = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        \n",
    "\n",
    "    def _build_nas_layer(self):\n",
    "        \"\"\"\n",
    "        build a nas layer consisting all possible branches\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        # conv3, 0\n",
    "        conv3 = Operation('conv3', self.out_channels)\n",
    "        layers.append(conv3)\n",
    "        # conv5, 1\n",
    "        conv5 = Operation('conv5', self.out_channels)\n",
    "        layers.append(conv5)\n",
    "        # avgpool3, 2\n",
    "        avgpool3 = Operation('avgpool3', self.out_channels)\n",
    "        layers.append(avgpool3)\n",
    "        # maxpool3, 3\n",
    "        maxpool3 = Operation('maxpool3', self.out_channels)\n",
    "        layers.append(maxpool3)\n",
    "        # create a module list\n",
    "        layers = nn.ModuleList(layers)\n",
    "    \n",
    "        return layers\n",
    "\n",
    "    def layer_op(self, x, op):\n",
    "        \"\"\"\n",
    "        Run the operation of a nas layer\n",
    "        Args:\n",
    "            x: in_map\n",
    "            op: operation to run\n",
    "                0 - conv3\n",
    "                1 - conv5\n",
    "                2 - avgpool3\n",
    "                3 - maxpool3\n",
    "        Returns:\n",
    "            x: out_map\n",
    "        \"\"\"\n",
    "        x = self.layers[op](x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def skip(self, prev_layers, connections):\n",
    "        \"\"\"\n",
    "        Concate the desired preve layers of a nas layer\n",
    "        Args:\n",
    "            prev_layers: previous layers\n",
    "            config: describe all the combined layers\n",
    "        Returns:\n",
    "            y: ofmap\n",
    "        \"\"\"\n",
    "        # add all the desired prev layers together\n",
    "        offset = 1\n",
    "        num_layer = len(prev_layers)-offset\n",
    "        x = []\n",
    "        for i in range(num_layer):\n",
    "            if connections[i]:\n",
    "                x.append(prev_layers[i+offset])\n",
    "        if len(x):\n",
    "            x = torch.stack(x) # stack all the tensors in an additional axis (i.e., 0)\n",
    "            x = torch.sum(x, dim=0) # add along axis 0\n",
    "        else:\n",
    "            x = torch.zeros(prev_layers[0].size()) .cuda()\n",
    "        return x\n",
    "\n",
    "\n",
    "    def __call__(self, cnt_layer, prev_layers, op, connections):\n",
    "        \"\"\"\n",
    "        describe the forward of the layer\n",
    "        Args:\n",
    "            prev_layers: all previous layers\n",
    "            layer_config: op and connectivity\n",
    "        \"\"\"\n",
    "        x = prev_layers[-1]\n",
    "        # run op of the enas layer\n",
    "        x = self.layer_op(x, op)\n",
    "        if cnt_layer > 0:\n",
    "            # combine the skip (add skips with x)\n",
    "            y = self.skip(prev_layers, connections)\n",
    "            # if DEBUG: print('skip_out\\n', y.data)\n",
    "            # combine op and skip results\n",
    "            # gpu not supporting x + y\n",
    "            x = torch.stack([x, y])\n",
    "            x = torch.sum(x, dim=0)\n",
    "            x = self.bn_out(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(ChildModel, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # data set used for training, validating, testing\n",
    "        self.class_num = class_num \n",
    "        # parameters for building a child model\n",
    "        self.num_layers = num_layers # \n",
    "        self.out_channels = out_channels\n",
    "        # build DAG = net\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.graph = self._build_graph(self.class_num)\n",
    "        # fc for final classification\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(self.out_channels, class_num, bias=True) \n",
    "        \n",
    "\n",
    "    def _build_graph(self, class_num):\n",
    "        graph = []\n",
    "        \n",
    "        # major part of the graph consisting of all NasLayers\n",
    "        for _ in range(self.num_layers):\n",
    "            graph.append(Node(self.out_channels))\n",
    "        # create a ModuleList, or the parameters cannot be added\n",
    "        graph = nn.ModuleList(graph)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    # TODO: no reduction !\n",
    "    def model(self, x, ops,skips):\n",
    "        \"\"\"\n",
    "        run (like forward) a child model determined by sample_arch\n",
    "        Args:\n",
    "            sample_arch: a list consisting of 2 * num_layers elements\n",
    "                op_id = sample_arch[2k]: operation id\n",
    "                skip = sample_arch[2k + 1]: element i of such binary vector \n",
    "                    is used to describe whether the previous layer i is used \n",
    "                    as an input\n",
    "            x: input of the child model\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        # layers\n",
    "        prev_layers = []\n",
    "        # stem_conv\n",
    "        x = self.head(x)\n",
    "        prev_layers.append(x)\n",
    "        # nas_layers\n",
    "        for cnt_layer in range(self.num_layers):\n",
    "            x = self.graph[cnt_layer]( cnt_layer,prev_layers, ops[cnt_layer],skips[cnt_layer])\n",
    "            prev_layers.append(x)\n",
    "        # global_avgpool\n",
    "        x = self.tail(x)\n",
    "        x = global_avgpool(x)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Child(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32,\n",
    "               device='cpu', \n",
    "               lr_init=0.05,\n",
    "               lr_gamma=0.1,\n",
    "               lr_cos_lmin=0.001,\n",
    "               lr_cos_Tmax=2,\n",
    "               l2_reg=1e-4,\n",
    "               run_loss_every=100\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(Child, self).__init__() \n",
    "        self.class_num = class_num # number of classes\n",
    "        self.num_layers = num_layers # \n",
    "        self.out_channels = out_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.run_loss_every = run_loss_every\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.lr_cos_lmin = lr_cos_lmin\n",
    "        self.lr_cos_Tmax = lr_cos_Tmax\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.net = ChildModel(class_num, num_layers, out_channels)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.optimizer = optim.SGD([{'params': self.net.parameters(), 'initial_lr': self.lr_init}], lr=self.lr_init, weight_decay=self.l2_reg, momentum=0.9, nesterov=True)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.lr_cos_Tmax, eta_min=self.lr_cos_lmin)\n",
    "    \n",
    "    def get_batch(self, images, labels, step):\n",
    "        batch_size = self.batch_size\n",
    "        batch_images = images[step * batch_size : (step + 1) * batch_size] \n",
    "        batch_labels = labels[step * batch_size : (step + 1) * batch_size] \n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def train_epoch(self, ops, skips, images, labels, epoch, train_step):    \n",
    "        running_loss = 0.0\n",
    "        print('lr=', self.scheduler.get_last_lr())\n",
    "        for step in range(train_step): \n",
    "            batch_inputs, batch_labels = self.get_batch(images, labels, step)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net.model(batch_inputs, ops,skips)\n",
    "            loss = self.criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # update running loss\n",
    "            running_loss += loss.item()\n",
    "            if step % self.run_loss_every == (self.run_loss_every - 1):\n",
    "                print('Epoch %d, Iter %d /%d, loss: %.3f' %\n",
    "                    (epoch + 1, step + 1,train_step, running_loss / self.run_loss_every))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        self.scheduler.step()\n",
    "\n",
    "    def eval_mini(self, ops, skips, images, labels):    \n",
    "        # validating\n",
    "        # get a minibatch for cpu or gpu\n",
    "        high = labels.size()[0] // self.batch_size\n",
    "        \n",
    "        batch_idx = torch.randint(high, (1,1))\n",
    "        # batch_idx = random.randint(0, high)\n",
    "        batch_inputs, batch_labels = self.get_batch(images, labels, batch_idx)\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net.model(batch_inputs, ops,skips)\n",
    "        \n",
    "        # cal accuracy\n",
    "        _, idx = torch.topk(outputs, 1)\n",
    "        idx = idx.reshape((-1))\n",
    "        accuracy = (idx == batch_labels).float().sum()\n",
    "        accuracy /= self.batch_size\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def eval(self, sample_arch, images, labels):\n",
    "        step_num = labels.size()[0] // self.batch_size\n",
    "        total_accuracy = 0\n",
    "        for i in range(step_num):\n",
    "            batch_inputs, batch_labels = self.get_batch(images, labels, i)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.net.model(batch_inputs, sample_arch)\n",
    "            _, idx = torch.topk(outputs, 1)\n",
    "            idx = idx.reshape((-1))\n",
    "            total_accuracy += (idx == batch_labels).float().sum() # count the correct prediction\n",
    "        total_accuracy /= (step_num * self.batch_size)\n",
    "        \n",
    "        return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class StackLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    StackLSTM class.\n",
    "    It describes a stacked LSTM which only \n",
    "    run a single step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, lstm_num_layers=2):\n",
    "        super(StackLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.net = self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        \n",
    "        net = []\n",
    "        for _ in range(self.lstm_num_layers):\n",
    "            layer = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            net.append(layer)\n",
    "        net = nn.ModuleList(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "    def __call__(self, inputs, prev_h, prev_c):\n",
    "        \"\"\"\n",
    "        Forward of stacked LSTM\n",
    "        Args:\n",
    "            inputs: input of the stack lstm, [batch=1, input_size]\n",
    "            prev_h & prev_c: hidden and cell states of each layer at the previous time step.\n",
    "                size = [lstm_num_layer, hidden_size]\n",
    "        Returns:\n",
    "            next_h & next_c\n",
    "        \"\"\"\n",
    "        net = self.net\n",
    "        next_h, next_c = [], []\n",
    "        for i in range(self.lstm_num_layers):\n",
    "            if i == 0: x = inputs\n",
    "            else: x = next_h[-1]\n",
    "            cur_h, cur_c = net[i](x, (prev_h[i], prev_c[i]))\n",
    "            next_h.append(cur_h)\n",
    "            next_c.append(cur_c)\n",
    "        \n",
    "        return next_h, next_c\n",
    "\n",
    "class ControllerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "               child_num_layers=6,\n",
    "               lstm_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               num_op=4,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               skip_target=0.4,\n",
    "               device='gpu'\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(ControllerModel, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # parameters for building a child model\n",
    "        self.child_num_layers = child_num_layers # \n",
    "        # parameters for building a controller\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_op = num_op\n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        # build controller = net\n",
    "        # claim all the layers and parameters\n",
    "        self.net = self._build_net()\n",
    "        # add g_emb as a parameter to ControllerModel\n",
    "        # initialized by uniform distribution between -0.1 to 0.1\n",
    "        # 0 <= torch.rand < 1\n",
    "        g_emb_init = 0.2 * torch.rand(1,self.lstm_size) - 0.1\n",
    "        self.register_parameter(name='g_emb', param=torch.nn.Parameter(g_emb_init))\n",
    "        # results of net sample\n",
    "        self.sample_arch = []\n",
    "        self.sample_entropy = []\n",
    "        self.sample_log_prob = []\n",
    "        self.sample_skip_count = []\n",
    "        self.sample_skip_penaltys = []\n",
    "        \n",
    "\n",
    "    def _build_net(self):\n",
    "\n",
    "        net = {}\n",
    "        # layers & params shared by op and skip\n",
    "        net['lstm'] = StackLSTM(self.lstm_size, self.lstm_size, self.lstm_num_layers)\n",
    "        # layers & params used only by op\n",
    "        net['op_fc'] = nn.Linear(self.lstm_size, self.num_op)\n",
    "        net['op_emb_lookup'] = nn.Embedding(self.num_op, self.lstm_size)\n",
    "        # layers & params used only by skip\n",
    "        net['skip_attn1'] = nn.Linear(self.lstm_size, self.lstm_size) # w_attn1 \n",
    "        net['skip_attn2'] = nn.Linear(self.lstm_size, self.lstm_size) # w_attn2)\n",
    "        net['skip_attn3'] = nn.Linear(self.lstm_size, 1)              # v_attn\n",
    "        net = nn.ModuleDict(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def _op_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample an op (it is a part of controller's forward)\n",
    "        Args: consisting of the following parts\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        net = self.net\n",
    "        inputs, prev_h, prev_c, arc_seq, log_probs, entropys = args\n",
    "        # lstm - process hidden states\n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "        # fc - calculate logit\n",
    "        logit = net['op_fc'](next_h[-1])    # h state of the last layer\n",
    "        # temperature\n",
    "        if self.temperature is not None:\n",
    "            logit /= self.temperature\n",
    "        # tanh and then scaled by a constant\n",
    "        if self.tanh_constant is not None:\n",
    "            logit = self.tanh_constant * torch.tanh(logit)\n",
    "        # use softmax transfer logits to probs\n",
    "        # or the logits may be negative it can not represent a prob\n",
    "        prob = f.softmax(logit, dim=1)\n",
    "        # multinomial for sampling an op\n",
    "        op_id = torch.multinomial(prob, 1) # logit = probs of each type of operation, 1 = sample a single op\n",
    "        op_id = op_id[0]\n",
    "        # generate input for skip_sample using embedding lookup\n",
    "        inputs = net['op_emb_lookup'](op_id.long())\n",
    "        # calculate log_prob\n",
    "        log_prob = f.cross_entropy(logit, op_id)\n",
    "        # calculate entropy\n",
    "        entropy = log_prob * torch.exp(-log_prob)\n",
    "        # add op to arc_seq\n",
    "        op = int(op_id.cpu().data.numpy()) # to an int\n",
    "        arc_seq.append(op)\n",
    "        # add to log_probs\n",
    "        log_probs.append(log_prob)\n",
    "        # add to entropys\n",
    "        entropys.append(entropy)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys        \n",
    "\n",
    "    def _skip_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample skip connections for layer_id (it is a part of controller's forward)\n",
    "        Args:\n",
    "            layer_id\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "            archors & anchors_w_1: archor points and its weighed values\n",
    "            skip_targets & skip_penaltys & skip_count: used to enforce the sparsity of skip connections\n",
    "        Return:\n",
    "            all args except layer_id\n",
    "        \"\"\"    \n",
    "        layer_id, inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = args\n",
    "        net = self.net\n",
    "        # lstm - process hidden states\n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "        if layer_id > 0:\n",
    "            # use attention mechanism to generate logits\n",
    "            # concate the weighed anchors\n",
    "            query = torch.cat(anchors_w_1, dim=0) \n",
    "            # attention 2 - fc\n",
    "            query = torch.tanh(net['skip_attn2'](next_h[-1]) + query)\n",
    "            # attention 3 - fc            \n",
    "            query = net['skip_attn3'](query)\n",
    "            # generate logit\n",
    "            logit = torch.cat([-query, query], dim=1)\n",
    "            # process logit with temperature\n",
    "            if self.temperature is not None:\n",
    "                logit /= self.temperature\n",
    "            # process logit with tanh and scale it\n",
    "            if self.temperature is not None:\n",
    "                logit = self.tanh_constant * torch.tanh(logit)\n",
    "            # calculate prob of skip (see NAS paper, Sec3.3)\n",
    "            skip_prob = torch.sigmoid(logit) # use sigmoid to convert skip to its prob\n",
    "            # sample skip connections using multinomial distribution sampler\n",
    "            skip = torch.multinomial(skip_prob, 1)  # 0 - used as an input, 1 - not an input\n",
    "            # calcualte kl as skip penalty\n",
    "            kl = skip_prob * torch.log(skip_prob / skip_targets) # calculate kl\n",
    "            kl = torch.sum(kl)\n",
    "            skip_penaltys.append(kl)\n",
    "            # cal log_prob and append it - used by REINFORCE to calculate gradients of controller (i.e., LSTM)\n",
    "            log_prob = f.cross_entropy(logit, skip.squeeze(dim=1))\n",
    "            log_probs.append(torch.sum(log_prob))\n",
    "            # cal entropys and append it\n",
    "            entropy = log_prob * torch.exp(-log_prob)\n",
    "            entropy = torch.sum(entropy)\n",
    "            entropys.append(entropy)\n",
    "            # update count of skips\n",
    "            skip_count.append(skip.sum())\n",
    "            arc_seq.append(skip.cpu().squeeze(dim=1).data.numpy().tolist())\n",
    "            # generate inputs for the next time step\n",
    "            skip = torch.reshape(skip, (1, layer_id)) # reshape skip\n",
    "            cat_anchors = torch.cat(anchors, dim=0)\n",
    "            # skip = 1 x layer_id (layer_id > 0) \n",
    "            # cat_anchors = layer_id x lstm_size\n",
    "            inputs = torch.matmul(skip.float(), cat_anchors) \n",
    "            inputs /= (1.0 + torch.sum(skip))\n",
    "        else:\n",
    "            inputs = self.g_emb\n",
    "            arc_seq.append([]) # no skip, use empty list to occupy the position\n",
    "        \n",
    "        # cal the\n",
    "        anchors.append(next_h[-1])\n",
    "        # cal attention 1\n",
    "        attn1 = net['skip_attn1'](next_h[-1])\n",
    "        anchors_w_1.append(attn1)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count\n",
    "\n",
    "    def net_sample(self):\n",
    "        \"\"\"\n",
    "        run (like forward) a controller model to sample an neural architecture\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        # net sample\n",
    "        ops = []\n",
    "        skips = []\n",
    "        \n",
    "        entropys = []\n",
    "        log_probs = []\n",
    "        # skip sample \n",
    "        anchors = []        # store hidden states of skip lstm; anchor = hidden states of skip lstm (i.e., layer_id)\n",
    "        anchors_w_1 = []    # store results of attention 1 (input=h, w_attn1)\n",
    "        skip_count = []\n",
    "        skip_penaltys = []\n",
    "\n",
    "\n",
    "        # init inputs and states\n",
    "        # init prev cell states to zeros for each layer of the lstm\n",
    "        prev_c = [torch.zeros((1, self.lstm_size)).cuda() for _ in range(self.lstm_num_layers)]\n",
    "        # init prev hidden states to zeros for each layer of the lstm\n",
    "        prev_h = [torch.zeros((1, self.lstm_size)).cuda() for _ in range(self.lstm_num_layers)]\n",
    "        # inputs\n",
    "        inputs = self.g_emb.cuda()\n",
    "        # skip_target = 0.4 = the prob of a layer used as an input of another layer\n",
    "        # 1 - skip_target = 0.6; the probability that this layer is not used as an input\n",
    "        skip_targets = torch.tensor([1.0 - self.skip_target, self.skip_target], dtype=torch.float).cuda()\n",
    "        \n",
    "\n",
    "        # sample an arch\n",
    "        for layer_id in range(self.child_num_layers):\n",
    "            arg_op_sample = [inputs, prev_h, prev_c, ops, log_probs, entropys]\n",
    "            returns_op_sample = self._op_sample(arg_op_sample)\n",
    "            inputs, prev_h, prev_c, ops, log_probs, entropys = returns_op_sample\n",
    "            arg_skip_sample = [layer_id, inputs, prev_h, prev_c, skips, log_probs, entropys, \n",
    "                                anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count]\n",
    "            returns_skip_sample = self._skip_sample(arg_skip_sample)\n",
    "            inputs, prev_h, prev_c, skips, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = returns_skip_sample\n",
    "\n",
    "        # generate sample arch\n",
    "        # [[op], [skip]] * num_layer\n",
    "        self.ops = ops\n",
    "        self.skips = skips\n",
    "        \n",
    "        # cal sample entropy\n",
    "        entropys = torch.stack(entropys)\n",
    "        self.sample_entropy = torch.sum(entropys)\n",
    "   \n",
    "        # cal sample log_probs\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        self.sample_log_prob = torch.sum(log_probs)\n",
    "   \n",
    "            \n",
    "        # cal skip count\n",
    "        skip_count = torch.stack(skip_count)\n",
    "        self.sample_skip_count = torch.sum(skip_count)\n",
    "     \n",
    "        # cal skip penaltys\n",
    "        skip_penaltys = torch.stack(skip_penaltys)\n",
    "        self.sample_skip_penaltys = torch.sum(skip_penaltys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Controller class.\n",
    "    It describes how to train a controller\n",
    "        1) train\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               device='gpu',\n",
    "               lstm_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               child_num_layers=6,\n",
    "               num_op=4,\n",
    "               train_step_num=50,\n",
    "               ctrl_batch_size=20,\n",
    "               lr_init=0.00035,\n",
    "               lr_gamma=0.1,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               entropy_weight=0.0001,\n",
    "               baseline_decay=0.999,\n",
    "               skip_target=0.4,\n",
    "               skip_weight=0.8\n",
    "              ):\n",
    "        super(Controller, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # config of controller model\n",
    "        # child model\n",
    "        self.child_num_layers = child_num_layers # imgs of dataset\n",
    "        # ctrl model\n",
    "        self.lstm_size = lstm_size # labels of dataset \n",
    "        self.lstm_num_layers = lstm_num_layers # number of classes\n",
    "        self.num_op = num_op # \n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        # ctrl training\n",
    "        self.ctrl_batch_size=ctrl_batch_size\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.train_step_num = train_step_num\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.baseline_decay = baseline_decay\n",
    "        self.skip_weight = skip_weight\n",
    "        # device\n",
    "        self.device = device\n",
    "        # # training parameters on gpu\n",
    "        self.reward = torch.zeros(1).cuda() # rewards of samples\n",
    "        self.baseline = torch.zeros(1).cuda() # base line\n",
    "        self.log_prob = torch.zeros(1).cuda() # log_probs of samples\n",
    "        self.entropy = torch.zeros(1).cuda() # entropys of samples\n",
    "        # self.skip_rate = torch.zeros(1) # skip_rates of samples\n",
    "        self.skip_penalty = torch.zeros(1).cuda() # skip_penaltys of samples\n",
    "        self.loss = torch.zeros(1).cuda() # loss\n",
    "\n",
    "\n",
    "        # build controller\n",
    "        self.ctrl = ControllerModel(child_num_layers=child_num_layers,\n",
    "               lstm_size=lstm_size,\n",
    "               lstm_num_layers=lstm_num_layers,\n",
    "               num_op=num_op,\n",
    "               temperature=temperature,\n",
    "               tanh_constant=tanh_constant,\n",
    "               skip_target=skip_target,\n",
    "               device=device)\n",
    "        self.optimizer = optim.Adam(self.ctrl.parameters(), lr=self.lr_init)\n",
    "        \n",
    "      \n",
    "    \n",
    "    def train_epoch(self, child_model, images, labels):    \n",
    "\n",
    "            \n",
    "\n",
    "        for step in range(self.train_step_num):\n",
    "            # a single step of training\n",
    "            # sample a batch of child archs and obtain their metrics\n",
    "            loss = torch.zeros(self.ctrl_batch_size).cuda()\n",
    "            for sample_cnt in range(self.ctrl_batch_size):\n",
    "                # sample a child arch\n",
    "                self.ctrl.net_sample()\n",
    "                # valid a sampled arch and obtain reward\n",
    "                self.reward = child_model.eval_mini(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "                # add weighed entropy to reward\n",
    "                self.entropy = self.ctrl.sample_entropy\n",
    "                self.reward += self.entropy_weight * self.entropy\n",
    "                # update baseline\n",
    "                with torch.no_grad():\n",
    "                    self.baseline = self.baseline + (1 - self.baseline_decay) * (self.reward - self.baseline)\n",
    "                # update loss\n",
    "                self.log_prob = self.ctrl.sample_log_prob\n",
    "                self.skip_penalty = self.ctrl.sample_skip_penaltys\n",
    "                loss[sample_cnt] = self.log_prob * (self.reward - self.baseline) + self.skip_weight * self.skip_penalty\n",
    "            self.loss = loss.sum() / self.ctrl_batch_size # avg loss\n",
    "            print(step,self.loss.cpu().data)\n",
    "            # zero grads\n",
    "            self.optimizer.zero_grad()\n",
    "            # cal grads\n",
    "            # self.loss.backward(retain_graph=True)\n",
    "            self.loss.backward()\n",
    "            # update weights\n",
    "            self.optimizer.step()\n",
    "            # print(self.ctrl.net['op_fc'].weight.grad) # check grad is updated\n",
    "            # cal time consumed per step\n",
    "\n",
    "\n",
    "    def eval(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        evaluate controller using validating data set.\n",
    "        It samples several archs and validate them on \n",
    "        the whole validate set.\n",
    "            \n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "        # obtain averaged op_history\n",
    "        return accuracy\n",
    "\n",
    "    def derive_best_arch(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        derive the final child model using controller\n",
    "        procedure\n",
    "            1. sample 1000 archs\n",
    "            2. test them on test data set\n",
    "            3. select the one with highest accuracy as the best arch\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            best_arch\n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        best_arch = []\n",
    "        best_accuracy = 0\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "            # select the best arch\n",
    "            if eval_acc > best_accuracy:\n",
    "                best_accuracy = eval_acc\n",
    "                best_arch = self.ctrl.sample_arch\n",
    "\n",
    "        return best_accuracy, best_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = read_data(config.child_data_path, config.child_num_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = Child(\n",
    "        class_num=config.class_num,\n",
    "        num_layers=config.child_num_layers,\n",
    "        out_channels=config.child_out_channels,\n",
    "        batch_size=config.child_batch_size,\n",
    "        device=config.platform, \n",
    "        lr_init=config.child_lr_init,\n",
    "        lr_gamma=config.child_lr_gamma,\n",
    "        lr_cos_lmin=config.child_lr_cos_lmin,\n",
    "        lr_cos_Tmax=config.child_lr_cos_Tmax,\n",
    "        l2_reg=config.child_l2_reg,\n",
    "        run_loss_every=config.child_run_loss_every\n",
    "    )\n",
    "child = child.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = Controller(\n",
    "        device=config.platform,\n",
    "        lstm_size=config.ctrl_lstm_size,\n",
    "        lstm_num_layers=config.ctrl_lstm_num_layers,\n",
    "        child_num_layers=config.child_num_layers,\n",
    "        num_op=config.child_num_op,\n",
    "        train_step_num=config.ctrl_train_step_num,\n",
    "        ctrl_batch_size=config.ctrl_batch_size,\n",
    "        lr_init=config.ctrl_lr_init,\n",
    "        lr_gamma=config.ctrl_lr_gamma,\n",
    "        temperature=config.ctrl_temperature,\n",
    "        tanh_constant=config.ctrl_tanh_constant,\n",
    "        entropy_weight=config.ctrl_entropy_weight,\n",
    "        baseline_decay=config.ctrl_baseline_decay,\n",
    "        skip_target=config.ctrl_skip_target,\n",
    "        skip_weight=config.ctrl_skip_weight)\n",
    "ctrl = ctrl.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = images['train'].cuda()\n",
    "train_labels = labels['train'].cuda()\n",
    "valid_imgs = images['valid'].cuda()\n",
    "valid_labels = labels['valid'].cuda()\n",
    "test_imgs = images['test'].cuda()\n",
    "test_labels = labels['test'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = int(train_imgs.size()[0] / config.child_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "-------- train child --------\n",
      "ops [1, 0, 3, 2, 0, 2] skips [[], [1], [0, 1], [1, 1, 0], [0, 0, 0, 1], [0, 0, 1, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 1, Iter 100 /703, loss: 0.923\n",
      "Epoch 1, Iter 200 /703, loss: 0.854\n",
      "Epoch 1, Iter 300 /703, loss: 0.838\n",
      "Epoch 1, Iter 400 /703, loss: 0.805\n",
      "Epoch 1, Iter 500 /703, loss: 0.786\n",
      "Epoch 1, Iter 600 /703, loss: 0.795\n",
      "Epoch 1, Iter 700 /703, loss: 0.752\n",
      "epoch: 1\n",
      "-------- train child --------\n",
      "ops [2, 0, 1, 2, 0, 1] skips [[], [0], [0, 1], [0, 1, 0], [1, 0, 1, 1], [0, 1, 1, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 2, Iter 100 /703, loss: 1.199\n",
      "Epoch 2, Iter 200 /703, loss: 1.000\n",
      "Epoch 2, Iter 300 /703, loss: 0.959\n",
      "Epoch 2, Iter 400 /703, loss: 0.929\n",
      "Epoch 2, Iter 500 /703, loss: 0.895\n",
      "Epoch 2, Iter 600 /703, loss: 0.903\n",
      "Epoch 2, Iter 700 /703, loss: 0.876\n",
      "-------- train controller --------\n",
      "0 tensor(4.6234)\n",
      "1 tensor(5.1272)\n",
      "2 tensor(5.2284)\n",
      "3 tensor(3.7518)\n",
      "4 tensor(4.7315)\n",
      "5 tensor(6.3141)\n",
      "6 tensor(3.5018)\n",
      "7 tensor(4.5995)\n",
      "8 tensor(4.0186)\n",
      "9 tensor(4.9233)\n",
      "epoch: 2\n",
      "-------- train child --------\n",
      "ops [3, 0, 1, 3, 1, 3] skips [[], [1], [0, 0], [0, 0, 1], [1, 1, 0, 1], [1, 1, 1, 0, 0]]\n",
      "lr= [0.050000000000000114]\n",
      "Epoch 3, Iter 100 /703, loss: 1.087\n",
      "Epoch 3, Iter 200 /703, loss: 0.983\n",
      "Epoch 3, Iter 300 /703, loss: 0.938\n",
      "Epoch 3, Iter 400 /703, loss: 0.926\n",
      "Epoch 3, Iter 500 /703, loss: 0.894\n",
      "Epoch 3, Iter 600 /703, loss: 0.913\n",
      "Epoch 3, Iter 700 /703, loss: 0.887\n",
      "epoch: 3\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 1, 1, 3] skips [[], [0], [1, 1], [1, 1, 1], [1, 1, 0, 1], [0, 0, 1, 0, 1]]\n",
      "lr= [0.025500000000000033]\n",
      "Epoch 4, Iter 100 /703, loss: 0.972\n",
      "Epoch 4, Iter 200 /703, loss: 0.890\n",
      "Epoch 4, Iter 300 /703, loss: 0.859\n",
      "Epoch 4, Iter 400 /703, loss: 0.841\n",
      "Epoch 4, Iter 500 /703, loss: 0.800\n",
      "Epoch 4, Iter 600 /703, loss: 0.826\n",
      "Epoch 4, Iter 700 /703, loss: 0.799\n",
      "-------- train controller --------\n",
      "0 tensor(5.1640)\n",
      "1 tensor(4.3909)\n",
      "2 tensor(4.5535)\n",
      "3 tensor(4.9342)\n",
      "4 tensor(6.0291)\n",
      "5 tensor(4.5857)\n",
      "6 tensor(3.5566)\n",
      "7 tensor(4.3252)\n",
      "8 tensor(5.3661)\n",
      "9 tensor(3.7686)\n",
      "epoch: 4\n",
      "-------- train child --------\n",
      "ops [1, 1, 1, 3, 3, 2] skips [[], [0], [0, 1], [0, 1, 1], [1, 0, 1, 1], [1, 0, 0, 1, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 5, Iter 100 /703, loss: 0.976\n",
      "Epoch 5, Iter 200 /703, loss: 0.880\n",
      "Epoch 5, Iter 300 /703, loss: 0.856\n",
      "Epoch 5, Iter 400 /703, loss: 0.823\n",
      "Epoch 5, Iter 500 /703, loss: 0.796\n",
      "Epoch 5, Iter 600 /703, loss: 0.791\n",
      "Epoch 5, Iter 700 /703, loss: 0.761\n",
      "epoch: 5\n",
      "-------- train child --------\n",
      "ops [0, 3, 0, 0, 0, 1] skips [[], [1], [1, 0], [1, 0, 1], [1, 0, 1, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 6, Iter 100 /703, loss: 1.003\n",
      "Epoch 6, Iter 200 /703, loss: 0.866\n",
      "Epoch 6, Iter 300 /703, loss: 0.849\n",
      "Epoch 6, Iter 400 /703, loss: 0.828\n",
      "Epoch 6, Iter 500 /703, loss: 0.799\n",
      "Epoch 6, Iter 600 /703, loss: 0.807\n",
      "Epoch 6, Iter 700 /703, loss: 0.777\n",
      "-------- train controller --------\n",
      "0 tensor(4.1836)\n",
      "1 tensor(4.1520)\n",
      "2 tensor(4.5980)\n",
      "3 tensor(4.3926)\n",
      "4 tensor(4.7524)\n",
      "5 tensor(4.8195)\n",
      "6 tensor(3.8788)\n",
      "7 tensor(4.4666)\n",
      "8 tensor(4.9763)\n",
      "9 tensor(5.1183)\n",
      "epoch: 6\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 2, 2, 2] skips [[], [0], [1, 0], [1, 1, 1], [1, 1, 1, 0], [0, 1, 1, 1, 0]]\n",
      "lr= [0.05000000000000013]\n",
      "Epoch 7, Iter 100 /703, loss: 0.936\n",
      "Epoch 7, Iter 200 /703, loss: 0.921\n",
      "Epoch 7, Iter 300 /703, loss: 0.886\n",
      "Epoch 7, Iter 400 /703, loss: 0.887\n",
      "Epoch 7, Iter 500 /703, loss: 0.853\n",
      "Epoch 7, Iter 600 /703, loss: 0.868\n",
      "Epoch 7, Iter 700 /703, loss: 0.846\n",
      "epoch: 7\n",
      "-------- train child --------\n",
      "ops [3, 1, 0, 3, 0, 3] skips [[], [1], [1, 0], [1, 1, 1], [0, 0, 1, 1], [1, 1, 1, 0, 0]]\n",
      "lr= [0.025500000000000047]\n",
      "Epoch 8, Iter 100 /703, loss: 0.994\n",
      "Epoch 8, Iter 200 /703, loss: 0.886\n",
      "Epoch 8, Iter 300 /703, loss: 0.876\n",
      "Epoch 8, Iter 400 /703, loss: 0.856\n",
      "Epoch 8, Iter 500 /703, loss: 0.831\n",
      "Epoch 8, Iter 600 /703, loss: 0.837\n",
      "Epoch 8, Iter 700 /703, loss: 0.803\n",
      "-------- train controller --------\n",
      "0 tensor(4.5452)\n",
      "1 tensor(5.3487)\n",
      "2 tensor(4.2824)\n",
      "3 tensor(3.7120)\n",
      "4 tensor(3.7534)\n",
      "5 tensor(4.4621)\n",
      "6 tensor(4.3366)\n",
      "7 tensor(4.8970)\n",
      "8 tensor(4.4512)\n",
      "9 tensor(5.3110)\n",
      "epoch: 8\n",
      "-------- train child --------\n",
      "ops [3, 3, 2, 3, 1, 1] skips [[], [1], [0, 0], [1, 1, 1], [1, 1, 0, 0], [0, 1, 1, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 9, Iter 100 /703, loss: 1.487\n",
      "Epoch 9, Iter 200 /703, loss: 1.217\n",
      "Epoch 9, Iter 300 /703, loss: 1.151\n",
      "Epoch 9, Iter 400 /703, loss: 1.096\n",
      "Epoch 9, Iter 500 /703, loss: 1.068\n",
      "Epoch 9, Iter 600 /703, loss: 1.053\n",
      "Epoch 9, Iter 700 /703, loss: 1.005\n",
      "epoch: 9\n",
      "-------- train child --------\n",
      "ops [0, 1, 3, 1, 2, 0] skips [[], [0], [0, 0], [0, 0, 1], [0, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 10, Iter 100 /703, loss: 1.060\n",
      "Epoch 10, Iter 200 /703, loss: 0.889\n",
      "Epoch 10, Iter 300 /703, loss: 0.849\n",
      "Epoch 10, Iter 400 /703, loss: 0.829\n",
      "Epoch 10, Iter 500 /703, loss: 0.789\n",
      "Epoch 10, Iter 600 /703, loss: 0.804\n",
      "Epoch 10, Iter 700 /703, loss: 0.775\n",
      "-------- train controller --------\n",
      "0 tensor(4.6560)\n",
      "1 tensor(4.3437)\n",
      "2 tensor(4.8730)\n",
      "3 tensor(4.5035)\n",
      "4 tensor(4.1905)\n",
      "5 tensor(4.6906)\n",
      "6 tensor(3.3579)\n",
      "7 tensor(4.1816)\n",
      "8 tensor(5.3990)\n",
      "9 tensor(3.9758)\n",
      "epoch: 10\n",
      "-------- train child --------\n",
      "ops [0, 2, 3, 1, 1, 3] skips [[], [0], [1, 0], [1, 1, 0], [1, 1, 0, 0], [1, 0, 0, 1, 0]]\n",
      "lr= [0.050000000000000135]\n",
      "Epoch 11, Iter 100 /703, loss: 0.946\n",
      "Epoch 11, Iter 200 /703, loss: 0.846\n",
      "Epoch 11, Iter 300 /703, loss: 0.837\n",
      "Epoch 11, Iter 400 /703, loss: 0.820\n",
      "Epoch 11, Iter 500 /703, loss: 0.781\n",
      "Epoch 11, Iter 600 /703, loss: 0.808\n",
      "Epoch 11, Iter 700 /703, loss: 0.775\n",
      "epoch: 11\n",
      "-------- train child --------\n",
      "ops [0, 0, 1, 1, 3, 0] skips [[], [1], [1, 1], [0, 0, 1], [1, 1, 0, 1], [0, 1, 0, 1, 0]]\n",
      "lr= [0.025500000000000057]\n",
      "Epoch 12, Iter 100 /703, loss: 0.816\n",
      "Epoch 12, Iter 200 /703, loss: 0.751\n",
      "Epoch 12, Iter 300 /703, loss: 0.737\n",
      "Epoch 12, Iter 400 /703, loss: 0.723\n",
      "Epoch 12, Iter 500 /703, loss: 0.689\n",
      "Epoch 12, Iter 600 /703, loss: 0.717\n",
      "Epoch 12, Iter 700 /703, loss: 0.676\n",
      "-------- train controller --------\n",
      "0 tensor(4.7751)\n",
      "1 tensor(3.0499)\n",
      "2 tensor(5.4017)\n",
      "3 tensor(3.5774)\n",
      "4 tensor(4.0058)\n",
      "5 tensor(3.7161)\n",
      "6 tensor(4.1377)\n",
      "7 tensor(4.4447)\n",
      "8 tensor(2.9612)\n",
      "9 tensor(5.5807)\n",
      "epoch: 12\n",
      "-------- train child --------\n",
      "ops [2, 3, 1, 2, 0, 1] skips [[], [0], [0, 0], [1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 13, Iter 100 /703, loss: 1.306\n",
      "Epoch 13, Iter 200 /703, loss: 1.044\n",
      "Epoch 13, Iter 300 /703, loss: 0.986\n",
      "Epoch 13, Iter 400 /703, loss: 0.933\n",
      "Epoch 13, Iter 500 /703, loss: 0.900\n",
      "Epoch 13, Iter 600 /703, loss: 0.891\n",
      "Epoch 13, Iter 700 /703, loss: 0.844\n",
      "epoch: 13\n",
      "-------- train child --------\n",
      "ops [3, 1, 1, 2, 0, 0] skips [[], [0], [0, 0], [0, 1, 1], [0, 1, 1, 0], [1, 1, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 14, Iter 100 /703, loss: 0.908\n",
      "Epoch 14, Iter 200 /703, loss: 0.836\n",
      "Epoch 14, Iter 300 /703, loss: 0.831\n",
      "Epoch 14, Iter 400 /703, loss: 0.811\n",
      "Epoch 14, Iter 500 /703, loss: 0.781\n",
      "Epoch 14, Iter 600 /703, loss: 0.806\n",
      "Epoch 14, Iter 700 /703, loss: 0.763\n",
      "-------- train controller --------\n",
      "0 tensor(3.5324)\n",
      "1 tensor(3.7568)\n",
      "2 tensor(4.1632)\n",
      "3 tensor(4.4945)\n",
      "4 tensor(3.2405)\n",
      "5 tensor(4.6829)\n",
      "6 tensor(4.4862)\n",
      "7 tensor(3.8526)\n",
      "8 tensor(4.9595)\n",
      "9 tensor(5.1305)\n",
      "epoch: 14\n",
      "-------- train child --------\n",
      "ops [1, 3, 1, 1, 2, 2] skips [[], [1], [1, 0], [0, 0, 0], [0, 1, 1, 1], [1, 1, 0, 1, 0]]\n",
      "lr= [0.05000000000000016]\n",
      "Epoch 15, Iter 100 /703, loss: 0.876\n",
      "Epoch 15, Iter 200 /703, loss: 0.799\n",
      "Epoch 15, Iter 300 /703, loss: 0.781\n",
      "Epoch 15, Iter 400 /703, loss: 0.772\n",
      "Epoch 15, Iter 500 /703, loss: 0.731\n",
      "Epoch 15, Iter 600 /703, loss: 0.769\n",
      "Epoch 15, Iter 700 /703, loss: 0.725\n",
      "epoch: 15\n",
      "-------- train child --------\n",
      "ops [1, 2, 0, 0, 3, 2] skips [[], [0], [1, 0], [1, 1, 1], [0, 1, 1, 1], [1, 1, 0, 1, 0]]\n",
      "lr= [0.025500000000000075]\n",
      "Epoch 16, Iter 100 /703, loss: 0.754\n",
      "Epoch 16, Iter 200 /703, loss: 0.719\n",
      "Epoch 16, Iter 300 /703, loss: 0.716\n",
      "Epoch 16, Iter 400 /703, loss: 0.713\n",
      "Epoch 16, Iter 500 /703, loss: 0.674\n",
      "Epoch 16, Iter 600 /703, loss: 0.703\n",
      "Epoch 16, Iter 700 /703, loss: 0.668\n",
      "-------- train controller --------\n",
      "0 tensor(4.3416)\n",
      "1 tensor(5.1215)\n",
      "2 tensor(3.8114)\n",
      "3 tensor(3.9401)\n",
      "4 tensor(5.0376)\n",
      "5 tensor(2.8651)\n",
      "6 tensor(3.8700)\n",
      "7 tensor(3.4914)\n",
      "8 tensor(5.6103)\n",
      "9 tensor(4.5806)\n",
      "epoch: 16\n",
      "-------- train child --------\n",
      "ops [0, 1, 1, 3, 1, 1] skips [[], [0], [0, 0], [0, 1, 0], [0, 0, 1, 1], [1, 0, 0, 0, 1]]\n",
      "lr= [0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Iter 100 /703, loss: 0.921\n",
      "Epoch 17, Iter 200 /703, loss: 0.799\n",
      "Epoch 17, Iter 300 /703, loss: 0.782\n",
      "Epoch 17, Iter 400 /703, loss: 0.755\n",
      "Epoch 17, Iter 500 /703, loss: 0.717\n",
      "Epoch 17, Iter 600 /703, loss: 0.735\n",
      "Epoch 17, Iter 700 /703, loss: 0.669\n",
      "epoch: 17\n",
      "-------- train child --------\n",
      "ops [2, 0, 1, 1, 0, 1] skips [[], [1], [1, 0], [0, 1, 1], [1, 0, 0, 1], [1, 0, 1, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 18, Iter 100 /703, loss: 0.790\n",
      "Epoch 18, Iter 200 /703, loss: 0.742\n",
      "Epoch 18, Iter 300 /703, loss: 0.720\n",
      "Epoch 18, Iter 400 /703, loss: 0.718\n",
      "Epoch 18, Iter 500 /703, loss: 0.685\n",
      "Epoch 18, Iter 600 /703, loss: 0.706\n",
      "Epoch 18, Iter 700 /703, loss: 0.676\n",
      "-------- train controller --------\n",
      "0 tensor(3.5750)\n",
      "1 tensor(4.0934)\n",
      "2 tensor(2.1740)\n",
      "3 tensor(4.0330)\n",
      "4 tensor(3.8823)\n",
      "5 tensor(4.6996)\n",
      "6 tensor(4.3128)\n",
      "7 tensor(4.8834)\n",
      "8 tensor(3.4215)\n",
      "9 tensor(3.6196)\n",
      "epoch: 18\n",
      "-------- train child --------\n",
      "ops [0, 0, 0, 1, 1, 2] skips [[], [0], [1, 1], [1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.05000000000000017]\n",
      "Epoch 19, Iter 100 /703, loss: 0.827\n",
      "Epoch 19, Iter 200 /703, loss: 0.767\n",
      "Epoch 19, Iter 300 /703, loss: 0.735\n",
      "Epoch 19, Iter 400 /703, loss: 0.731\n",
      "Epoch 19, Iter 500 /703, loss: 0.693\n",
      "Epoch 19, Iter 600 /703, loss: 0.718\n",
      "Epoch 19, Iter 700 /703, loss: 0.677\n",
      "epoch: 19\n",
      "-------- train child --------\n",
      "ops [0, 0, 2, 1, 3, 1] skips [[], [0], [0, 0], [0, 1, 1], [1, 1, 0, 1], [0, 0, 1, 1, 0]]\n",
      "lr= [0.025500000000000085]\n",
      "Epoch 20, Iter 100 /703, loss: 0.793\n",
      "Epoch 20, Iter 200 /703, loss: 0.706\n",
      "Epoch 20, Iter 300 /703, loss: 0.687\n",
      "Epoch 20, Iter 400 /703, loss: 0.680\n",
      "Epoch 20, Iter 500 /703, loss: 0.625\n",
      "Epoch 20, Iter 600 /703, loss: 0.664\n",
      "Epoch 20, Iter 700 /703, loss: 0.624\n",
      "-------- train controller --------\n",
      "0 tensor(1.7241)\n",
      "1 tensor(4.5790)\n",
      "2 tensor(2.5128)\n",
      "3 tensor(2.6490)\n",
      "4 tensor(3.9936)\n",
      "5 tensor(4.1565)\n",
      "6 tensor(1.7780)\n",
      "7 tensor(3.0699)\n",
      "8 tensor(2.6359)\n",
      "9 tensor(3.8682)\n",
      "epoch: 20\n",
      "-------- train child --------\n",
      "ops [3, 0, 1, 2, 1, 1] skips [[], [1], [1, 0], [0, 1, 0], [0, 1, 1, 1], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 21, Iter 100 /703, loss: 0.904\n",
      "Epoch 21, Iter 200 /703, loss: 0.818\n",
      "Epoch 21, Iter 300 /703, loss: 0.823\n",
      "Epoch 21, Iter 400 /703, loss: 0.773\n",
      "Epoch 21, Iter 500 /703, loss: 0.749\n",
      "Epoch 21, Iter 600 /703, loss: 0.758\n",
      "Epoch 21, Iter 700 /703, loss: 0.706\n",
      "epoch: 21\n",
      "-------- train child --------\n",
      "ops [1, 3, 1, 2, 1, 2] skips [[], [0], [1, 1], [1, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 22, Iter 100 /703, loss: 0.667\n",
      "Epoch 22, Iter 200 /703, loss: 0.644\n",
      "Epoch 22, Iter 300 /703, loss: 0.645\n",
      "Epoch 22, Iter 400 /703, loss: 0.647\n",
      "Epoch 22, Iter 500 /703, loss: 0.598\n",
      "Epoch 22, Iter 600 /703, loss: 0.638\n",
      "Epoch 22, Iter 700 /703, loss: 0.604\n",
      "-------- train controller --------\n",
      "0 tensor(4.9833)\n",
      "1 tensor(2.4632)\n",
      "2 tensor(3.1125)\n",
      "3 tensor(2.9275)\n",
      "4 tensor(3.0931)\n",
      "5 tensor(3.2916)\n",
      "6 tensor(3.6876)\n",
      "7 tensor(4.2979)\n",
      "8 tensor(0.6212)\n",
      "9 tensor(2.1716)\n",
      "epoch: 22\n",
      "-------- train child --------\n",
      "ops [2, 1, 1, 0, 1, 2] skips [[], [0], [1, 0], [0, 0, 1], [0, 0, 0, 1], [0, 0, 1, 1, 1]]\n",
      "lr= [0.05000000000000018]\n",
      "Epoch 23, Iter 100 /703, loss: 0.901\n",
      "Epoch 23, Iter 200 /703, loss: 0.819\n",
      "Epoch 23, Iter 300 /703, loss: 0.782\n",
      "Epoch 23, Iter 400 /703, loss: 0.771\n",
      "Epoch 23, Iter 500 /703, loss: 0.745\n",
      "Epoch 23, Iter 600 /703, loss: 0.766\n",
      "Epoch 23, Iter 700 /703, loss: 0.729\n",
      "epoch: 23\n",
      "-------- train child --------\n",
      "ops [1, 1, 1, 2, 3, 0] skips [[], [0], [0, 1], [0, 1, 1], [1, 0, 0, 0], [1, 1, 1, 0, 1]]\n",
      "lr= [0.0255000000000001]\n",
      "Epoch 24, Iter 100 /703, loss: 0.702\n",
      "Epoch 24, Iter 200 /703, loss: 0.647\n",
      "Epoch 24, Iter 300 /703, loss: 0.644\n",
      "Epoch 24, Iter 400 /703, loss: 0.644\n",
      "Epoch 24, Iter 500 /703, loss: 0.603\n",
      "Epoch 24, Iter 600 /703, loss: 0.642\n",
      "Epoch 24, Iter 700 /703, loss: 0.597\n",
      "-------- train controller --------\n",
      "0 tensor(3.4951)\n",
      "1 tensor(3.2118)\n",
      "2 tensor(5.3335)\n",
      "3 tensor(2.4985)\n",
      "4 tensor(3.6240)\n",
      "5 tensor(1.7153)\n",
      "6 tensor(3.7365)\n",
      "7 tensor(2.2349)\n",
      "8 tensor(3.7794)\n",
      "9 tensor(2.7436)\n",
      "epoch: 24\n",
      "-------- train child --------\n",
      "ops [0, 2, 1, 3, 2, 2] skips [[], [0], [1, 0], [1, 0, 1], [1, 0, 0, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 25, Iter 100 /703, loss: 1.008\n",
      "Epoch 25, Iter 200 /703, loss: 0.823\n",
      "Epoch 25, Iter 300 /703, loss: 0.800\n",
      "Epoch 25, Iter 400 /703, loss: 0.769\n",
      "Epoch 25, Iter 500 /703, loss: 0.722\n",
      "Epoch 25, Iter 600 /703, loss: 0.736\n",
      "Epoch 25, Iter 700 /703, loss: 0.698\n",
      "epoch: 25\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 3, 0, 1] skips [[], [1], [0, 1], [0, 1, 0], [0, 1, 0, 0], [1, 1, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 26, Iter 100 /703, loss: 0.882\n",
      "Epoch 26, Iter 200 /703, loss: 0.782\n",
      "Epoch 26, Iter 300 /703, loss: 0.779\n",
      "Epoch 26, Iter 400 /703, loss: 0.756\n",
      "Epoch 26, Iter 500 /703, loss: 0.728\n",
      "Epoch 26, Iter 600 /703, loss: 0.756\n",
      "Epoch 26, Iter 700 /703, loss: 0.721\n",
      "-------- train controller --------\n",
      "0 tensor(4.2017)\n",
      "1 tensor(4.4274)\n",
      "2 tensor(3.9192)\n",
      "3 tensor(2.2630)\n",
      "4 tensor(4.3836)\n",
      "5 tensor(2.9146)\n",
      "6 tensor(3.8312)\n",
      "7 tensor(4.1106)\n",
      "8 tensor(4.2429)\n",
      "9 tensor(4.0009)\n",
      "epoch: 26\n",
      "-------- train child --------\n",
      "ops [2, 0, 1, 3, 0, 0] skips [[], [1], [1, 0], [0, 0, 1], [1, 0, 0, 0], [1, 1, 1, 1, 0]]\n",
      "lr= [0.05000000000000019]\n",
      "Epoch 27, Iter 100 /703, loss: 0.801\n",
      "Epoch 27, Iter 200 /703, loss: 0.740\n",
      "Epoch 27, Iter 300 /703, loss: 0.733\n",
      "Epoch 27, Iter 400 /703, loss: 0.735\n",
      "Epoch 27, Iter 500 /703, loss: 0.705\n",
      "Epoch 27, Iter 600 /703, loss: 0.725\n",
      "Epoch 27, Iter 700 /703, loss: 0.684\n",
      "epoch: 27\n",
      "-------- train child --------\n",
      "ops [0, 0, 0, 1, 3, 2] skips [[], [1], [0, 1], [0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 1, 1]]\n",
      "lr= [0.025500000000000106]\n",
      "Epoch 28, Iter 100 /703, loss: 0.748\n",
      "Epoch 28, Iter 200 /703, loss: 0.667\n",
      "Epoch 28, Iter 300 /703, loss: 0.655\n",
      "Epoch 28, Iter 400 /703, loss: 0.641\n",
      "Epoch 28, Iter 500 /703, loss: 0.606\n",
      "Epoch 28, Iter 600 /703, loss: 0.635\n",
      "Epoch 28, Iter 700 /703, loss: 0.592\n",
      "-------- train controller --------\n",
      "0 tensor(3.9748)\n",
      "1 tensor(5.6867)\n",
      "2 tensor(3.6201)\n",
      "3 tensor(2.5564)\n",
      "4 tensor(2.8255)\n",
      "5 tensor(3.6145)\n",
      "6 tensor(2.9337)\n",
      "7 tensor(3.4836)\n",
      "8 tensor(2.6620)\n",
      "9 tensor(2.8755)\n",
      "epoch: 28\n",
      "-------- train child --------\n",
      "ops [0, 1, 1, 3, 3, 1] skips [[], [0], [1, 0], [0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 29, Iter 100 /703, loss: 0.779\n",
      "Epoch 29, Iter 200 /703, loss: 0.679\n",
      "Epoch 29, Iter 300 /703, loss: 0.669\n",
      "Epoch 29, Iter 400 /703, loss: 0.660\n",
      "Epoch 29, Iter 500 /703, loss: 0.613\n",
      "Epoch 29, Iter 600 /703, loss: 0.629\n",
      "Epoch 29, Iter 700 /703, loss: 0.576\n",
      "epoch: 29\n",
      "-------- train child --------\n",
      "ops [0, 3, 0, 3, 0, 1] skips [[], [1], [0, 0], [1, 0, 0], [1, 0, 0, 0], [1, 0, 1, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 30, Iter 100 /703, loss: 0.680\n",
      "Epoch 30, Iter 200 /703, loss: 0.648\n",
      "Epoch 30, Iter 300 /703, loss: 0.636\n",
      "Epoch 30, Iter 400 /703, loss: 0.636\n",
      "Epoch 30, Iter 500 /703, loss: 0.595\n",
      "Epoch 30, Iter 600 /703, loss: 0.620\n",
      "Epoch 30, Iter 700 /703, loss: 0.588\n",
      "-------- train controller --------\n",
      "0 tensor(2.2703)\n",
      "1 tensor(4.2919)\n",
      "2 tensor(2.9821)\n",
      "3 tensor(3.3889)\n",
      "4 tensor(3.4254)\n",
      "5 tensor(2.6621)\n",
      "6 tensor(2.7444)\n",
      "7 tensor(4.6549)\n",
      "8 tensor(2.2580)\n",
      "9 tensor(3.8278)\n",
      "epoch: 30\n",
      "-------- train child --------\n",
      "ops [2, 2, 1, 3, 1, 1] skips [[], [0], [1, 0], [0, 1, 0], [1, 0, 1, 1], [0, 1, 1, 0, 1]]\n",
      "lr= [0.050000000000000204]\n",
      "Epoch 31, Iter 100 /703, loss: 0.873\n",
      "Epoch 31, Iter 200 /703, loss: 0.782\n",
      "Epoch 31, Iter 300 /703, loss: 0.756\n",
      "Epoch 31, Iter 400 /703, loss: 0.756\n",
      "Epoch 31, Iter 500 /703, loss: 0.714\n",
      "Epoch 31, Iter 600 /703, loss: 0.747\n",
      "Epoch 31, Iter 700 /703, loss: 0.711\n",
      "epoch: 31\n",
      "-------- train child --------\n",
      "ops [1, 0, 0, 2, 2, 3] skips [[], [1], [1, 0], [1, 0, 0], [1, 1, 0, 1], [0, 0, 1, 0, 0]]\n",
      "lr= [0.025500000000000293]\n",
      "Epoch 32, Iter 100 /703, loss: 0.695\n",
      "Epoch 32, Iter 200 /703, loss: 0.650\n",
      "Epoch 32, Iter 300 /703, loss: 0.645\n",
      "Epoch 32, Iter 400 /703, loss: 0.649\n",
      "Epoch 32, Iter 500 /703, loss: 0.605\n",
      "Epoch 32, Iter 600 /703, loss: 0.644\n",
      "Epoch 32, Iter 700 /703, loss: 0.601\n",
      "-------- train controller --------\n",
      "0 tensor(4.4092)\n",
      "1 tensor(3.3577)\n",
      "2 tensor(3.9015)\n",
      "3 tensor(1.3472)\n",
      "4 tensor(4.1245)\n",
      "5 tensor(3.4626)\n",
      "6 tensor(3.6288)\n",
      "7 tensor(3.4639)\n",
      "8 tensor(3.4897)\n",
      "9 tensor(3.4348)\n",
      "epoch: 32\n",
      "-------- train child --------\n",
      "ops [0, 0, 1, 1, 0, 1] skips [[], [0], [0, 0], [1, 1, 0], [0, 1, 1, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 33, Iter 100 /703, loss: 0.874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Iter 200 /703, loss: 0.772\n",
      "Epoch 33, Iter 300 /703, loss: 0.735\n",
      "Epoch 33, Iter 400 /703, loss: 0.692\n",
      "Epoch 33, Iter 500 /703, loss: 0.667\n",
      "Epoch 33, Iter 600 /703, loss: 0.670\n",
      "Epoch 33, Iter 700 /703, loss: 0.600\n",
      "epoch: 33\n",
      "-------- train child --------\n",
      "ops [1, 1, 1, 0, 2, 1] skips [[], [1], [0, 0], [1, 1, 0], [0, 1, 1, 0], [0, 1, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 34, Iter 100 /703, loss: 0.660\n",
      "Epoch 34, Iter 200 /703, loss: 0.628\n",
      "Epoch 34, Iter 300 /703, loss: 0.618\n",
      "Epoch 34, Iter 400 /703, loss: 0.612\n",
      "Epoch 34, Iter 500 /703, loss: 0.578\n",
      "Epoch 34, Iter 600 /703, loss: 0.599\n",
      "Epoch 34, Iter 700 /703, loss: 0.558\n",
      "-------- train controller --------\n",
      "0 tensor(3.1183)\n",
      "1 tensor(3.4557)\n",
      "2 tensor(2.7107)\n",
      "3 tensor(1.8347)\n",
      "4 tensor(1.4385)\n",
      "5 tensor(3.8641)\n",
      "6 tensor(1.0276)\n",
      "7 tensor(2.2948)\n",
      "8 tensor(2.8670)\n",
      "9 tensor(1.7212)\n",
      "epoch: 34\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 3, 1, 3] skips [[], [0], [0, 1], [1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.05000000000000021]\n",
      "Epoch 35, Iter 100 /703, loss: 1.121\n",
      "Epoch 35, Iter 200 /703, loss: 0.902\n",
      "Epoch 35, Iter 300 /703, loss: 0.872\n",
      "Epoch 35, Iter 400 /703, loss: 0.840\n",
      "Epoch 35, Iter 500 /703, loss: 0.780\n",
      "Epoch 35, Iter 600 /703, loss: 0.818\n",
      "Epoch 35, Iter 700 /703, loss: 0.770\n",
      "epoch: 35\n",
      "-------- train child --------\n",
      "ops [2, 1, 3, 1, 2, 3] skips [[], [0], [0, 0], [1, 1, 0], [0, 0, 1, 1], [1, 0, 0, 0, 0]]\n",
      "lr= [0.025500000000000127]\n",
      "Epoch 36, Iter 100 /703, loss: 0.967\n",
      "Epoch 36, Iter 200 /703, loss: 0.802\n",
      "Epoch 36, Iter 300 /703, loss: 0.780\n",
      "Epoch 36, Iter 400 /703, loss: 0.764\n",
      "Epoch 36, Iter 500 /703, loss: 0.710\n",
      "Epoch 36, Iter 600 /703, loss: 0.742\n",
      "Epoch 36, Iter 700 /703, loss: 0.714\n",
      "-------- train controller --------\n",
      "0 tensor(3.2625)\n",
      "1 tensor(1.9384)\n",
      "2 tensor(3.1893)\n",
      "3 tensor(3.6763)\n",
      "4 tensor(2.2331)\n",
      "5 tensor(4.1694)\n",
      "6 tensor(3.1906)\n",
      "7 tensor(2.0175)\n",
      "8 tensor(3.3605)\n",
      "9 tensor(3.5999)\n",
      "epoch: 36\n",
      "-------- train child --------\n",
      "ops [0, 1, 3, 0, 3, 3] skips [[], [0], [0, 0], [0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 1, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 37, Iter 100 /703, loss: 0.997\n",
      "Epoch 37, Iter 200 /703, loss: 0.857\n",
      "Epoch 37, Iter 300 /703, loss: 0.835\n",
      "Epoch 37, Iter 400 /703, loss: 0.800\n",
      "Epoch 37, Iter 500 /703, loss: 0.765\n",
      "Epoch 37, Iter 600 /703, loss: 0.763\n",
      "Epoch 37, Iter 700 /703, loss: 0.719\n",
      "epoch: 37\n",
      "-------- train child --------\n",
      "ops [0, 0, 0, 3, 2, 2] skips [[], [1], [0, 0], [0, 0, 1], [0, 1, 1, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 38, Iter 100 /703, loss: 0.733\n",
      "Epoch 38, Iter 200 /703, loss: 0.686\n",
      "Epoch 38, Iter 300 /703, loss: 0.668\n",
      "Epoch 38, Iter 400 /703, loss: 0.666\n",
      "Epoch 38, Iter 500 /703, loss: 0.618\n",
      "Epoch 38, Iter 600 /703, loss: 0.656\n",
      "Epoch 38, Iter 700 /703, loss: 0.618\n",
      "-------- train controller --------\n",
      "0 tensor(2.4493)\n",
      "1 tensor(2.0628)\n",
      "2 tensor(3.5416)\n",
      "3 tensor(1.8060)\n",
      "4 tensor(3.2441)\n",
      "5 tensor(2.5583)\n",
      "6 tensor(3.6042)\n",
      "7 tensor(3.6005)\n",
      "8 tensor(2.4253)\n",
      "9 tensor(1.1698)\n",
      "epoch: 38\n",
      "-------- train child --------\n",
      "ops [0, 2, 3, 2, 0, 2] skips [[], [1], [1, 0], [1, 0, 1], [0, 1, 0, 1], [1, 1, 0, 1, 0]]\n",
      "lr= [0.04999999999999988]\n",
      "Epoch 39, Iter 100 /703, loss: 0.873\n",
      "Epoch 39, Iter 200 /703, loss: 0.800\n",
      "Epoch 39, Iter 300 /703, loss: 0.775\n",
      "Epoch 39, Iter 400 /703, loss: 0.764\n",
      "Epoch 39, Iter 500 /703, loss: 0.734\n",
      "Epoch 39, Iter 600 /703, loss: 0.756\n",
      "Epoch 39, Iter 700 /703, loss: 0.726\n",
      "epoch: 39\n",
      "-------- train child --------\n",
      "ops [2, 1, 0, 0, 1, 0] skips [[], [0], [1, 0], [1, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0, 1]]\n",
      "lr= [0.025500000000000148]\n",
      "Epoch 40, Iter 100 /703, loss: 0.810\n",
      "Epoch 40, Iter 200 /703, loss: 0.735\n",
      "Epoch 40, Iter 300 /703, loss: 0.702\n",
      "Epoch 40, Iter 400 /703, loss: 0.696\n",
      "Epoch 40, Iter 500 /703, loss: 0.648\n",
      "Epoch 40, Iter 600 /703, loss: 0.677\n",
      "Epoch 40, Iter 700 /703, loss: 0.646\n",
      "-------- train controller --------\n",
      "0 tensor(3.0352)\n",
      "1 tensor(2.9846)\n",
      "2 tensor(4.1368)\n",
      "3 tensor(3.1582)\n",
      "4 tensor(3.3709)\n",
      "5 tensor(1.4270)\n",
      "6 tensor(2.8819)\n",
      "7 tensor(2.0531)\n",
      "8 tensor(0.5817)\n",
      "9 tensor(1.9549)\n",
      "epoch: 40\n",
      "-------- train child --------\n",
      "ops [0, 0, 3, 2, 2, 1] skips [[], [1], [1, 0], [0, 0, 1], [1, 1, 0, 1], [0, 0, 1, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 41, Iter 100 /703, loss: 0.712\n",
      "Epoch 41, Iter 200 /703, loss: 0.650\n",
      "Epoch 41, Iter 300 /703, loss: 0.638\n",
      "Epoch 41, Iter 400 /703, loss: 0.625\n",
      "Epoch 41, Iter 500 /703, loss: 0.599\n",
      "Epoch 41, Iter 600 /703, loss: 0.596\n",
      "Epoch 41, Iter 700 /703, loss: 0.543\n",
      "epoch: 41\n",
      "-------- train child --------\n",
      "ops [0, 0, 2, 3, 2, 3] skips [[], [0], [0, 0], [1, 0, 0], [0, 0, 0, 1], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 42, Iter 100 /703, loss: 0.744\n",
      "Epoch 42, Iter 200 /703, loss: 0.695\n",
      "Epoch 42, Iter 300 /703, loss: 0.675\n",
      "Epoch 42, Iter 400 /703, loss: 0.670\n",
      "Epoch 42, Iter 500 /703, loss: 0.638\n",
      "Epoch 42, Iter 600 /703, loss: 0.668\n",
      "Epoch 42, Iter 700 /703, loss: 0.632\n",
      "-------- train controller --------\n",
      "0 tensor(2.5444)\n",
      "1 tensor(1.8968)\n",
      "2 tensor(2.2473)\n",
      "3 tensor(1.2756)\n",
      "4 tensor(1.7180)\n",
      "5 tensor(3.2097)\n",
      "6 tensor(2.0991)\n",
      "7 tensor(2.6245)\n",
      "8 tensor(3.1090)\n",
      "9 tensor(2.3898)\n",
      "epoch: 42\n",
      "-------- train child --------\n",
      "ops [2, 2, 2, 0, 2, 1] skips [[], [1], [0, 0], [0, 1, 0], [0, 0, 0, 0], [1, 0, 0, 0, 0]]\n",
      "lr= [0.05000000000000023]\n",
      "Epoch 43, Iter 100 /703, loss: 1.009\n",
      "Epoch 43, Iter 200 /703, loss: 0.842\n",
      "Epoch 43, Iter 300 /703, loss: 0.833\n",
      "Epoch 43, Iter 400 /703, loss: 0.803\n",
      "Epoch 43, Iter 500 /703, loss: 0.766\n",
      "Epoch 43, Iter 600 /703, loss: 0.791\n",
      "Epoch 43, Iter 700 /703, loss: 0.751\n",
      "epoch: 43\n",
      "-------- train child --------\n",
      "ops [3, 0, 1, 2, 3, 0] skips [[], [1], [0, 1], [1, 0, 0], [0, 1, 1, 0], [1, 1, 0, 0, 0]]\n",
      "lr= [0.025500000000000154]\n",
      "Epoch 44, Iter 100 /703, loss: 0.928\n",
      "Epoch 44, Iter 200 /703, loss: 0.778\n",
      "Epoch 44, Iter 300 /703, loss: 0.770\n",
      "Epoch 44, Iter 400 /703, loss: 0.750\n",
      "Epoch 44, Iter 500 /703, loss: 0.704\n",
      "Epoch 44, Iter 600 /703, loss: 0.725\n",
      "Epoch 44, Iter 700 /703, loss: 0.695\n",
      "-------- train controller --------\n",
      "0 tensor(1.1029)\n",
      "1 tensor(2.6516)\n",
      "2 tensor(2.3347)\n",
      "3 tensor(2.4204)\n",
      "4 tensor(0.5586)\n",
      "5 tensor(2.0772)\n",
      "6 tensor(3.4793)\n",
      "7 tensor(3.1189)\n",
      "8 tensor(3.2720)\n",
      "9 tensor(2.1376)\n",
      "epoch: 44\n",
      "-------- train child --------\n",
      "ops [3, 1, 3, 0, 3, 3] skips [[], [0], [1, 0], [1, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 45, Iter 100 /703, loss: 1.250\n",
      "Epoch 45, Iter 200 /703, loss: 1.045\n",
      "Epoch 45, Iter 300 /703, loss: 1.017\n",
      "Epoch 45, Iter 400 /703, loss: 0.970\n",
      "Epoch 45, Iter 500 /703, loss: 0.943\n",
      "Epoch 45, Iter 600 /703, loss: 0.945\n",
      "Epoch 45, Iter 700 /703, loss: 0.910\n",
      "epoch: 45\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 1, 0, 3] skips [[], [1], [1, 0], [0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 46, Iter 100 /703, loss: 0.819\n",
      "Epoch 46, Iter 200 /703, loss: 0.734\n",
      "Epoch 46, Iter 300 /703, loss: 0.719\n",
      "Epoch 46, Iter 400 /703, loss: 0.711\n",
      "Epoch 46, Iter 500 /703, loss: 0.666\n",
      "Epoch 46, Iter 600 /703, loss: 0.701\n",
      "Epoch 46, Iter 700 /703, loss: 0.668\n",
      "-------- train controller --------\n",
      "0 tensor(3.4373)\n",
      "1 tensor(2.4349)\n",
      "2 tensor(2.2643)\n",
      "3 tensor(1.8935)\n",
      "4 tensor(3.1130)\n",
      "5 tensor(2.5648)\n",
      "6 tensor(2.7115)\n",
      "7 tensor(2.7606)\n",
      "8 tensor(2.2116)\n",
      "9 tensor(2.7219)\n",
      "epoch: 46\n",
      "-------- train child --------\n",
      "ops [2, 3, 2, 1, 1, 2] skips [[], [0], [1, 0], [0, 0, 1], [1, 0, 0, 1], [0, 0, 1, 0, 1]]\n",
      "lr= [0.0499999999999999]\n",
      "Epoch 47, Iter 100 /703, loss: 0.923\n",
      "Epoch 47, Iter 200 /703, loss: 0.816\n",
      "Epoch 47, Iter 300 /703, loss: 0.784\n",
      "Epoch 47, Iter 400 /703, loss: 0.790\n",
      "Epoch 47, Iter 500 /703, loss: 0.739\n",
      "Epoch 47, Iter 600 /703, loss: 0.764\n",
      "Epoch 47, Iter 700 /703, loss: 0.730\n",
      "epoch: 47\n",
      "-------- train child --------\n",
      "ops [0, 3, 3, 0, 1, 2] skips [[], [0], [1, 1], [0, 0, 1], [0, 0, 1, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.02550000000000017]\n",
      "Epoch 48, Iter 100 /703, loss: 0.748\n",
      "Epoch 48, Iter 200 /703, loss: 0.650\n",
      "Epoch 48, Iter 300 /703, loss: 0.638\n",
      "Epoch 48, Iter 400 /703, loss: 0.628\n",
      "Epoch 48, Iter 500 /703, loss: 0.584\n",
      "Epoch 48, Iter 600 /703, loss: 0.618\n",
      "Epoch 48, Iter 700 /703, loss: 0.579\n",
      "-------- train controller --------\n",
      "0 tensor(1.8661)\n",
      "1 tensor(2.2178)\n",
      "2 tensor(3.4454)\n",
      "3 tensor(3.7418)\n",
      "4 tensor(2.2908)\n",
      "5 tensor(3.0768)\n",
      "6 tensor(1.9098)\n",
      "7 tensor(3.4018)\n",
      "8 tensor(1.7557)\n",
      "9 tensor(3.4037)\n",
      "epoch: 48\n",
      "-------- train child --------\n",
      "ops [2, 1, 0, 0, 0, 1] skips [[], [0], [1, 0], [0, 0, 1], [0, 0, 1, 1], [1, 1, 1, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 49, Iter 100 /703, loss: 0.756\n",
      "Epoch 49, Iter 200 /703, loss: 0.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Iter 300 /703, loss: 0.692\n",
      "Epoch 49, Iter 400 /703, loss: 0.650\n",
      "Epoch 49, Iter 500 /703, loss: 0.630\n",
      "Epoch 49, Iter 600 /703, loss: 0.631\n",
      "Epoch 49, Iter 700 /703, loss: 0.585\n",
      "epoch: 49\n",
      "-------- train child --------\n",
      "ops [1, 1, 3, 3, 3, 3] skips [[], [1], [1, 0], [0, 1, 1], [1, 0, 0, 1], [0, 1, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 50, Iter 100 /703, loss: 0.699\n",
      "Epoch 50, Iter 200 /703, loss: 0.654\n",
      "Epoch 50, Iter 300 /703, loss: 0.656\n",
      "Epoch 50, Iter 400 /703, loss: 0.650\n",
      "Epoch 50, Iter 500 /703, loss: 0.619\n",
      "Epoch 50, Iter 600 /703, loss: 0.645\n",
      "Epoch 50, Iter 700 /703, loss: 0.610\n",
      "-------- train controller --------\n",
      "0 tensor(2.9015)\n",
      "1 tensor(2.4640)\n",
      "2 tensor(1.6525)\n",
      "3 tensor(2.0494)\n",
      "4 tensor(3.0844)\n",
      "5 tensor(2.3572)\n",
      "6 tensor(2.6121)\n",
      "7 tensor(2.5912)\n",
      "8 tensor(3.0621)\n",
      "9 tensor(3.2157)\n",
      "epoch: 50\n",
      "-------- train child --------\n",
      "ops [0, 3, 2, 3, 1, 0] skips [[], [1], [0, 0], [1, 1, 1], [0, 0, 0, 1], [0, 0, 1, 1, 1]]\n",
      "lr= [0.050000000000000266]\n",
      "Epoch 51, Iter 100 /703, loss: 0.656\n",
      "Epoch 51, Iter 200 /703, loss: 0.657\n",
      "Epoch 51, Iter 300 /703, loss: 0.633\n",
      "Epoch 51, Iter 400 /703, loss: 0.639\n",
      "Epoch 51, Iter 500 /703, loss: 0.599\n",
      "Epoch 51, Iter 600 /703, loss: 0.645\n",
      "Epoch 51, Iter 700 /703, loss: 0.600\n",
      "epoch: 51\n",
      "-------- train child --------\n",
      "ops [0, 2, 0, 1, 2, 0] skips [[], [1], [0, 1], [0, 1, 1], [0, 0, 1, 0], [1, 0, 0, 0, 1]]\n",
      "lr= [0.025500000000000182]\n",
      "Epoch 52, Iter 100 /703, loss: 0.631\n",
      "Epoch 52, Iter 200 /703, loss: 0.606\n",
      "Epoch 52, Iter 300 /703, loss: 0.587\n",
      "Epoch 52, Iter 400 /703, loss: 0.588\n",
      "Epoch 52, Iter 500 /703, loss: 0.560\n",
      "Epoch 52, Iter 600 /703, loss: 0.578\n",
      "Epoch 52, Iter 700 /703, loss: 0.535\n",
      "-------- train controller --------\n",
      "0 tensor(2.3606)\n",
      "1 tensor(2.3022)\n",
      "2 tensor(2.0098)\n",
      "3 tensor(2.1485)\n",
      "4 tensor(2.6139)\n",
      "5 tensor(2.4262)\n",
      "6 tensor(3.2379)\n",
      "7 tensor(1.8162)\n",
      "8 tensor(2.5819)\n",
      "9 tensor(2.2456)\n",
      "epoch: 52\n",
      "-------- train child --------\n",
      "ops [1, 2, 1, 3, 2, 1] skips [[], [1], [0, 0], [0, 0, 1], [0, 0, 1, 0], [1, 1, 0, 1, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 53, Iter 100 /703, loss: 0.760\n",
      "Epoch 53, Iter 200 /703, loss: 0.650\n",
      "Epoch 53, Iter 300 /703, loss: 0.649\n",
      "Epoch 53, Iter 400 /703, loss: 0.614\n",
      "Epoch 53, Iter 500 /703, loss: 0.576\n",
      "Epoch 53, Iter 600 /703, loss: 0.570\n",
      "Epoch 53, Iter 700 /703, loss: 0.513\n",
      "epoch: 53\n",
      "-------- train child --------\n",
      "ops [1, 0, 0, 1, 3, 2] skips [[], [0], [1, 0], [0, 0, 1], [0, 0, 0, 1], [1, 0, 0, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 54, Iter 100 /703, loss: 0.609\n",
      "Epoch 54, Iter 200 /703, loss: 0.589\n",
      "Epoch 54, Iter 300 /703, loss: 0.592\n",
      "Epoch 54, Iter 400 /703, loss: 0.581\n",
      "Epoch 54, Iter 500 /703, loss: 0.548\n",
      "Epoch 54, Iter 600 /703, loss: 0.581\n",
      "Epoch 54, Iter 700 /703, loss: 0.537\n",
      "-------- train controller --------\n",
      "0 tensor(2.4674)\n",
      "1 tensor(2.1009)\n",
      "2 tensor(2.2176)\n",
      "3 tensor(1.2604)\n",
      "4 tensor(2.2238)\n",
      "5 tensor(1.6026)\n",
      "6 tensor(0.5300)\n",
      "7 tensor(1.6766)\n",
      "8 tensor(1.8447)\n",
      "9 tensor(1.9913)\n",
      "epoch: 54\n",
      "-------- train child --------\n",
      "ops [2, 2, 2, 0, 3, 2] skips [[], [1], [0, 0], [1, 1, 1], [0, 1, 0, 0], [0, 0, 1, 0, 1]]\n",
      "lr= [0.04999999999999992]\n",
      "Epoch 55, Iter 100 /703, loss: 1.211\n",
      "Epoch 55, Iter 200 /703, loss: 0.979\n",
      "Epoch 55, Iter 300 /703, loss: 0.951\n",
      "Epoch 55, Iter 400 /703, loss: 0.903\n",
      "Epoch 55, Iter 500 /703, loss: 0.868\n",
      "Epoch 55, Iter 600 /703, loss: 0.890\n",
      "Epoch 55, Iter 700 /703, loss: 0.854\n",
      "epoch: 55\n",
      "-------- train child --------\n",
      "ops [0, 2, 1, 2, 3, 1] skips [[], [1], [0, 1], [0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.02550000000000019]\n",
      "Epoch 56, Iter 100 /703, loss: 0.732\n",
      "Epoch 56, Iter 200 /703, loss: 0.651\n",
      "Epoch 56, Iter 300 /703, loss: 0.626\n",
      "Epoch 56, Iter 400 /703, loss: 0.614\n",
      "Epoch 56, Iter 500 /703, loss: 0.574\n",
      "Epoch 56, Iter 600 /703, loss: 0.607\n",
      "Epoch 56, Iter 700 /703, loss: 0.562\n",
      "-------- train controller --------\n",
      "0 tensor(1.8085)\n",
      "1 tensor(2.5931)\n",
      "2 tensor(2.6198)\n",
      "3 tensor(2.6634)\n",
      "4 tensor(1.9048)\n",
      "5 tensor(2.0525)\n",
      "6 tensor(0.5638)\n",
      "7 tensor(2.3091)\n",
      "8 tensor(2.0803)\n",
      "9 tensor(2.5069)\n",
      "epoch: 56\n",
      "-------- train child --------\n",
      "ops [1, 1, 3, 1, 0, 2] skips [[], [1], [1, 1], [1, 1, 0], [1, 1, 0, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 57, Iter 100 /703, loss: 0.674\n",
      "Epoch 57, Iter 200 /703, loss: 0.624\n",
      "Epoch 57, Iter 300 /703, loss: 0.610\n",
      "Epoch 57, Iter 400 /703, loss: 0.595\n",
      "Epoch 57, Iter 500 /703, loss: 0.554\n",
      "Epoch 57, Iter 600 /703, loss: 0.563\n",
      "Epoch 57, Iter 700 /703, loss: 0.509\n",
      "epoch: 57\n",
      "-------- train child --------\n",
      "ops [3, 1, 1, 2, 3, 0] skips [[], [1], [0, 0], [0, 1, 1], [0, 1, 1, 0], [1, 0, 1, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 58, Iter 100 /703, loss: 0.752\n",
      "Epoch 58, Iter 200 /703, loss: 0.700\n",
      "Epoch 58, Iter 300 /703, loss: 0.700\n",
      "Epoch 58, Iter 400 /703, loss: 0.689\n",
      "Epoch 58, Iter 500 /703, loss: 0.656\n",
      "Epoch 58, Iter 600 /703, loss: 0.688\n",
      "Epoch 58, Iter 700 /703, loss: 0.648\n",
      "-------- train controller --------\n",
      "0 tensor(1.8529)\n",
      "1 tensor(2.2800)\n",
      "2 tensor(2.5448)\n",
      "3 tensor(1.8614)\n",
      "4 tensor(2.2995)\n",
      "5 tensor(1.6377)\n",
      "6 tensor(2.9550)\n",
      "7 tensor(1.4679)\n",
      "8 tensor(2.3348)\n",
      "9 tensor(1.8511)\n",
      "epoch: 58\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 0, 3, 0] skips [[], [0], [1, 1], [0, 0, 0], [1, 0, 0, 0], [1, 0, 0, 1, 0]]\n",
      "lr= [0.05000000000000029]\n",
      "Epoch 59, Iter 100 /703, loss: 0.843\n",
      "Epoch 59, Iter 200 /703, loss: 0.747\n",
      "Epoch 59, Iter 300 /703, loss: 0.735\n",
      "Epoch 59, Iter 400 /703, loss: 0.716\n",
      "Epoch 59, Iter 500 /703, loss: 0.682\n",
      "Epoch 59, Iter 600 /703, loss: 0.720\n",
      "Epoch 59, Iter 700 /703, loss: 0.684\n",
      "epoch: 59\n",
      "-------- train child --------\n",
      "ops [2, 2, 3, 0, 3, 3] skips [[], [0], [0, 0], [0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0, 0]]\n",
      "lr= [0.025500000000000203]\n",
      "Epoch 60, Iter 100 /703, loss: 1.067\n",
      "Epoch 60, Iter 200 /703, loss: 0.901\n",
      "Epoch 60, Iter 300 /703, loss: 0.882\n",
      "Epoch 60, Iter 400 /703, loss: 0.865\n",
      "Epoch 60, Iter 500 /703, loss: 0.837\n",
      "Epoch 60, Iter 600 /703, loss: 0.863\n",
      "Epoch 60, Iter 700 /703, loss: 0.825\n",
      "-------- train controller --------\n",
      "0 tensor(1.0514)\n",
      "1 tensor(1.3618)\n",
      "2 tensor(0.3280)\n",
      "3 tensor(0.8430)\n",
      "4 tensor(1.3811)\n",
      "5 tensor(1.6392)\n",
      "6 tensor(1.5657)\n",
      "7 tensor(1.5659)\n",
      "8 tensor(1.9063)\n",
      "9 tensor(1.0875)\n",
      "epoch: 60\n",
      "-------- train child --------\n",
      "ops [3, 2, 0, 1, 2, 1] skips [[], [0], [0, 1], [0, 0, 0], [0, 0, 0, 1], [1, 1, 1, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 61, Iter 100 /703, loss: 1.047\n",
      "Epoch 61, Iter 200 /703, loss: 0.894\n",
      "Epoch 61, Iter 300 /703, loss: 0.873\n",
      "Epoch 61, Iter 400 /703, loss: 0.828\n",
      "Epoch 61, Iter 500 /703, loss: 0.791\n",
      "Epoch 61, Iter 600 /703, loss: 0.803\n",
      "Epoch 61, Iter 700 /703, loss: 0.757\n",
      "epoch: 61\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 2, 2, 1] skips [[], [1], [0, 0], [1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 62, Iter 100 /703, loss: 0.899\n",
      "Epoch 62, Iter 200 /703, loss: 0.786\n",
      "Epoch 62, Iter 300 /703, loss: 0.766\n",
      "Epoch 62, Iter 400 /703, loss: 0.761\n",
      "Epoch 62, Iter 500 /703, loss: 0.713\n",
      "Epoch 62, Iter 600 /703, loss: 0.747\n",
      "Epoch 62, Iter 700 /703, loss: 0.709\n",
      "-------- train controller --------\n",
      "0 tensor(0.7824)\n",
      "1 tensor(1.6256)\n",
      "2 tensor(0.9164)\n",
      "3 tensor(0.7088)\n",
      "4 tensor(2.2824)\n",
      "5 tensor(0.2945)\n",
      "6 tensor(0.3142)\n",
      "7 tensor(1.0950)\n",
      "8 tensor(0.3558)\n",
      "9 tensor(1.2768)\n",
      "epoch: 62\n",
      "-------- train child --------\n",
      "ops [0, 2, 1, 0, 1, 3] skips [[], [0], [1, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 1, 1, 1]]\n",
      "lr= [0.049999999999999954]\n",
      "Epoch 63, Iter 100 /703, loss: 0.785\n",
      "Epoch 63, Iter 200 /703, loss: 0.680\n",
      "Epoch 63, Iter 300 /703, loss: 0.655\n",
      "Epoch 63, Iter 400 /703, loss: 0.641\n",
      "Epoch 63, Iter 500 /703, loss: 0.612\n",
      "Epoch 63, Iter 600 /703, loss: 0.639\n",
      "Epoch 63, Iter 700 /703, loss: 0.601\n",
      "epoch: 63\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 2, 1, 3] skips [[], [0], [0, 1], [1, 1, 0], [1, 1, 0, 0], [0, 1, 1, 1, 0]]\n",
      "lr= [0.025500000000000217]\n",
      "Epoch 64, Iter 100 /703, loss: 0.796\n",
      "Epoch 64, Iter 200 /703, loss: 0.708\n",
      "Epoch 64, Iter 300 /703, loss: 0.718\n",
      "Epoch 64, Iter 400 /703, loss: 0.695\n",
      "Epoch 64, Iter 500 /703, loss: 0.658\n",
      "Epoch 64, Iter 600 /703, loss: 0.695\n",
      "Epoch 64, Iter 700 /703, loss: 0.651\n",
      "-------- train controller --------\n",
      "0 tensor(2.0264)\n",
      "1 tensor(2.4194)\n",
      "2 tensor(1.5364)\n",
      "3 tensor(0.2792)\n",
      "4 tensor(0.7074)\n",
      "5 tensor(1.0923)\n",
      "6 tensor(1.0285)\n",
      "7 tensor(1.0277)\n",
      "8 tensor(2.5949)\n",
      "9 tensor(0.4324)\n",
      "epoch: 64\n",
      "-------- train child --------\n",
      "ops [0, 0, 0, 1, 1, 2] skips [[], [0], [1, 1], [0, 1, 0], [0, 0, 1, 0], [1, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 65, Iter 100 /703, loss: 0.774\n",
      "Epoch 65, Iter 200 /703, loss: 0.709\n",
      "Epoch 65, Iter 300 /703, loss: 0.668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Iter 400 /703, loss: 0.648\n",
      "Epoch 65, Iter 500 /703, loss: 0.621\n",
      "Epoch 65, Iter 600 /703, loss: 0.619\n",
      "Epoch 65, Iter 700 /703, loss: 0.573\n",
      "epoch: 65\n",
      "-------- train child --------\n",
      "ops [3, 3, 0, 1, 3, 1] skips [[], [1], [0, 0], [1, 1, 0], [0, 0, 1, 0], [1, 0, 1, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 66, Iter 100 /703, loss: 1.025\n",
      "Epoch 66, Iter 200 /703, loss: 0.826\n",
      "Epoch 66, Iter 300 /703, loss: 0.818\n",
      "Epoch 66, Iter 400 /703, loss: 0.783\n",
      "Epoch 66, Iter 500 /703, loss: 0.736\n",
      "Epoch 66, Iter 600 /703, loss: 0.776\n",
      "Epoch 66, Iter 700 /703, loss: 0.743\n",
      "-------- train controller --------\n",
      "0 tensor(2.4616)\n",
      "1 tensor(1.0468)\n",
      "2 tensor(-0.3730)\n",
      "3 tensor(1.7597)\n",
      "4 tensor(1.7789)\n",
      "5 tensor(1.9519)\n",
      "6 tensor(1.3275)\n",
      "7 tensor(1.9134)\n",
      "8 tensor(1.0242)\n",
      "9 tensor(1.0040)\n",
      "epoch: 66\n",
      "-------- train child --------\n",
      "ops [1, 2, 0, 0, 0, 0] skips [[], [0], [0, 1], [1, 0, 0], [1, 1, 0, 0], [0, 0, 1, 1, 0]]\n",
      "lr= [0.05000000000000031]\n",
      "Epoch 67, Iter 100 /703, loss: 0.722\n",
      "Epoch 67, Iter 200 /703, loss: 0.682\n",
      "Epoch 67, Iter 300 /703, loss: 0.659\n",
      "Epoch 67, Iter 400 /703, loss: 0.651\n",
      "Epoch 67, Iter 500 /703, loss: 0.623\n",
      "Epoch 67, Iter 600 /703, loss: 0.658\n",
      "Epoch 67, Iter 700 /703, loss: 0.613\n",
      "epoch: 67\n",
      "-------- train child --------\n",
      "ops [2, 3, 0, 3, 0, 3] skips [[], [0], [1, 1], [0, 0, 0], [0, 0, 0, 1], [1, 0, 1, 0, 1]]\n",
      "lr= [0.025500000000000224]\n",
      "Epoch 68, Iter 100 /703, loss: 0.997\n",
      "Epoch 68, Iter 200 /703, loss: 0.838\n",
      "Epoch 68, Iter 300 /703, loss: 0.822\n",
      "Epoch 68, Iter 400 /703, loss: 0.788\n",
      "Epoch 68, Iter 500 /703, loss: 0.747\n",
      "Epoch 68, Iter 600 /703, loss: 0.784\n",
      "Epoch 68, Iter 700 /703, loss: 0.739\n",
      "-------- train controller --------\n",
      "0 tensor(1.2985)\n",
      "1 tensor(0.8077)\n",
      "2 tensor(1.5822)\n",
      "3 tensor(1.3217)\n",
      "4 tensor(2.0494)\n",
      "5 tensor(2.1099)\n",
      "6 tensor(1.2090)\n",
      "7 tensor(1.0748)\n",
      "8 tensor(1.7736)\n",
      "9 tensor(2.7697)\n",
      "epoch: 68\n",
      "-------- train child --------\n",
      "ops [3, 2, 1, 0, 1, 1] skips [[], [1], [0, 0], [0, 0, 1], [0, 0, 0, 0], [1, 1, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 69, Iter 100 /703, loss: 1.068\n",
      "Epoch 69, Iter 200 /703, loss: 0.879\n",
      "Epoch 69, Iter 300 /703, loss: 0.839\n",
      "Epoch 69, Iter 400 /703, loss: 0.796\n",
      "Epoch 69, Iter 500 /703, loss: 0.752\n",
      "Epoch 69, Iter 600 /703, loss: 0.763\n",
      "Epoch 69, Iter 700 /703, loss: 0.713\n",
      "epoch: 69\n",
      "-------- train child --------\n",
      "ops [1, 0, 0, 3, 2, 0] skips [[], [0], [1, 1], [0, 0, 0], [1, 0, 1, 0], [0, 0, 1, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 70, Iter 100 /703, loss: 0.619\n",
      "Epoch 70, Iter 200 /703, loss: 0.577\n",
      "Epoch 70, Iter 300 /703, loss: 0.563\n",
      "Epoch 70, Iter 400 /703, loss: 0.562\n",
      "Epoch 70, Iter 500 /703, loss: 0.518\n",
      "Epoch 70, Iter 600 /703, loss: 0.562\n",
      "Epoch 70, Iter 700 /703, loss: 0.506\n",
      "-------- train controller --------\n",
      "0 tensor(1.9298)\n",
      "1 tensor(0.9830)\n",
      "2 tensor(2.2739)\n",
      "3 tensor(0.3953)\n",
      "4 tensor(1.6409)\n",
      "5 tensor(2.2298)\n",
      "6 tensor(1.8611)\n",
      "7 tensor(0.8412)\n",
      "8 tensor(1.1424)\n",
      "9 tensor(1.3363)\n",
      "epoch: 70\n",
      "-------- train child --------\n",
      "ops [2, 2, 2, 1, 1, 1] skips [[], [0], [0, 0], [0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 0, 1]]\n",
      "lr= [0.049999999999999975]\n",
      "Epoch 71, Iter 100 /703, loss: 0.885\n",
      "Epoch 71, Iter 200 /703, loss: 0.778\n",
      "Epoch 71, Iter 300 /703, loss: 0.749\n",
      "Epoch 71, Iter 400 /703, loss: 0.731\n",
      "Epoch 71, Iter 500 /703, loss: 0.695\n",
      "Epoch 71, Iter 600 /703, loss: 0.719\n",
      "Epoch 71, Iter 700 /703, loss: 0.694\n",
      "epoch: 71\n",
      "-------- train child --------\n",
      "ops [3, 2, 0, 1, 1, 3] skips [[], [0], [0, 1], [0, 0, 0], [0, 0, 1, 1], [0, 1, 1, 0, 0]]\n",
      "lr= [0.025500000000000238]\n",
      "Epoch 72, Iter 100 /703, loss: 0.834\n",
      "Epoch 72, Iter 200 /703, loss: 0.740\n",
      "Epoch 72, Iter 300 /703, loss: 0.729\n",
      "Epoch 72, Iter 400 /703, loss: 0.706\n",
      "Epoch 72, Iter 500 /703, loss: 0.675\n",
      "Epoch 72, Iter 600 /703, loss: 0.709\n",
      "Epoch 72, Iter 700 /703, loss: 0.680\n",
      "-------- train controller --------\n",
      "0 tensor(2.1065)\n",
      "1 tensor(1.4761)\n",
      "2 tensor(1.6710)\n",
      "3 tensor(1.7699)\n",
      "4 tensor(2.1217)\n",
      "5 tensor(0.8762)\n",
      "6 tensor(1.2098)\n",
      "7 tensor(1.7667)\n",
      "8 tensor(2.2967)\n",
      "9 tensor(1.2630)\n",
      "epoch: 72\n",
      "-------- train child --------\n",
      "ops [2, 2, 2, 3, 2, 3] skips [[], [0], [0, 0], [1, 1, 0], [0, 1, 1, 1], [1, 0, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 73, Iter 100 /703, loss: 1.584\n",
      "Epoch 73, Iter 200 /703, loss: 1.345\n",
      "Epoch 73, Iter 300 /703, loss: 1.268\n",
      "Epoch 73, Iter 400 /703, loss: 1.193\n",
      "Epoch 73, Iter 500 /703, loss: 1.154\n",
      "Epoch 73, Iter 600 /703, loss: 1.176\n",
      "Epoch 73, Iter 700 /703, loss: 1.129\n",
      "epoch: 73\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 0, 3, 0] skips [[], [0], [1, 0], [0, 0, 0], [0, 0, 1, 1], [0, 1, 1, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 74, Iter 100 /703, loss: 0.734\n",
      "Epoch 74, Iter 200 /703, loss: 0.665\n",
      "Epoch 74, Iter 300 /703, loss: 0.646\n",
      "Epoch 74, Iter 400 /703, loss: 0.634\n",
      "Epoch 74, Iter 500 /703, loss: 0.595\n",
      "Epoch 74, Iter 600 /703, loss: 0.622\n",
      "Epoch 74, Iter 700 /703, loss: 0.592\n",
      "-------- train controller --------\n",
      "0 tensor(2.2786)\n",
      "1 tensor(2.1905)\n",
      "2 tensor(1.6491)\n",
      "3 tensor(1.6521)\n",
      "4 tensor(1.8799)\n",
      "5 tensor(1.0981)\n",
      "6 tensor(2.4147)\n",
      "7 tensor(1.5930)\n",
      "8 tensor(1.5574)\n",
      "9 tensor(1.2889)\n",
      "epoch: 74\n",
      "-------- train child --------\n",
      "ops [3, 1, 2, 3, 2, 2] skips [[], [0], [0, 1], [0, 0, 1], [0, 0, 1, 1], [1, 0, 1, 0, 1]]\n",
      "lr= [0.049999999999999635]\n",
      "Epoch 75, Iter 100 /703, loss: 0.921\n",
      "Epoch 75, Iter 200 /703, loss: 0.844\n",
      "Epoch 75, Iter 300 /703, loss: 0.825\n",
      "Epoch 75, Iter 400 /703, loss: 0.808\n",
      "Epoch 75, Iter 500 /703, loss: 0.765\n",
      "Epoch 75, Iter 600 /703, loss: 0.808\n",
      "Epoch 75, Iter 700 /703, loss: 0.758\n",
      "epoch: 75\n",
      "-------- train child --------\n",
      "ops [0, 1, 1, 0, 1, 3] skips [[], [0], [0, 0], [1, 0, 0], [0, 0, 1, 1], [0, 1, 0, 1, 0]]\n",
      "lr= [0.0254999999999999]\n",
      "Epoch 76, Iter 100 /703, loss: 0.699\n",
      "Epoch 76, Iter 200 /703, loss: 0.622\n",
      "Epoch 76, Iter 300 /703, loss: 0.608\n",
      "Epoch 76, Iter 400 /703, loss: 0.586\n",
      "Epoch 76, Iter 500 /703, loss: 0.561\n",
      "Epoch 76, Iter 600 /703, loss: 0.591\n",
      "Epoch 76, Iter 700 /703, loss: 0.544\n",
      "-------- train controller --------\n",
      "0 tensor(1.6037)\n",
      "1 tensor(1.8230)\n",
      "2 tensor(1.7973)\n",
      "3 tensor(1.1584)\n",
      "4 tensor(0.7441)\n",
      "5 tensor(1.6171)\n",
      "6 tensor(2.1323)\n",
      "7 tensor(1.7986)\n",
      "8 tensor(0.6839)\n",
      "9 tensor(1.8828)\n",
      "epoch: 76\n",
      "-------- train child --------\n",
      "ops [0, 1, 2, 1, 2, 1] skips [[], [0], [0, 0], [0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 77, Iter 100 /703, loss: 1.156\n",
      "Epoch 77, Iter 200 /703, loss: 0.936\n",
      "Epoch 77, Iter 300 /703, loss: 0.894\n",
      "Epoch 77, Iter 400 /703, loss: 0.829\n",
      "Epoch 77, Iter 500 /703, loss: 0.762\n",
      "Epoch 77, Iter 600 /703, loss: 0.778\n",
      "Epoch 77, Iter 700 /703, loss: 0.727\n",
      "epoch: 77\n",
      "-------- train child --------\n",
      "ops [2, 1, 0, 3, 0, 1] skips [[], [1], [0, 0], [0, 1, 1], [0, 1, 0, 1], [1, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 78, Iter 100 /703, loss: 0.703\n",
      "Epoch 78, Iter 200 /703, loss: 0.657\n",
      "Epoch 78, Iter 300 /703, loss: 0.642\n",
      "Epoch 78, Iter 400 /703, loss: 0.634\n",
      "Epoch 78, Iter 500 /703, loss: 0.583\n",
      "Epoch 78, Iter 600 /703, loss: 0.619\n",
      "Epoch 78, Iter 700 /703, loss: 0.584\n",
      "-------- train controller --------\n",
      "0 tensor(1.1213)\n",
      "1 tensor(2.0881)\n",
      "2 tensor(1.7125)\n",
      "3 tensor(1.6655)\n",
      "4 tensor(0.5737)\n",
      "5 tensor(1.9103)\n",
      "6 tensor(0.2624)\n",
      "7 tensor(1.6167)\n",
      "8 tensor(2.2113)\n",
      "9 tensor(1.5680)\n",
      "epoch: 78\n",
      "-------- train child --------\n",
      "ops [0, 3, 1, 0, 3, 2] skips [[], [0], [0, 0], [1, 0, 1], [1, 0, 1, 0], [1, 1, 0, 1, 1]]\n",
      "lr= [0.05000000000000069]\n",
      "Epoch 79, Iter 100 /703, loss: 0.731\n",
      "Epoch 79, Iter 200 /703, loss: 0.689\n",
      "Epoch 79, Iter 300 /703, loss: 0.673\n",
      "Epoch 79, Iter 400 /703, loss: 0.647\n",
      "Epoch 79, Iter 500 /703, loss: 0.640\n",
      "Epoch 79, Iter 600 /703, loss: 0.681\n",
      "Epoch 79, Iter 700 /703, loss: 0.623\n",
      "epoch: 79\n",
      "-------- train child --------\n",
      "ops [2, 1, 1, 1, 2, 0] skips [[], [1], [1, 1], [0, 0, 1], [1, 0, 0, 1], [0, 1, 0, 0, 1]]\n",
      "lr= [0.025500000000000262]\n",
      "Epoch 80, Iter 100 /703, loss: 0.663\n",
      "Epoch 80, Iter 200 /703, loss: 0.627\n",
      "Epoch 80, Iter 300 /703, loss: 0.614\n",
      "Epoch 80, Iter 400 /703, loss: 0.613\n",
      "Epoch 80, Iter 500 /703, loss: 0.564\n",
      "Epoch 80, Iter 600 /703, loss: 0.594\n",
      "Epoch 80, Iter 700 /703, loss: 0.560\n",
      "-------- train controller --------\n",
      "0 tensor(2.3025)\n",
      "1 tensor(1.7084)\n",
      "2 tensor(1.7654)\n",
      "3 tensor(2.0261)\n",
      "4 tensor(1.8840)\n",
      "5 tensor(1.4861)\n",
      "6 tensor(1.1782)\n",
      "7 tensor(1.2778)\n",
      "8 tensor(2.3923)\n",
      "9 tensor(1.2092)\n",
      "epoch: 80\n",
      "-------- train child --------\n",
      "ops [3, 1, 1, 0, 1, 3] skips [[], [1], [0, 0], [1, 1, 0], [0, 0, 0, 0], [0, 1, 1, 1, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 81, Iter 100 /703, loss: 0.721\n",
      "Epoch 81, Iter 200 /703, loss: 0.670\n",
      "Epoch 81, Iter 300 /703, loss: 0.681\n",
      "Epoch 81, Iter 400 /703, loss: 0.638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Iter 500 /703, loss: 0.606\n",
      "Epoch 81, Iter 600 /703, loss: 0.610\n",
      "Epoch 81, Iter 700 /703, loss: 0.560\n",
      "epoch: 81\n",
      "-------- train child --------\n",
      "ops [3, 2, 3, 1, 3, 3] skips [[], [0], [1, 0], [0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 82, Iter 100 /703, loss: 1.056\n",
      "Epoch 82, Iter 200 /703, loss: 0.884\n",
      "Epoch 82, Iter 300 /703, loss: 0.863\n",
      "Epoch 82, Iter 400 /703, loss: 0.837\n",
      "Epoch 82, Iter 500 /703, loss: 0.787\n",
      "Epoch 82, Iter 600 /703, loss: 0.822\n",
      "Epoch 82, Iter 700 /703, loss: 0.776\n",
      "-------- train controller --------\n",
      "0 tensor(1.1605)\n",
      "1 tensor(1.1083)\n",
      "2 tensor(2.1533)\n",
      "3 tensor(1.4851)\n",
      "4 tensor(1.4354)\n",
      "5 tensor(1.7419)\n",
      "6 tensor(-0.0729)\n",
      "7 tensor(0.6829)\n",
      "8 tensor(1.1741)\n",
      "9 tensor(0.7207)\n",
      "epoch: 82\n",
      "-------- train child --------\n",
      "ops [3, 1, 3, 1, 0, 2] skips [[], [1], [0, 0], [0, 0, 1], [0, 0, 0, 0], [0, 0, 1, 0, 0]]\n",
      "lr= [0.05000000000000036]\n",
      "Epoch 83, Iter 100 /703, loss: 0.821\n",
      "Epoch 83, Iter 200 /703, loss: 0.768\n",
      "Epoch 83, Iter 300 /703, loss: 0.759\n",
      "Epoch 83, Iter 400 /703, loss: 0.745\n",
      "Epoch 83, Iter 500 /703, loss: 0.707\n",
      "Epoch 83, Iter 600 /703, loss: 0.739\n",
      "Epoch 83, Iter 700 /703, loss: 0.709\n",
      "epoch: 83\n",
      "-------- train child --------\n",
      "ops [1, 0, 1, 3, 0, 2] skips [[], [1], [0, 0], [0, 0, 1], [1, 0, 1, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.025500000000000626]\n",
      "Epoch 84, Iter 100 /703, loss: 0.773\n",
      "Epoch 84, Iter 200 /703, loss: 0.639\n",
      "Epoch 84, Iter 300 /703, loss: 0.620\n",
      "Epoch 84, Iter 400 /703, loss: 0.606\n",
      "Epoch 84, Iter 500 /703, loss: 0.566\n",
      "Epoch 84, Iter 600 /703, loss: 0.600\n",
      "Epoch 84, Iter 700 /703, loss: 0.551\n",
      "-------- train controller --------\n",
      "0 tensor(1.8974)\n",
      "1 tensor(-0.5555)\n",
      "2 tensor(1.2514)\n",
      "3 tensor(0.8320)\n",
      "4 tensor(0.6947)\n",
      "5 tensor(1.4421)\n",
      "6 tensor(-0.1008)\n",
      "7 tensor(1.1938)\n",
      "8 tensor(1.1724)\n",
      "9 tensor(1.6035)\n",
      "epoch: 84\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 0, 1, 3] skips [[], [0], [0, 0], [0, 0, 0], [0, 1, 0, 1], [0, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 85, Iter 100 /703, loss: 1.099\n",
      "Epoch 85, Iter 200 /703, loss: 0.875\n",
      "Epoch 85, Iter 300 /703, loss: 0.824\n",
      "Epoch 85, Iter 400 /703, loss: 0.779\n",
      "Epoch 85, Iter 500 /703, loss: 0.723\n",
      "Epoch 85, Iter 600 /703, loss: 0.754\n",
      "Epoch 85, Iter 700 /703, loss: 0.691\n",
      "epoch: 85\n",
      "-------- train child --------\n",
      "ops [3, 0, 0, 3, 3, 0] skips [[], [0], [1, 1], [1, 1, 1], [0, 1, 0, 0], [0, 0, 1, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 86, Iter 100 /703, loss: 0.762\n",
      "Epoch 86, Iter 200 /703, loss: 0.680\n",
      "Epoch 86, Iter 300 /703, loss: 0.677\n",
      "Epoch 86, Iter 400 /703, loss: 0.674\n",
      "Epoch 86, Iter 500 /703, loss: 0.636\n",
      "Epoch 86, Iter 600 /703, loss: 0.675\n",
      "Epoch 86, Iter 700 /703, loss: 0.632\n",
      "-------- train controller --------\n",
      "0 tensor(1.2789)\n",
      "1 tensor(0.6601)\n",
      "2 tensor(1.7790)\n",
      "3 tensor(0.8024)\n",
      "4 tensor(1.8837)\n",
      "5 tensor(1.2330)\n",
      "6 tensor(1.5252)\n",
      "7 tensor(0.6040)\n",
      "8 tensor(1.9686)\n",
      "9 tensor(1.1722)\n",
      "epoch: 86\n",
      "-------- train child --------\n",
      "ops [1, 0, 0, 3, 2, 2] skips [[], [0], [0, 0], [1, 1, 1], [1, 1, 0, 0], [0, 1, 0, 0, 0]]\n",
      "lr= [0.05000000000000002]\n",
      "Epoch 87, Iter 100 /703, loss: 0.741\n",
      "Epoch 87, Iter 200 /703, loss: 0.685\n",
      "Epoch 87, Iter 300 /703, loss: 0.664\n",
      "Epoch 87, Iter 400 /703, loss: 0.667\n",
      "Epoch 87, Iter 500 /703, loss: 0.631\n",
      "Epoch 87, Iter 600 /703, loss: 0.676\n",
      "Epoch 87, Iter 700 /703, loss: 0.617\n",
      "epoch: 87\n",
      "-------- train child --------\n",
      "ops [1, 3, 3, 0, 2, 1] skips [[], [1], [0, 0], [1, 1, 0], [1, 1, 1, 1], [0, 0, 0, 0, 1]]\n",
      "lr= [0.025500000000000286]\n",
      "Epoch 88, Iter 100 /703, loss: 0.611\n",
      "Epoch 88, Iter 200 /703, loss: 0.570\n",
      "Epoch 88, Iter 300 /703, loss: 0.552\n",
      "Epoch 88, Iter 400 /703, loss: 0.553\n",
      "Epoch 88, Iter 500 /703, loss: 0.523\n",
      "Epoch 88, Iter 600 /703, loss: 0.551\n",
      "Epoch 88, Iter 700 /703, loss: 0.508\n",
      "-------- train controller --------\n",
      "0 tensor(0.9498)\n",
      "1 tensor(1.8011)\n",
      "2 tensor(1.3797)\n",
      "3 tensor(0.3889)\n",
      "4 tensor(1.3106)\n",
      "5 tensor(-0.0285)\n",
      "6 tensor(-0.3974)\n",
      "7 tensor(1.8093)\n",
      "8 tensor(1.1901)\n",
      "9 tensor(1.1052)\n",
      "epoch: 88\n",
      "-------- train child --------\n",
      "ops [1, 1, 1, 1, 2, 1] skips [[], [1], [0, 0], [0, 1, 0], [0, 0, 1, 0], [1, 1, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 89, Iter 100 /703, loss: 0.660\n",
      "Epoch 89, Iter 200 /703, loss: 0.597\n",
      "Epoch 89, Iter 300 /703, loss: 0.573\n",
      "Epoch 89, Iter 400 /703, loss: 0.553\n",
      "Epoch 89, Iter 500 /703, loss: 0.519\n",
      "Epoch 89, Iter 600 /703, loss: 0.518\n",
      "Epoch 89, Iter 700 /703, loss: 0.465\n",
      "epoch: 89\n",
      "-------- train child --------\n",
      "ops [1, 3, 3, 2, 3, 1] skips [[], [1], [1, 1], [1, 0, 0], [0, 0, 1, 1], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 90, Iter 100 /703, loss: 0.526\n",
      "Epoch 90, Iter 200 /703, loss: 0.510\n",
      "Epoch 90, Iter 300 /703, loss: 0.509\n",
      "Epoch 90, Iter 400 /703, loss: 0.512\n",
      "Epoch 90, Iter 500 /703, loss: 0.484\n",
      "Epoch 90, Iter 600 /703, loss: 0.513\n",
      "Epoch 90, Iter 700 /703, loss: 0.467\n",
      "-------- train controller --------\n",
      "0 tensor(0.0695)\n",
      "1 tensor(1.0629)\n",
      "2 tensor(1.5312)\n",
      "3 tensor(0.2381)\n",
      "4 tensor(1.5198)\n",
      "5 tensor(1.3486)\n",
      "6 tensor(0.4845)\n",
      "7 tensor(1.2974)\n",
      "8 tensor(0.3925)\n",
      "9 tensor(-0.3193)\n",
      "epoch: 90\n",
      "-------- train child --------\n",
      "ops [1, 3, 0, 2, 0, 0] skips [[], [1], [1, 0], [1, 1, 0], [0, 0, 0, 1], [0, 1, 1, 0, 1]]\n",
      "lr= [0.049999999999999684]\n",
      "Epoch 91, Iter 100 /703, loss: 0.587\n",
      "Epoch 91, Iter 200 /703, loss: 0.592\n",
      "Epoch 91, Iter 300 /703, loss: 0.586\n",
      "Epoch 91, Iter 400 /703, loss: 0.572\n",
      "Epoch 91, Iter 500 /703, loss: 0.552\n",
      "Epoch 91, Iter 600 /703, loss: 0.586\n",
      "Epoch 91, Iter 700 /703, loss: 0.538\n",
      "epoch: 91\n",
      "-------- train child --------\n",
      "ops [0, 0, 2, 1, 0, 1] skips [[], [0], [0, 0], [1, 1, 1], [0, 0, 1, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.02549999999999995]\n",
      "Epoch 92, Iter 100 /703, loss: 0.761\n",
      "Epoch 92, Iter 200 /703, loss: 0.643\n",
      "Epoch 92, Iter 300 /703, loss: 0.631\n",
      "Epoch 92, Iter 400 /703, loss: 0.614\n",
      "Epoch 92, Iter 500 /703, loss: 0.572\n",
      "Epoch 92, Iter 600 /703, loss: 0.607\n",
      "Epoch 92, Iter 700 /703, loss: 0.557\n",
      "-------- train controller --------\n",
      "0 tensor(-0.7194)\n",
      "1 tensor(0.1187)\n",
      "2 tensor(1.7571)\n",
      "3 tensor(0.1714)\n",
      "4 tensor(1.6034)\n",
      "5 tensor(1.6723)\n",
      "6 tensor(0.5638)\n",
      "7 tensor(0.6632)\n",
      "8 tensor(1.0889)\n",
      "9 tensor(1.9307)\n",
      "epoch: 92\n",
      "-------- train child --------\n",
      "ops [0, 2, 0, 0, 2, 1] skips [[], [0], [0, 0], [0, 0, 0], [1, 0, 1, 0], [1, 0, 0, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 93, Iter 100 /703, loss: 0.618\n",
      "Epoch 93, Iter 200 /703, loss: 0.577\n",
      "Epoch 93, Iter 300 /703, loss: 0.562\n",
      "Epoch 93, Iter 400 /703, loss: 0.545\n",
      "Epoch 93, Iter 500 /703, loss: 0.506\n",
      "Epoch 93, Iter 600 /703, loss: 0.518\n",
      "Epoch 93, Iter 700 /703, loss: 0.461\n",
      "epoch: 93\n",
      "-------- train child --------\n",
      "ops [3, 2, 3, 0, 2, 0] skips [[], [0], [0, 1], [0, 0, 0], [0, 1, 0, 0], [1, 1, 1, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 94, Iter 100 /703, loss: 1.004\n",
      "Epoch 94, Iter 200 /703, loss: 0.821\n",
      "Epoch 94, Iter 300 /703, loss: 0.807\n",
      "Epoch 94, Iter 400 /703, loss: 0.770\n",
      "Epoch 94, Iter 500 /703, loss: 0.734\n",
      "Epoch 94, Iter 600 /703, loss: 0.773\n",
      "Epoch 94, Iter 700 /703, loss: 0.736\n",
      "-------- train controller --------\n",
      "0 tensor(1.1929)\n",
      "1 tensor(1.3481)\n",
      "2 tensor(1.3251)\n",
      "3 tensor(0.7202)\n",
      "4 tensor(0.2708)\n",
      "5 tensor(1.5777)\n",
      "6 tensor(0.7048)\n",
      "7 tensor(0.8719)\n",
      "8 tensor(0.4964)\n",
      "9 tensor(1.1931)\n",
      "epoch: 94\n",
      "-------- train child --------\n",
      "ops [3, 0, 1, 3, 0, 1] skips [[], [0], [1, 1], [1, 1, 1], [1, 0, 0, 1], [0, 1, 0, 0, 1]]\n",
      "lr= [0.050000000000000745]\n",
      "Epoch 95, Iter 100 /703, loss: 0.742\n",
      "Epoch 95, Iter 200 /703, loss: 0.684\n",
      "Epoch 95, Iter 300 /703, loss: 0.689\n",
      "Epoch 95, Iter 400 /703, loss: 0.673\n",
      "Epoch 95, Iter 500 /703, loss: 0.633\n",
      "Epoch 95, Iter 600 /703, loss: 0.675\n",
      "Epoch 95, Iter 700 /703, loss: 0.638\n",
      "epoch: 95\n",
      "-------- train child --------\n",
      "ops [0, 1, 1, 1, 0, 1] skips [[], [0], [0, 0], [0, 1, 0], [0, 1, 1, 1], [1, 0, 0, 0, 0]]\n",
      "lr= [0.025500000000000314]\n",
      "Epoch 96, Iter 100 /703, loss: 0.678\n",
      "Epoch 96, Iter 200 /703, loss: 0.611\n",
      "Epoch 96, Iter 300 /703, loss: 0.578\n",
      "Epoch 96, Iter 400 /703, loss: 0.568\n",
      "Epoch 96, Iter 500 /703, loss: 0.539\n",
      "Epoch 96, Iter 600 /703, loss: 0.556\n",
      "Epoch 96, Iter 700 /703, loss: 0.519\n",
      "-------- train controller --------\n",
      "0 tensor(1.0523)\n",
      "1 tensor(0.1317)\n",
      "2 tensor(0.8916)\n",
      "3 tensor(1.1117)\n",
      "4 tensor(1.4641)\n",
      "5 tensor(1.0163)\n",
      "6 tensor(0.8788)\n",
      "7 tensor(0.1829)\n",
      "8 tensor(0.6614)\n",
      "9 tensor(1.7293)\n",
      "epoch: 96\n",
      "-------- train child --------\n",
      "ops [0, 0, 2, 2, 1, 1] skips [[], [1], [1, 0], [1, 0, 0], [0, 1, 0, 1], [0, 1, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 97, Iter 100 /703, loss: 0.583\n",
      "Epoch 97, Iter 200 /703, loss: 0.547\n",
      "Epoch 97, Iter 300 /703, loss: 0.527\n",
      "Epoch 97, Iter 400 /703, loss: 0.516\n",
      "Epoch 97, Iter 500 /703, loss: 0.490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Iter 600 /703, loss: 0.502\n",
      "Epoch 97, Iter 700 /703, loss: 0.446\n",
      "epoch: 97\n",
      "-------- train child --------\n",
      "ops [1, 1, 1, 0, 2, 0] skips [[], [0], [0, 1], [0, 1, 1], [1, 1, 1, 0], [0, 1, 0, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 98, Iter 100 /703, loss: 0.569\n",
      "Epoch 98, Iter 200 /703, loss: 0.546\n",
      "Epoch 98, Iter 300 /703, loss: 0.532\n",
      "Epoch 98, Iter 400 /703, loss: 0.524\n",
      "Epoch 98, Iter 500 /703, loss: 0.483\n",
      "Epoch 98, Iter 600 /703, loss: 0.511\n",
      "Epoch 98, Iter 700 /703, loss: 0.466\n",
      "-------- train controller --------\n",
      "0 tensor(0.0828)\n",
      "1 tensor(1.7196)\n",
      "2 tensor(0.6253)\n",
      "3 tensor(0.5960)\n",
      "4 tensor(0.6367)\n",
      "5 tensor(0.5456)\n",
      "6 tensor(1.4888)\n",
      "7 tensor(0.4592)\n",
      "8 tensor(1.3822)\n",
      "9 tensor(-0.1939)\n",
      "epoch: 98\n",
      "-------- train child --------\n",
      "ops [3, 0, 0, 0, 1, 1] skips [[], [0], [1, 0], [0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.05000000000000041]\n",
      "Epoch 99, Iter 100 /703, loss: 0.804\n",
      "Epoch 99, Iter 200 /703, loss: 0.705\n",
      "Epoch 99, Iter 300 /703, loss: 0.698\n",
      "Epoch 99, Iter 400 /703, loss: 0.673\n",
      "Epoch 99, Iter 500 /703, loss: 0.635\n",
      "Epoch 99, Iter 600 /703, loss: 0.655\n",
      "Epoch 99, Iter 700 /703, loss: 0.634\n",
      "epoch: 99\n",
      "-------- train child --------\n",
      "ops [1, 0, 3, 0, 1, 1] skips [[], [0], [0, 1], [0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.025500000000000675]\n",
      "Epoch 100, Iter 100 /703, loss: 0.577\n",
      "Epoch 100, Iter 200 /703, loss: 0.530\n",
      "Epoch 100, Iter 300 /703, loss: 0.504\n",
      "Epoch 100, Iter 400 /703, loss: 0.498\n",
      "Epoch 100, Iter 500 /703, loss: 0.470\n",
      "Epoch 100, Iter 600 /703, loss: 0.493\n",
      "Epoch 100, Iter 700 /703, loss: 0.448\n",
      "-------- train controller --------\n",
      "0 tensor(-1.6848)\n",
      "1 tensor(0.9772)\n",
      "2 tensor(0.4622)\n",
      "3 tensor(1.2630)\n",
      "4 tensor(0.5941)\n",
      "5 tensor(1.2923)\n",
      "6 tensor(0.7117)\n",
      "7 tensor(1.8298)\n",
      "8 tensor(-0.7379)\n",
      "9 tensor(0.6687)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(config.epoch_num):\n",
    "    print(\"epoch:\",epoch)\n",
    "    print('-------- train child --------')\n",
    "    #sample an arch\n",
    "    ctrl.ctrl.net_sample()\n",
    "    ops = ctrl.ctrl.ops\n",
    "    skips = ctrl.ctrl.skips\n",
    "    print('ops',ops,'skips',skips)\n",
    "    child.train_epoch(ops,skips, train_imgs, train_labels, epoch, train_step)\n",
    "    \n",
    "    if (epoch + 1) % config.ctrl_train_every_epochs == 0:\n",
    "        print('-------- train controller --------')\n",
    "        ctrl.train_epoch(child, valid_imgs, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildChildModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               ops,\n",
    "               skips,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32\n",
    "              ):\n",
    "        super(BuildChildModel, self).__init__() \n",
    "        self.class_num = class_num \n",
    "        self.num_layers = num_layers \n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.ops  = ops\n",
    "        self.skips = skips\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layers = self._build_layer(self.class_num)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(self.out_channels, class_num, bias=True) \n",
    "        \n",
    "\n",
    "    def _build_layer(self, class_num):\n",
    "        layer = []\n",
    "        \n",
    "        # major part of the graph consisting of all NasLayers\n",
    "        for i in range(self.num_layers):\n",
    "            if self.ops[i] == 0:\n",
    "                kernel = nn.Conv2d(\n",
    "                in_channels=self.out_channels, \n",
    "                out_channels=self.out_channels, \n",
    "                kernel_size = 3, \n",
    "                padding=(1,1), # (3-1)/2\n",
    "                stride=1)\n",
    "            elif self.ops[i] == 1:\n",
    "                kernel = nn.Conv2d(\n",
    "                    in_channels=self.out_channels, \n",
    "                    out_channels=self.out_channels, \n",
    "                    kernel_size = 5, \n",
    "                    padding=(2,2), # (5-1)/2\n",
    "                    stride=1)\n",
    "            elif self.ops[i] == 2:\n",
    "                kernel = nn.AvgPool2d(\n",
    "                    kernel_size=3, \n",
    "                    padding=(1,1),\n",
    "                    stride=1)\n",
    "            elif self.ops[i] == 3:\n",
    "                kernel = nn.MaxPool2d(\n",
    "                    kernel_size=3, \n",
    "                    padding=(1,1),\n",
    "                    stride=1)\n",
    "            layer.append(kernel)\n",
    "            # bn_out\n",
    "            if (self.ops[i] == 0) or (self.ops[i] == 1):\n",
    "                layer.append(nn.ReLU(inplace = True))\n",
    "        # create a ModuleList, or the parameters cannot be added\n",
    "        layer = nn.ModuleList(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    # TODO: no reduction !\n",
    "    def model(self, x):\n",
    "        \"\"\"\n",
    "        run (like forward) a child model determined by sample_arch\n",
    "        Args:\n",
    "            sample_arch: a list consisting of 2 * num_layers elements\n",
    "                op_id = sample_arch[2k]: operation id\n",
    "                skip = sample_arch[2k + 1]: element i of such binary vector \n",
    "                    is used to describe whether the previous layer i is used \n",
    "                    as an input\n",
    "            x: input of the child model\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        # layers\n",
    "        prev_layers = []\n",
    "        # stem_conv\n",
    "        x = self.head(x)\n",
    "        # nas_layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "            for j in range(i):\n",
    "                if skip[i][j]:\n",
    "                    x = x + prev_layers[j]\n",
    "            prev_layers.append(x)\n",
    "        # global_avgpool\n",
    "        x = self.tail(x)\n",
    "        x = global_avgpool(x)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
