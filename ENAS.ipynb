{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "\n",
    "    epoch_num = 100\n",
    "    retrain_epoch_num = 1\n",
    "    platform = \"cpu\"\n",
    "    # -----------\n",
    "    # child \n",
    "    # -----------\n",
    "    \n",
    "    # model\n",
    "    class_num = 10\n",
    "    child_num_layers = 6\n",
    "    child_out_channels = 32\n",
    "    child_num_op = 5\n",
    "    # training\n",
    "    child_data_path = './data/cifar-10-batches-py/'\n",
    "    child_num_valids = 5000\n",
    "    child_batch_size = 64\n",
    "    # cosine learning \n",
    "    child_lr_init = 0.05\n",
    "    child_lr_gamma = 0.1\n",
    "    child_lr_cos_lmin = 0.001\n",
    "    child_lr_cos_Tmax = 2\n",
    "    # weight decay \n",
    "    child_l2_reg = 1e-4\n",
    "\n",
    "    child_run_loss_every = 100\n",
    "    # validate shared parameters\n",
    "    child_valid_every_epochs = 1\n",
    "    \n",
    "\n",
    "    # -----------\n",
    "    # controller \n",
    "    # -----------\n",
    "    # model\n",
    "    ctrl_lstm_size = 64\n",
    "    ctrl_lstm_num_layers = 2\n",
    "    # child\n",
    "    # --- training\n",
    "    ctrl_train_step_num = 10 # number of training steps per epoch\n",
    "    ctrl_batch_size = 5 # number of samples per training step\n",
    "    ctrl_train_every_epochs = 1\n",
    "    # learning scheduler = exponential decaying\n",
    "    ctrl_lr_init = 0.0001\n",
    "    ctrl_lr_gamma = 0.1\n",
    "    # baseline - reduce high variance; exponential moving average\n",
    "    ctrl_baseline_decay = 0.999\n",
    "    # prevent from being permature of controller\n",
    "    # applied to logits\n",
    "    ctrl_temperature = 5\n",
    "    ctrl_tanh_constant = 2.5\n",
    "    # add entropy to reward\n",
    "    ctrl_entropy_weight = 0.0001\n",
    "    # enforce skip sparsity \n",
    "    # add skip penalty to loss\n",
    "    ctrl_skip_target = 0.4\n",
    "    ctrl_skip_weight = 0.8\n",
    "    # validate/test controller \n",
    "    ctrl_valid_every_epochs = 1\n",
    "    ctrl_eval_arc_num = 2\n",
    "    ctrl_final_arc_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import torch\n",
    "\n",
    "def unpickle(file):\n",
    "    \"\"\"\n",
    "    Read a batch\n",
    "    \"\"\"\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def _read_data(file_list):\n",
    "    \"\"\"\n",
    "    Read a dataset and reshape it\n",
    "    args: \n",
    "        file_list: path of dataset\n",
    "    returns:\n",
    "        data_set: all data\n",
    "            format NCHW: x, 3072 -> x, 3, 32, 32\n",
    "        label_set: all labels of the data\n",
    "    \"\"\"\n",
    "    for i in range(len(file_list)):\n",
    "        file = file_list[i]\n",
    "        if DEBUG: print(file)\n",
    "        data_batch = unpickle(file)\n",
    "        data = data_batch[b'data']\n",
    "        data = data.astype('float32')   # change to \n",
    "        data = data.reshape((-1, 3, 32, 32))\n",
    "        labels = np.array(data_batch[b'labels']).astype('int')\n",
    "        if i == 0:\n",
    "            data_set = data\n",
    "            label_set = labels\n",
    "        else:\n",
    "            data_set = np.concatenate((data_set, data), axis = 0)\n",
    "            label_set = np.concatenate((label_set, labels), axis = 0)\n",
    "        # if DEBUG:\n",
    "        #     print(data.shape)\n",
    "        #     print(labels.shape)\n",
    "\n",
    "\n",
    "    return (data_set, label_set)\n",
    "\n",
    "def read_data(data_path, num_valids=5000):\n",
    "    \"\"\"\n",
    "    Read train/valid/test data sets\n",
    "    args: \n",
    "        num_valids: num of images in a valid set\n",
    "    returns:\n",
    "        images: N, C, H, W\n",
    "            in case of CIFAR-10, it consists of \n",
    "            \n",
    "            train: \n",
    "                no valid set: 50k\n",
    "                with valid set: 45k\n",
    "            test:\n",
    "                10k\n",
    "            valid\n",
    "                last 5k of training set\n",
    "        label_set: all labels of the datasets\n",
    "    \"\"\"\n",
    "    # create the file list\n",
    "    # data_path = 'F:/2 Work/0 Solo/data/cifar-10-python/'\n",
    "    train_files = []\n",
    "    for i in range(5):\n",
    "        # train_files.append('../data/cifar-10-python/data_batch_' + str(i+1))\n",
    "        train_files.append(data_path + 'data_batch_' + str(i+1))\n",
    "        # path:\n",
    "            # F:\\2 Work\\0 Solo invented by InvisibleForce\\pytorch\\data\\cifar-10-batches-py\n",
    "    test_files = []\n",
    "    test_files.append(data_path + 'test_batch')\n",
    "\n",
    "    # create image and label dict\n",
    "    images, labels = {}, {}\n",
    "    # read train set\n",
    "    images['train'], labels['train'] = _read_data(train_files)\n",
    "    # read valid set\n",
    "    if num_valids: # need\n",
    "        # valid set\n",
    "        images['valid'] = images['train'][-num_valids:] # last num_valids images\n",
    "        labels['valid'] = labels['train'][-num_valids:]\n",
    "        # train set = the remaining orginal train set\n",
    "        images['train'] = images['train'][:-num_valids] # last num_valids images\n",
    "        labels['train'] = labels['train'][:-num_valids]\n",
    "\n",
    "    # read test set\n",
    "    images['test'], labels['test'] = _read_data(test_files)\n",
    "\n",
    "\n",
    "    # normalize data\n",
    "    # 1. sub mean\n",
    "    # 2. divide std\n",
    "    # proc train set\n",
    "    images['train'] = normalize(images['train'])\n",
    "    # convert data to tensor\n",
    "    images['train'] = torch.from_numpy(images['train'])\n",
    "    # proc valid set\n",
    "    if num_valids:\n",
    "        images['valid'] = normalize(images['valid'])\n",
    "        images['valid'] = torch.from_numpy(images['valid'])\n",
    "    # proc test set\n",
    "    images['test'] = normalize(images['test'])\n",
    "    images['test'] = torch.from_numpy(images['test'])\n",
    "    # convert labels from np array to torch.tensor.long\n",
    "    labels['train'] = torch.from_numpy(labels['train'])\n",
    "    labels['train'] = labels['train'].long()\n",
    "    if num_valids:\n",
    "        labels['valid'] = torch.from_numpy(labels['valid'])\n",
    "        labels['valid'] = labels['valid'].long()\n",
    "    labels['test'] = torch.from_numpy(labels['test'])\n",
    "    labels['test'] = labels['test'].long()\n",
    "    return images, labels\n",
    "\n",
    "def normalize(dataset):\n",
    "    \"\"\"\n",
    "    1. sub mean\n",
    "    2. divide std\n",
    "    arg:\n",
    "        img: dataset, (N, C, H, W)\n",
    "    return: \n",
    "        img: normalized dataset\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = np.transpose(dataset, [0, 2, 3, 1]) # NCHW -> NHWC\n",
    "    dataset = dataset / 255.0 # 0-255 -> 0-1\n",
    "    mean = np.mean(dataset, axis=(0, 1, 2), keepdims=True)\n",
    "    std = np.std(dataset, axis=(0, 1, 2), keepdims=True)\n",
    "    dataset = (dataset - mean) / std \n",
    "    dataset = np.transpose(dataset, [0, 3, 1, 2]) # NHWC -> NCHW\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def augment(batch):\n",
    "    \"\"\"\n",
    "    Processed on GPU\n",
    "    1 upsample: 32x32 -> 40x40\n",
    "    2 randomly crop: 40x40 -> 32x32\n",
    "    3 flip horizontally: left -> right\n",
    "    arg:\n",
    "        img: a batch of images, (N, C, H, W)\n",
    "    return: \n",
    "        img: augmented batch\n",
    "    \"\"\"\n",
    "    # convert batch from tensor to nparray\n",
    "    # batch = batch.data.numpy() # only supported by cpu devices\n",
    "    if DEBUG: print('augment', type(batch))\n",
    "    # parameters\n",
    "    N, C, H, W = batch.size()\n",
    "    # augment\n",
    "    # for i in range(N):\n",
    "    # img = batch[i, :, :, :]\n",
    "    # 1 upsample: 32x32 -> 40x40 (H, W)\n",
    "    batch = pad(batch, [[4, 4], [4, 4]])\n",
    "    # 2 randomly crop: 40x40 -> 32x32\n",
    "    batch = crop(batch, [H, W])\n",
    "    # 3 flip horizontally: left -> right\n",
    "    batch = flip_left_right(batch)\n",
    "    # store it back to batch\n",
    "    # batch[i, :, :, :] = img_flip\n",
    "\n",
    "    # batch = torch.from_numpy(batch) # only for cpu\n",
    "    if DEBUG: print('augment', type(batch))\n",
    "    return batch \n",
    "\n",
    "def pad(batch, pad_size):\n",
    "    \"\"\"\n",
    "    pad zeros to an img\n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "        pad_size: [[C_before, C_after], [H_top, H_bottom], [W_left, W_right]]\n",
    "    return: \n",
    "        img: a zero-padded img\n",
    "    \"\"\"\n",
    "    # params\n",
    "    N, C, H, W = batch.size()\n",
    "    H_zeros, W_zeros = pad_size\n",
    "    # zero padding\n",
    "    H_zp = H + H_zeros[0] + H_zeros[1]\n",
    "    W_zp = W + W_zeros[0] + W_zeros[1]\n",
    "    batch_zp = torch.zeros((N, C, H_zp, W_zp)).cuda()\n",
    "    # batch_zp = batch_zp\n",
    "    batch_zp[:, :, H_zeros[0] : H_zeros[0] + H, W_zeros[0] : W_zeros[0] + W] = batch\n",
    "    \n",
    "    return batch_zp\n",
    "\n",
    "def crop(batch, crop_size):\n",
    "    \"\"\"\n",
    "    randomly crop img to a smaller one\n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "        crop_size: [C_crop, H_crop, W_crop]\n",
    "    return: \n",
    "        img_crop: a cropped img\n",
    "    \"\"\"\n",
    "    # parameter\n",
    "    N, C, H, W = batch.size()\n",
    "    H_crop, W_crop = crop_size\n",
    "    H_diff = H - H_crop + 1\n",
    "    W_diff = W - W_crop + 1\n",
    "    # randomly sample the crop offset\n",
    "    H_offset = int(np.random.randint(0, H_diff, size=1))\n",
    "    W_offset = int(np.random.randint(0, W_diff, size=1))\n",
    "    # crop the img\n",
    "    batch_crop = batch[:, :, H_offset : H_offset + H_crop, W_offset : W_offset + W_crop]\n",
    "\n",
    "    return batch_crop\n",
    "\n",
    "def flip_left_right(batch):\n",
    "    \"\"\"\n",
    "    flip img from left to right. \n",
    "    CHW data format needs to flip axis=2 (i.e., W-axis) \n",
    "    arg:\n",
    "        img: an image, (C, H, W)\n",
    "    return: \n",
    "        img_flip: a left-right-flipped img \n",
    "    \"\"\"\n",
    "    # img_flip = np.flip(img, axis=2)\n",
    "    # batch = torch.flip(batch, )\n",
    "    batch = batch.flip(3) # flip along w-axis\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def test_data_proc():\n",
    "    images, labels = read_data()\n",
    "    batch = images['train'][0:3]\n",
    "    print(batch[0, 0, 5, :])\n",
    "    print(torch.sum(batch))\n",
    "    batch_aug = augment(batch)\n",
    "    print(batch_aug[0, 0, 5, :])\n",
    "    print(torch.sum(batch_aug))\n",
    "    # diff = batch - batch_aug\n",
    "    # print(batch)\n",
    "    # print(batch_aug)\n",
    "    # print(diff)\n",
    "    print(batch.shape)\n",
    "    print(batch_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# installed\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "DEBUG = 0\n",
    "\n",
    "\n",
    "def global_avgpool(x):\n",
    "    \"\"\"\n",
    "    An operation used to reduce the H and W axis\n",
    "    x = [N, C, H, W] -> [N, C, 1, 1]\n",
    "    \"\"\"\n",
    "    H = x.size()[2]\n",
    "    W = x.size()[3]\n",
    "    x = torch.sum(x, dim=[2, 3])\n",
    "    x = x / (H * W)\n",
    "\n",
    "    return x\n",
    "\n",
    "# TODO: rename LayserOp as NodeOp\n",
    "class Operation(nn.Module):\n",
    "    \"\"\"\n",
    "    An operation used by a nas layer\n",
    "    Args:\n",
    "        op: conv1, conv3, conv5, avgpool3, maxpool3\n",
    "        out_channels: = M, num of filters\n",
    "    \"\"\"\n",
    "    def __init__(self, op, out_channels):\n",
    "        super(Operation, self).__init__() \n",
    "        self.op = op \n",
    "        self.out_channels = out_channels\n",
    "        self.layers = self._build_layer()\n",
    "    \n",
    "    def _build_layer(self):\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        # kernel\n",
    "        if self.op == 'conv1':\n",
    "             kernel = nn.Conv2d(\n",
    "            in_channels=self.out_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size = 1, \n",
    "            padding=(0,0), \n",
    "            stride=1)\n",
    "        elif self.op == 'conv3':\n",
    "            kernel = nn.Conv2d(\n",
    "            in_channels=self.out_channels, \n",
    "            out_channels=self.out_channels, \n",
    "            kernel_size = 3, \n",
    "            padding=(1,1), # (3-1)/2\n",
    "            stride=1)\n",
    "        elif self.op == 'conv5':\n",
    "            kernel = nn.Conv2d(\n",
    "                in_channels=self.out_channels, \n",
    "                out_channels=self.out_channels, \n",
    "                kernel_size = 5, \n",
    "                padding=(2,2), # (5-1)/2\n",
    "                stride=1)\n",
    "        elif self.op == 'avgpool3':\n",
    "            kernel = nn.AvgPool2d(\n",
    "                kernel_size=3, \n",
    "                padding=(1,1),\n",
    "                stride=1)\n",
    "        elif self.op == 'maxpool3':\n",
    "            kernel = nn.MaxPool2d(\n",
    "                kernel_size=3, \n",
    "                padding=(1,1),\n",
    "                stride=1)\n",
    "        layers.append(kernel)\n",
    "        # bn_out\n",
    "        if (self.op == 'conv1') or (self.op == 'conv3') or (self.op == 'conv5'):\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        layers = nn.Sequential(*layers)\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        return self.layers(x)\n",
    "        \n",
    "\n",
    "class Node(nn.Module):\n",
    "    def __init__(self, out_channels=24):\n",
    "        super(Node, self).__init__()\n",
    "        \"\"\"\n",
    "        Create a nas layer\n",
    "        \"\"\"\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.layers = self._build_nas_layer()\n",
    "        \n",
    "        self.bn_out = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        \n",
    "\n",
    "    def _build_nas_layer(self):\n",
    "        \"\"\"\n",
    "        build a nas layer consisting all possible branches\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        # conv1, 0\n",
    "        conv1 = Operation('conv1', self.out_channels)\n",
    "        layers.append(conv1)\n",
    "        # conv3, 1\n",
    "        conv3 = Operation('conv3', self.out_channels)\n",
    "        layers.append(conv3)\n",
    "        # conv5, 2\n",
    "        conv5 = Operation('conv5', self.out_channels)\n",
    "        layers.append(conv5)\n",
    "        # avgpool3, 3\n",
    "        avgpool3 = Operation('avgpool3', self.out_channels)\n",
    "        layers.append(avgpool3)\n",
    "        # maxpool3, 4\n",
    "        maxpool3 = Operation('maxpool3', self.out_channels)\n",
    "        layers.append(maxpool3)\n",
    "        # create a module list\n",
    "        layers = nn.ModuleList(layers)\n",
    "    \n",
    "        return layers\n",
    "\n",
    "    def layer_op(self, x, op):\n",
    "        \"\"\"\n",
    "        Run the operation of a nas layer\n",
    "        Args:\n",
    "            x: in_map\n",
    "            op: operation to run\n",
    "                0 - conv1\n",
    "                1 - conv3\n",
    "                2 - conv5\n",
    "                3 - avgpool3\n",
    "                4 - maxpool3\n",
    "        Returns:\n",
    "            x: out_map\n",
    "        \"\"\"\n",
    "        x = self.layers[op](x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def skip(self, prev_layers, connections):\n",
    "        \"\"\"\n",
    "        Concate the desired preve layers of a nas layer\n",
    "        Args:\n",
    "            prev_layers: previous layers\n",
    "            config: describe all the combined layers\n",
    "        Returns:\n",
    "            y: ofmap\n",
    "        \"\"\"\n",
    "        # add all the desired prev layers together\n",
    "        offset = 1\n",
    "        num_layer = len(prev_layers)-offset\n",
    "        x = []\n",
    "        for i in range(num_layer):\n",
    "            if connections[i]:\n",
    "                x.append(prev_layers[i+offset])\n",
    "        if len(x):\n",
    "            x = torch.stack(x) # stack all the tensors in an additional axis (i.e., 0)\n",
    "            x = torch.sum(x, dim=0) # add along axis 0\n",
    "        else:\n",
    "            x = torch.zeros(prev_layers[0].size()) .cuda()\n",
    "        return x\n",
    "\n",
    "\n",
    "    def __call__(self, cnt_layer, prev_layers, op, connections):\n",
    "        \"\"\"\n",
    "        describe the forward of the layer\n",
    "        Args:\n",
    "            prev_layers: all previous layers\n",
    "            layer_config: op and connectivity\n",
    "        \"\"\"\n",
    "        x = prev_layers[-1]\n",
    "        # run op of the enas layer\n",
    "        x = self.layer_op(x, op)\n",
    "        if cnt_layer > 0:\n",
    "            # combine the skip (add skips with x)\n",
    "            y = self.skip(prev_layers, connections)\n",
    "            # if DEBUG: print('skip_out\\n', y.data)\n",
    "            # combine op and skip results\n",
    "            # gpu not supporting x + y\n",
    "            x = torch.stack([x, y])\n",
    "            x = torch.sum(x, dim=0)\n",
    "            x = self.bn_out(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(ChildModel, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # data set used for training, validating, testing\n",
    "        self.class_num = class_num \n",
    "        # parameters for building a child model\n",
    "        self.num_layers = num_layers # \n",
    "        self.out_channels = out_channels\n",
    "        # build DAG = net\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.graph = self._build_graph(self.class_num)\n",
    "        # fc for final classification\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(self.out_channels, class_num, bias=True) \n",
    "        \n",
    "\n",
    "    def _build_graph(self, class_num):\n",
    "        graph = []\n",
    "        \n",
    "        # major part of the graph consisting of all NasLayers\n",
    "        for _ in range(self.num_layers):\n",
    "            graph.append(Node(self.out_channels))\n",
    "        # create a ModuleList, or the parameters cannot be added\n",
    "        graph = nn.ModuleList(graph)\n",
    "\n",
    "        return graph\n",
    "\n",
    "    # TODO: no reduction !\n",
    "    def model(self, x, ops,skips):\n",
    "        \"\"\"\n",
    "        run (like forward) a child model determined by sample_arch\n",
    "        Args:\n",
    "            sample_arch: a list consisting of 2 * num_layers elements\n",
    "                op_id = sample_arch[2k]: operation id\n",
    "                skip = sample_arch[2k + 1]: element i of such binary vector \n",
    "                    is used to describe whether the previous layer i is used \n",
    "                    as an input\n",
    "            x: input of the child model\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        # layers\n",
    "        prev_layers = []\n",
    "        # stem_conv\n",
    "        x = self.head(x)\n",
    "        prev_layers.append(x)\n",
    "        # nas_layers\n",
    "        for cnt_layer in range(self.num_layers):\n",
    "            x = self.graph[cnt_layer]( cnt_layer,prev_layers, ops[cnt_layer],skips[cnt_layer])\n",
    "            prev_layers.append(x)\n",
    "        # global_avgpool\n",
    "        x = self.tail(x)\n",
    "        x = global_avgpool(x)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Child(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32,\n",
    "               device='cpu', \n",
    "               lr_init=0.05,\n",
    "               lr_gamma=0.1,\n",
    "               lr_cos_lmin=0.001,\n",
    "               lr_cos_Tmax=2,\n",
    "               l2_reg=1e-4,\n",
    "               run_loss_every=100\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(Child, self).__init__() \n",
    "        self.class_num = class_num # number of classes\n",
    "        self.num_layers = num_layers # \n",
    "        self.out_channels = out_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.run_loss_every = run_loss_every\n",
    "\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.lr_cos_lmin = lr_cos_lmin\n",
    "        self.lr_cos_Tmax = lr_cos_Tmax\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.net = ChildModel(class_num, num_layers, out_channels)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.optimizer = optim.SGD([{'params': self.net.parameters(), 'initial_lr': self.lr_init}], lr=self.lr_init, weight_decay=self.l2_reg, momentum=0.9, nesterov=True)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.lr_cos_Tmax, eta_min=self.lr_cos_lmin)\n",
    "    \n",
    "    def get_batch(self, images, labels, step):\n",
    "        batch_size = self.batch_size\n",
    "        batch_images = images[step * batch_size : (step + 1) * batch_size] \n",
    "        batch_labels = labels[step * batch_size : (step + 1) * batch_size] \n",
    "\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def train_epoch(self, ops, skips, images, labels, epoch, train_step):\n",
    "        all_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        print('lr=', self.scheduler.get_last_lr())\n",
    "        for step in range(train_step): \n",
    "            batch_inputs, batch_labels = self.get_batch(images, labels, step)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.net.model(batch_inputs, ops,skips)\n",
    "            loss = self.criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # update running loss\n",
    "            all_loss += loss.item()\n",
    "            running_loss += loss.item()\n",
    "            if step % self.run_loss_every == (self.run_loss_every - 1):\n",
    "                print('Epoch %d, Iter %d /%d, loss: %.3f' %\n",
    "                    (epoch + 1, step + 1,train_step, running_loss / self.run_loss_every))\n",
    "                running_loss = 0.0\n",
    "        self.scheduler.step()\n",
    "        return all_loss / train_step\n",
    "\n",
    "    def eval_mini(self, ops, skips, images, labels):    \n",
    "        # validating\n",
    "        # get a minibatch for cpu or gpu\n",
    "        high = labels.size()[0] // self.batch_size\n",
    "        \n",
    "        batch_idx = torch.randint(high, (1,1))\n",
    "        # batch_idx = random.randint(0, high)\n",
    "        batch_inputs, batch_labels = self.get_batch(images, labels, batch_idx)\n",
    "        \n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net.model(batch_inputs, ops,skips)\n",
    "        \n",
    "        # cal accuracy\n",
    "        _, idx = torch.topk(outputs, 1)\n",
    "        idx = idx.reshape((-1))\n",
    "        accuracy = (idx == batch_labels).float().sum()\n",
    "        accuracy /= self.batch_size\n",
    "        \n",
    "        return accuracy\n",
    "\n",
    "    def eval(self, sample_arch, images, labels):\n",
    "        step_num = labels.size()[0] // self.batch_size\n",
    "        total_accuracy = 0\n",
    "        for i in range(step_num):\n",
    "            batch_inputs, batch_labels = self.get_batch(images, labels, i)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.net.model(batch_inputs, sample_arch)\n",
    "            _, idx = torch.topk(outputs, 1)\n",
    "            idx = idx.reshape((-1))\n",
    "            total_accuracy += (idx == batch_labels).float().sum() # count the correct prediction\n",
    "        total_accuracy /= (step_num * self.batch_size)\n",
    "        \n",
    "        return total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class StackLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    StackLSTM class.\n",
    "    It describes a stacked LSTM which only \n",
    "    run a single step.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, lstm_num_layers=2):\n",
    "        super(StackLSTM, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        \n",
    "        self.net = self._build_net()\n",
    "\n",
    "    def _build_net(self):\n",
    "        \n",
    "        net = []\n",
    "        for _ in range(self.lstm_num_layers):\n",
    "            layer = nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "            net.append(layer)\n",
    "        net = nn.ModuleList(net)\n",
    "\n",
    "        return net\n",
    "\n",
    "    def __call__(self, inputs, prev_h, prev_c):\n",
    "        \"\"\"\n",
    "        Forward of stacked LSTM\n",
    "        Args:\n",
    "            inputs: input of the stack lstm, [batch=1, input_size]\n",
    "            prev_h & prev_c: hidden and cell states of each layer at the previous time step.\n",
    "                size = [lstm_num_layer, hidden_size]\n",
    "        Returns:\n",
    "            next_h & next_c\n",
    "        \"\"\"\n",
    "        net = self.net\n",
    "        next_h, next_c = [], []\n",
    "        for i in range(self.lstm_num_layers):\n",
    "            if i == 0: x = inputs\n",
    "            else: x = next_h[-1]\n",
    "            cur_h, cur_c = net[i](x, (prev_h[i], prev_c[i]))\n",
    "            next_h.append(cur_h)\n",
    "            next_c.append(cur_c)\n",
    "        \n",
    "        return next_h, next_c\n",
    "\n",
    "class ControllerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "               child_num_layers=6,\n",
    "               lstm_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               num_op=4,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               skip_target=0.4,\n",
    "               device='gpu'\n",
    "              ):\n",
    "        \"\"\"\n",
    "        1. init params\n",
    "        2. create a graph which contains the sampled subgraph\n",
    "        \"\"\"\n",
    "        super(ControllerModel, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # parameters for building a child model\n",
    "        self.child_num_layers = child_num_layers # \n",
    "        # parameters for building a controller\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_num_layers = lstm_num_layers\n",
    "        self.num_op = num_op\n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        # build controller = net\n",
    "        # claim all the layers and parameters\n",
    "        self.net = self._build_net()\n",
    "        # add g_emb as a parameter to ControllerModel\n",
    "        # initialized by uniform distribution between -0.1 to 0.1\n",
    "        # 0 <= torch.rand < 1\n",
    "        g_emb_init = 0.2 * torch.rand(1,self.lstm_size) - 0.1\n",
    "        self.register_parameter(name='g_emb', param=torch.nn.Parameter(g_emb_init))\n",
    "        # results of net sample\n",
    "        self.sample_arch = []\n",
    "        self.sample_entropy = []\n",
    "        self.sample_log_prob = []\n",
    "        self.sample_skip_count = []\n",
    "        self.sample_skip_penaltys = []\n",
    "        \n",
    "\n",
    "    def _build_net(self):\n",
    "\n",
    "        net = {}\n",
    "        # layers & params shared by op and skip\n",
    "        net['lstm'] = StackLSTM(self.lstm_size, self.lstm_size, self.lstm_num_layers)\n",
    "        # layers & params used only by op\n",
    "        net['op_fc'] = nn.Linear(self.lstm_size, self.num_op)\n",
    "        net['op_emb_lookup'] = nn.Embedding(self.num_op, self.lstm_size)\n",
    "        # layers & params used only by skip\n",
    "        net['skip_attn1'] = nn.Linear(self.lstm_size, self.lstm_size) # w_attn1 \n",
    "        net['skip_attn2'] = nn.Linear(self.lstm_size, self.lstm_size) # w_attn2)\n",
    "        net['skip_attn3'] = nn.Linear(self.lstm_size, 1)              # v_attn\n",
    "        net = nn.ModuleDict(net)\n",
    "        \n",
    "        return net\n",
    "\n",
    "    def _op_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample an op (it is a part of controller's forward)\n",
    "        Args: consisting of the following parts\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        net = self.net\n",
    "        inputs, prev_h, prev_c, arc_seq, log_probs, entropys = args\n",
    "        # lstm - process hidden states\n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "        # fc - calculate logit\n",
    "        logit = net['op_fc'](next_h[-1])    # h state of the last layer\n",
    "        # temperature\n",
    "        if self.temperature is not None:\n",
    "            logit /= self.temperature\n",
    "        # tanh and then scaled by a constant\n",
    "        if self.tanh_constant is not None:\n",
    "            logit = self.tanh_constant * torch.tanh(logit)\n",
    "        # use softmax transfer logits to probs\n",
    "        # or the logits may be negative it can not represent a prob\n",
    "        prob = f.softmax(logit, dim=1)\n",
    "        # multinomial for sampling an op\n",
    "        op_id = torch.multinomial(prob, 1) # logit = probs of each type of operation, 1 = sample a single op\n",
    "        op_id = op_id[0]\n",
    "        # generate input for skip_sample using embedding lookup\n",
    "        inputs = net['op_emb_lookup'](op_id.long())\n",
    "        # calculate log_prob\n",
    "        log_prob = f.cross_entropy(logit, op_id)\n",
    "        # calculate entropy\n",
    "        entropy = log_prob * torch.exp(-log_prob)\n",
    "        # add op to arc_seq\n",
    "        op = int(op_id.cpu().data.numpy()) # to an int\n",
    "        arc_seq.append(op)\n",
    "        # add to log_probs\n",
    "        log_probs.append(log_prob)\n",
    "        # add to entropys\n",
    "        entropys.append(entropy)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys        \n",
    "\n",
    "    def _skip_sample(self, args):\n",
    "        \"\"\"\n",
    "        sample skip connections for layer_id (it is a part of controller's forward)\n",
    "        Args:\n",
    "            layer_id\n",
    "            inputs: input of op_sample\n",
    "            prev_h & prev_c: the hidden and cell states of the prev layer\n",
    "            arc_seq: architecture sequence\n",
    "            log_probs: all the log probabilities used for training (recall the gradient calculation of REINFORCE)\n",
    "            entropys: all the entropys used for training\n",
    "            archors & anchors_w_1: archor points and its weighed values\n",
    "            skip_targets & skip_penaltys & skip_count: used to enforce the sparsity of skip connections\n",
    "        Return:\n",
    "            all args except layer_id\n",
    "        \"\"\"    \n",
    "        layer_id, inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = args\n",
    "        net = self.net\n",
    "        # lstm - process hidden states\n",
    "        next_h, next_c = net['lstm'](inputs, prev_h, prev_c)\n",
    "        prev_h, prev_c = next_h, next_c\n",
    "        if layer_id > 0:\n",
    "            # use attention mechanism to generate logits\n",
    "            # concate the weighed anchors\n",
    "            query = torch.cat(anchors_w_1, dim=0) \n",
    "            # attention 2 - fc\n",
    "            query = torch.tanh(net['skip_attn2'](next_h[-1]) + query)\n",
    "            # attention 3 - fc            \n",
    "            query = net['skip_attn3'](query)\n",
    "            # generate logit\n",
    "            logit = torch.cat([-query, query], dim=1)\n",
    "            # process logit with temperature\n",
    "            if self.temperature is not None:\n",
    "                logit /= self.temperature\n",
    "            # process logit with tanh and scale it\n",
    "            if self.temperature is not None:\n",
    "                logit = self.tanh_constant * torch.tanh(logit)\n",
    "            # calculate prob of skip (see NAS paper, Sec3.3)\n",
    "            skip_prob = torch.sigmoid(logit) # use sigmoid to convert skip to its prob\n",
    "            # sample skip connections using multinomial distribution sampler\n",
    "            skip = torch.multinomial(skip_prob, 1)  # 0 - used as an input, 1 - not an input\n",
    "            # calcualte kl as skip penalty\n",
    "            kl = skip_prob * torch.log(skip_prob / skip_targets) # calculate kl\n",
    "            kl = torch.sum(kl)\n",
    "            skip_penaltys.append(kl)\n",
    "            # cal log_prob and append it - used by REINFORCE to calculate gradients of controller (i.e., LSTM)\n",
    "            log_prob = f.cross_entropy(logit, skip.squeeze(dim=1))\n",
    "            log_probs.append(torch.sum(log_prob))\n",
    "            # cal entropys and append it\n",
    "            entropy = log_prob * torch.exp(-log_prob)\n",
    "            entropy = torch.sum(entropy)\n",
    "            entropys.append(entropy)\n",
    "            # update count of skips\n",
    "            skip_count.append(skip.sum())\n",
    "            arc_seq.append(skip.cpu().squeeze(dim=1).data.numpy().tolist())\n",
    "            # generate inputs for the next time step\n",
    "            skip = torch.reshape(skip, (1, layer_id)) # reshape skip\n",
    "            cat_anchors = torch.cat(anchors, dim=0)\n",
    "            # skip = 1 x layer_id (layer_id > 0) \n",
    "            # cat_anchors = layer_id x lstm_size\n",
    "            inputs = torch.matmul(skip.float(), cat_anchors) \n",
    "            inputs /= (1.0 + torch.sum(skip))\n",
    "        else:\n",
    "            inputs = self.g_emb\n",
    "            arc_seq.append([]) # no skip, use empty list to occupy the position\n",
    "        \n",
    "        # cal the\n",
    "        anchors.append(next_h[-1])\n",
    "        # cal attention 1\n",
    "        attn1 = net['skip_attn1'](next_h[-1])\n",
    "        anchors_w_1.append(attn1)\n",
    "\n",
    "        return inputs, prev_h, prev_c, arc_seq, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count\n",
    "\n",
    "    def net_sample(self):\n",
    "        \"\"\"\n",
    "        run (like forward) a controller model to sample an neural architecture\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        # net sample\n",
    "        ops = []\n",
    "        skips = []\n",
    "        \n",
    "        entropys = []\n",
    "        log_probs = []\n",
    "        # skip sample \n",
    "        anchors = []        # store hidden states of skip lstm; anchor = hidden states of skip lstm (i.e., layer_id)\n",
    "        anchors_w_1 = []    # store results of attention 1 (input=h, w_attn1)\n",
    "        skip_count = []\n",
    "        skip_penaltys = []\n",
    "\n",
    "\n",
    "        # init inputs and states\n",
    "        # init prev cell states to zeros for each layer of the lstm\n",
    "        prev_c = [torch.zeros((1, self.lstm_size)).cuda() for _ in range(self.lstm_num_layers)]\n",
    "        # init prev hidden states to zeros for each layer of the lstm\n",
    "        prev_h = [torch.zeros((1, self.lstm_size)).cuda() for _ in range(self.lstm_num_layers)]\n",
    "        # inputs\n",
    "        inputs = self.g_emb.cuda()\n",
    "        # skip_target = 0.4 = the prob of a layer used as an input of another layer\n",
    "        # 1 - skip_target = 0.6; the probability that this layer is not used as an input\n",
    "        skip_targets = torch.tensor([1.0 - self.skip_target, self.skip_target], dtype=torch.float).cuda()\n",
    "        \n",
    "\n",
    "        # sample an arch\n",
    "        for layer_id in range(self.child_num_layers):\n",
    "            arg_op_sample = [inputs, prev_h, prev_c, ops, log_probs, entropys]\n",
    "            returns_op_sample = self._op_sample(arg_op_sample)\n",
    "            inputs, prev_h, prev_c, ops, log_probs, entropys = returns_op_sample\n",
    "            arg_skip_sample = [layer_id, inputs, prev_h, prev_c, skips, log_probs, entropys, \n",
    "                                anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count]\n",
    "            returns_skip_sample = self._skip_sample(arg_skip_sample)\n",
    "            inputs, prev_h, prev_c, skips, log_probs, entropys, anchors, anchors_w_1, skip_targets, skip_penaltys, skip_count = returns_skip_sample\n",
    "\n",
    "        # generate sample arch\n",
    "        # [[op], [skip]] * num_layer\n",
    "        self.ops = ops\n",
    "        self.skips = skips\n",
    "        \n",
    "        # cal sample entropy\n",
    "        entropys = torch.stack(entropys)\n",
    "        self.sample_entropy = torch.sum(entropys)\n",
    "   \n",
    "        # cal sample log_probs\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        self.sample_log_prob = torch.sum(log_probs)\n",
    "   \n",
    "            \n",
    "        # cal skip count\n",
    "        skip_count = torch.stack(skip_count)\n",
    "        self.sample_skip_count = torch.sum(skip_count)\n",
    "     \n",
    "        # cal skip penaltys\n",
    "        skip_penaltys = torch.stack(skip_penaltys)\n",
    "        self.sample_skip_penaltys = torch.sum(skip_penaltys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(nn.Module):\n",
    "    \"\"\"\n",
    "    Controller class.\n",
    "    It describes how to train a controller\n",
    "        1) train\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "               device='gpu',\n",
    "               lstm_size=32,\n",
    "               lstm_num_layers=2,\n",
    "               child_num_layers=6,\n",
    "               num_op=4,\n",
    "               train_step_num=50,\n",
    "               ctrl_batch_size=20,\n",
    "               lr_init=0.00035,\n",
    "               lr_gamma=0.1,\n",
    "               temperature=5,\n",
    "               tanh_constant=2.5,\n",
    "               entropy_weight=0.0001,\n",
    "               baseline_decay=0.999,\n",
    "               skip_target=0.4,\n",
    "               skip_weight=0.8\n",
    "              ):\n",
    "        super(Controller, self).__init__() # init the parent class of Net, i.e., nn.Module\n",
    "        # config of controller model\n",
    "        # child model\n",
    "        self.child_num_layers = child_num_layers # imgs of dataset\n",
    "        # ctrl model\n",
    "        self.lstm_size = lstm_size # labels of dataset \n",
    "        self.lstm_num_layers = lstm_num_layers # number of classes\n",
    "        self.num_op = num_op # \n",
    "        self.temperature = temperature\n",
    "        self.tanh_constant = tanh_constant\n",
    "        self.skip_target = skip_target\n",
    "        # ctrl training\n",
    "        self.ctrl_batch_size=ctrl_batch_size\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_gamma = lr_gamma\n",
    "        self.train_step_num = train_step_num\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.baseline_decay = baseline_decay\n",
    "        self.skip_weight = skip_weight\n",
    "        # device\n",
    "        self.device = device\n",
    "        # # training parameters on gpu\n",
    "        self.reward = torch.zeros(1).cuda() # rewards of samples\n",
    "        self.baseline = torch.zeros(1).cuda() # base line\n",
    "        self.log_prob = torch.zeros(1).cuda() # log_probs of samples\n",
    "        self.entropy = torch.zeros(1).cuda() # entropys of samples\n",
    "        # self.skip_rate = torch.zeros(1) # skip_rates of samples\n",
    "        self.skip_penalty = torch.zeros(1).cuda() # skip_penaltys of samples\n",
    "        self.loss = torch.zeros(1).cuda() # loss\n",
    "\n",
    "\n",
    "        # build controller\n",
    "        self.ctrl = ControllerModel(child_num_layers=child_num_layers,\n",
    "               lstm_size=lstm_size,\n",
    "               lstm_num_layers=lstm_num_layers,\n",
    "               num_op=num_op,\n",
    "               temperature=temperature,\n",
    "               tanh_constant=tanh_constant,\n",
    "               skip_target=skip_target,\n",
    "               device=device)\n",
    "        self.optimizer = optim.Adam(self.ctrl.parameters(), lr=self.lr_init)\n",
    "        \n",
    "      \n",
    "    \n",
    "    def train_epoch(self, child_model, images, labels):    \n",
    "        for step in range(self.train_step_num):\n",
    "            loss = torch.zeros(self.ctrl_batch_size).cuda()\n",
    "            for sample_cnt in range(self.ctrl_batch_size):\n",
    "                # sample a child arch\n",
    "                self.ctrl.net_sample()\n",
    "                # valid a sampled arch and obtain reward\n",
    "                self.reward = child_model.eval_mini(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "                # add weighed entropy to reward\n",
    "                self.entropy = self.ctrl.sample_entropy\n",
    "                self.reward += self.entropy_weight * self.entropy\n",
    "                # update baseline\n",
    "                with torch.no_grad():\n",
    "                    self.baseline = self.baseline + (1 - self.baseline_decay) * (self.reward - self.baseline)\n",
    "                # update loss\n",
    "                self.log_prob = self.ctrl.sample_log_prob\n",
    "                self.skip_penalty = self.ctrl.sample_skip_penaltys\n",
    "                loss[sample_cnt] = self.log_prob * (self.reward - self.baseline) + self.skip_weight * self.skip_penalty\n",
    "            self.loss = loss.sum() / self.ctrl_batch_size # avg loss\n",
    "            print(step,self.loss.item())\n",
    "            self.optimizer.zero_grad()\n",
    "            self.loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return self.loss.item()\n",
    "\n",
    "\n",
    "    def eval(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        evaluate controller using validating data set.\n",
    "        It samples several archs and validate them on \n",
    "        the whole validate set.\n",
    "            \n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "        # obtain averaged op_history\n",
    "        return accuracy\n",
    "\n",
    "    def derive_best_arch(self, child_model, arc_num, images, labels, file):\n",
    "        \"\"\"\n",
    "        derive the final child model using controller\n",
    "        procedure\n",
    "            1. sample 1000 archs\n",
    "            2. test them on test data set\n",
    "            3. select the one with highest accuracy as the best arch\n",
    "        Args:\n",
    "            \n",
    "        Return:\n",
    "            best_arch\n",
    "        \"\"\"\n",
    "        accuracy = []\n",
    "        arcs = []\n",
    "        best_arch = []\n",
    "        best_accuracy = 0\n",
    "        for _ in range(arc_num):\n",
    "            # sample a child arch\n",
    "            self.ctrl.net_sample()\n",
    "            arcs.append(self.ctrl.sample_arch)\n",
    "            # valid a sampled arch and obtain reward\n",
    "            eval_acc = child_model.eval(self.ctrl.ops,self.ctrl.skips, images, labels) \n",
    "            accuracy.append(eval_acc)\n",
    "            # select the best arch\n",
    "            if eval_acc > best_accuracy:\n",
    "                best_accuracy = eval_acc\n",
    "                best_arch = self.ctrl.sample_arch\n",
    "\n",
    "        return best_accuracy, best_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = read_data(config.child_data_path, config.child_num_valids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "child = Child(\n",
    "        class_num=config.class_num,\n",
    "        num_layers=config.child_num_layers,\n",
    "        out_channels=config.child_out_channels,\n",
    "        batch_size=config.child_batch_size,\n",
    "        device=config.platform, \n",
    "        lr_init=config.child_lr_init,\n",
    "        lr_gamma=config.child_lr_gamma,\n",
    "        lr_cos_lmin=config.child_lr_cos_lmin,\n",
    "        lr_cos_Tmax=config.child_lr_cos_Tmax,\n",
    "        l2_reg=config.child_l2_reg,\n",
    "        run_loss_every=config.child_run_loss_every\n",
    "    )\n",
    "child = child.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl = Controller(\n",
    "        device=config.platform,\n",
    "        lstm_size=config.ctrl_lstm_size,\n",
    "        lstm_num_layers=config.ctrl_lstm_num_layers,\n",
    "        child_num_layers=config.child_num_layers,\n",
    "        num_op=config.child_num_op,\n",
    "        train_step_num=config.ctrl_train_step_num,\n",
    "        ctrl_batch_size=config.ctrl_batch_size,\n",
    "        lr_init=config.ctrl_lr_init,\n",
    "        lr_gamma=config.ctrl_lr_gamma,\n",
    "        temperature=config.ctrl_temperature,\n",
    "        tanh_constant=config.ctrl_tanh_constant,\n",
    "        entropy_weight=config.ctrl_entropy_weight,\n",
    "        baseline_decay=config.ctrl_baseline_decay,\n",
    "        skip_target=config.ctrl_skip_target,\n",
    "        skip_weight=config.ctrl_skip_weight)\n",
    "ctrl = ctrl.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = images['train'].cuda()\n",
    "train_labels = labels['train'].cuda()\n",
    "valid_imgs = images['valid'].cuda()\n",
    "valid_labels = labels['valid'].cuda()\n",
    "test_imgs = images['test'].cuda()\n",
    "test_labels = labels['test'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = int(train_imgs.size()[0] / config.child_batch_size)\n",
    "child_loss = []\n",
    "ctrl_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50\n",
      "-------- train child --------\n",
      "ops [2, 1, 3, 1, 4, 3] skips [[], [0], [0, 0], [0, 1, 0], [0, 0, 1, 0], [0, 0, 1, 1, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 51, Iter 100 /703, loss: 1.062\n",
      "Epoch 51, Iter 200 /703, loss: 0.874\n",
      "Epoch 51, Iter 300 /703, loss: 0.857\n",
      "Epoch 51, Iter 400 /703, loss: 0.800\n",
      "Epoch 51, Iter 500 /703, loss: 0.768\n",
      "Epoch 51, Iter 600 /703, loss: 0.774\n",
      "Epoch 51, Iter 700 /703, loss: 0.729\n",
      "-------- train controller --------\n",
      "0 1.4661896228790283\n",
      "1 2.5530550479888916\n",
      "2 2.8554046154022217\n",
      "3 0.1619141548871994\n",
      "4 3.155046224594116\n",
      "5 1.8491363525390625\n",
      "6 1.4369862079620361\n",
      "7 2.286153793334961\n",
      "8 1.4459104537963867\n",
      "9 2.7358474731445312\n",
      "epoch: 51\n",
      "-------- train child --------\n",
      "ops [2, 3, 3, 2, 3, 1] skips [[], [1], [0, 1], [1, 0, 0], [0, 1, 1, 0], [1, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 52, Iter 100 /703, loss: 0.766\n",
      "Epoch 52, Iter 200 /703, loss: 0.727\n",
      "Epoch 52, Iter 300 /703, loss: 0.719\n",
      "Epoch 52, Iter 400 /703, loss: 0.696\n",
      "Epoch 52, Iter 500 /703, loss: 0.670\n",
      "Epoch 52, Iter 600 /703, loss: 0.696\n",
      "Epoch 52, Iter 700 /703, loss: 0.656\n",
      "-------- train controller --------\n",
      "0 3.4617481231689453\n",
      "1 -0.38341736793518066\n",
      "2 1.5864537954330444\n",
      "3 1.2151025533676147\n",
      "4 1.2894737720489502\n",
      "5 2.382999897003174\n",
      "6 3.1432344913482666\n",
      "7 0.36452627182006836\n",
      "8 0.8274185061454773\n",
      "9 2.3617608547210693\n",
      "epoch: 52\n",
      "-------- train child --------\n",
      "ops [2, 4, 2, 1, 2, 0] skips [[], [0], [1, 0], [1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.05000000000000023]\n",
      "Epoch 53, Iter 100 /703, loss: 0.928\n",
      "Epoch 53, Iter 200 /703, loss: 0.810\n",
      "Epoch 53, Iter 300 /703, loss: 0.787\n",
      "Epoch 53, Iter 400 /703, loss: 0.770\n",
      "Epoch 53, Iter 500 /703, loss: 0.726\n",
      "Epoch 53, Iter 600 /703, loss: 0.745\n",
      "Epoch 53, Iter 700 /703, loss: 0.710\n",
      "-------- train controller --------\n",
      "0 0.7036557197570801\n",
      "1 1.943376898765564\n",
      "2 1.7089534997940063\n",
      "3 0.43433353304862976\n",
      "4 0.681850254535675\n",
      "5 1.3217442035675049\n",
      "6 0.652473509311676\n",
      "7 0.9864425659179688\n",
      "8 0.4524586796760559\n",
      "9 1.819051742553711\n",
      "epoch: 53\n",
      "-------- train child --------\n",
      "ops [2, 0, 3, 3, 1, 2] skips [[], [1], [0, 1], [0, 0, 0], [0, 0, 1, 0], [1, 1, 0, 1, 0]]\n",
      "lr= [0.025500000000000154]\n",
      "Epoch 54, Iter 100 /703, loss: 0.730\n",
      "Epoch 54, Iter 200 /703, loss: 0.676\n",
      "Epoch 54, Iter 300 /703, loss: 0.671\n",
      "Epoch 54, Iter 400 /703, loss: 0.656\n",
      "Epoch 54, Iter 500 /703, loss: 0.629\n",
      "Epoch 54, Iter 600 /703, loss: 0.649\n",
      "Epoch 54, Iter 700 /703, loss: 0.612\n",
      "-------- train controller --------\n",
      "0 1.1592535972595215\n",
      "1 1.8506311178207397\n",
      "2 0.550427258014679\n",
      "3 -0.843277096748352\n",
      "4 2.495574712753296\n",
      "5 1.820576548576355\n",
      "6 2.4593589305877686\n",
      "7 0.4591064453125\n",
      "8 1.7241802215576172\n",
      "9 2.134495496749878\n",
      "epoch: 54\n",
      "-------- train child --------\n",
      "ops [0, 1, 2, 2, 0, 2] skips [[], [1], [0, 1], [0, 0, 0], [0, 0, 0, 1], [1, 0, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 55, Iter 100 /703, loss: 1.072\n",
      "Epoch 55, Iter 200 /703, loss: 0.862\n",
      "Epoch 55, Iter 300 /703, loss: 0.840\n",
      "Epoch 55, Iter 400 /703, loss: 0.779\n",
      "Epoch 55, Iter 500 /703, loss: 0.742\n",
      "Epoch 55, Iter 600 /703, loss: 0.750\n",
      "Epoch 55, Iter 700 /703, loss: 0.712\n",
      "-------- train controller --------\n",
      "0 2.7576117515563965\n",
      "1 1.6837657690048218\n",
      "2 0.2098134607076645\n",
      "3 2.234389305114746\n",
      "4 0.12836889922618866\n",
      "5 1.050293207168579\n",
      "6 1.2872289419174194\n",
      "7 2.362398624420166\n",
      "8 1.2245692014694214\n",
      "9 1.856095552444458\n",
      "epoch: 55\n",
      "-------- train child --------\n",
      "ops [1, 3, 0, 4, 2, 4] skips [[], [0], [1, 1], [1, 0, 1], [1, 1, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 56, Iter 100 /703, loss: 0.798\n",
      "Epoch 56, Iter 200 /703, loss: 0.731\n",
      "Epoch 56, Iter 300 /703, loss: 0.721\n",
      "Epoch 56, Iter 400 /703, loss: 0.701\n",
      "Epoch 56, Iter 500 /703, loss: 0.674\n",
      "Epoch 56, Iter 600 /703, loss: 0.706\n",
      "Epoch 56, Iter 700 /703, loss: 0.665\n",
      "-------- train controller --------\n",
      "0 1.3611338138580322\n",
      "1 1.2058912515640259\n",
      "2 1.9319400787353516\n",
      "3 0.6883182525634766\n",
      "4 0.9701448678970337\n",
      "5 0.9300962686538696\n",
      "6 0.08804197609424591\n",
      "7 1.4655097723007202\n",
      "8 0.21127605438232422\n",
      "9 0.625930666923523\n",
      "epoch: 56\n",
      "-------- train child --------\n",
      "ops [1, 1, 2, 1, 3, 2] skips [[], [1], [1, 0], [0, 0, 1], [1, 0, 1, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.0499999999999999]\n",
      "Epoch 57, Iter 100 /703, loss: 0.782\n",
      "Epoch 57, Iter 200 /703, loss: 0.737\n",
      "Epoch 57, Iter 300 /703, loss: 0.725\n",
      "Epoch 57, Iter 400 /703, loss: 0.716\n",
      "Epoch 57, Iter 500 /703, loss: 0.687\n",
      "Epoch 57, Iter 600 /703, loss: 0.730\n",
      "Epoch 57, Iter 700 /703, loss: 0.677\n",
      "-------- train controller --------\n",
      "0 -0.5310192704200745\n",
      "1 0.19448518753051758\n",
      "2 2.144071578979492\n",
      "3 1.6099308729171753\n",
      "4 1.2615467309951782\n",
      "5 0.3793273866176605\n",
      "6 0.973599374294281\n",
      "7 0.7708048820495605\n",
      "8 1.199021339416504\n",
      "9 0.20457640290260315\n",
      "epoch: 57\n",
      "-------- train child --------\n",
      "ops [2, 3, 3, 3, 2, 4] skips [[], [1], [0, 0], [0, 1, 1], [0, 0, 0, 0], [0, 1, 0, 1, 0]]\n",
      "lr= [0.02550000000000017]\n",
      "Epoch 58, Iter 100 /703, loss: 0.701\n",
      "Epoch 58, Iter 200 /703, loss: 0.654\n",
      "Epoch 58, Iter 300 /703, loss: 0.649\n",
      "Epoch 58, Iter 400 /703, loss: 0.635\n",
      "Epoch 58, Iter 500 /703, loss: 0.608\n",
      "Epoch 58, Iter 600 /703, loss: 0.635\n",
      "Epoch 58, Iter 700 /703, loss: 0.593\n",
      "-------- train controller --------\n",
      "0 0.5465746521949768\n",
      "1 0.3028765320777893\n",
      "2 0.19875681400299072\n",
      "3 0.04888958856463432\n",
      "4 0.1321801245212555\n",
      "5 -0.5271419882774353\n",
      "6 2.5964553356170654\n",
      "7 1.8161426782608032\n",
      "8 3.1415998935699463\n",
      "9 1.2866427898406982\n",
      "epoch: 58\n",
      "-------- train child --------\n",
      "ops [3, 3, 2, 3, 3, 2] skips [[], [1], [0, 0], [0, 1, 1], [0, 0, 0, 1], [0, 1, 1, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 59, Iter 100 /703, loss: 1.545\n",
      "Epoch 59, Iter 200 /703, loss: 1.130\n",
      "Epoch 59, Iter 300 /703, loss: 1.057\n",
      "Epoch 59, Iter 400 /703, loss: 0.975\n",
      "Epoch 59, Iter 500 /703, loss: 0.934\n",
      "Epoch 59, Iter 600 /703, loss: 0.915\n",
      "Epoch 59, Iter 700 /703, loss: 0.850\n",
      "-------- train controller --------\n",
      "0 2.2323074340820312\n",
      "1 2.1989381313323975\n",
      "2 3.9185516834259033\n",
      "3 1.8246666193008423\n",
      "4 2.466587781906128\n",
      "5 2.43030047416687\n",
      "6 1.3653819561004639\n",
      "7 1.9808634519577026\n",
      "8 1.3383541107177734\n",
      "9 1.4052900075912476\n",
      "epoch: 59\n",
      "-------- train child --------\n",
      "ops [1, 2, 3, 3, 0, 2] skips [[], [1], [1, 0], [1, 0, 0], [1, 0, 0, 0], [1, 0, 1, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 60, Iter 100 /703, loss: 0.697\n",
      "Epoch 60, Iter 200 /703, loss: 0.647\n",
      "Epoch 60, Iter 300 /703, loss: 0.646\n",
      "Epoch 60, Iter 400 /703, loss: 0.628\n",
      "Epoch 60, Iter 500 /703, loss: 0.608\n",
      "Epoch 60, Iter 600 /703, loss: 0.634\n",
      "Epoch 60, Iter 700 /703, loss: 0.591\n",
      "-------- train controller --------\n",
      "0 0.9782392382621765\n",
      "1 1.6818485260009766\n",
      "2 -0.9818413853645325\n",
      "3 -0.05901212617754936\n",
      "4 1.5073198080062866\n",
      "5 0.5699461698532104\n",
      "6 1.621206283569336\n",
      "7 0.8122223019599915\n",
      "8 0.9090211987495422\n",
      "9 -0.1771044284105301\n",
      "epoch: 60\n",
      "-------- train child --------\n",
      "ops [3, 3, 2, 2, 4, 1] skips [[], [0], [1, 0], [1, 1, 0], [0, 0, 1, 1], [1, 0, 1, 1, 1]]\n",
      "lr= [0.050000000000000266]\n",
      "Epoch 61, Iter 100 /703, loss: 0.925\n",
      "Epoch 61, Iter 200 /703, loss: 0.833\n",
      "Epoch 61, Iter 300 /703, loss: 0.820\n",
      "Epoch 61, Iter 400 /703, loss: 0.797\n",
      "Epoch 61, Iter 500 /703, loss: 0.773\n",
      "Epoch 61, Iter 600 /703, loss: 0.784\n",
      "Epoch 61, Iter 700 /703, loss: 0.755\n",
      "-------- train controller --------\n",
      "0 1.703347086906433\n",
      "1 1.211559534072876\n",
      "2 2.4018032550811768\n",
      "3 1.4201412200927734\n",
      "4 1.0074121952056885\n",
      "5 -0.15324170887470245\n",
      "6 1.4126710891723633\n",
      "7 -0.1964966505765915\n",
      "8 1.5844703912734985\n",
      "9 0.41906020045280457\n",
      "epoch: 61\n",
      "-------- train child --------\n",
      "ops [0, 3, 0, 2, 4, 0] skips [[], [1], [0, 0], [0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1, 0]]\n",
      "lr= [0.025500000000000182]\n",
      "Epoch 62, Iter 100 /703, loss: 1.028\n",
      "Epoch 62, Iter 200 /703, loss: 0.834\n",
      "Epoch 62, Iter 300 /703, loss: 0.804\n",
      "Epoch 62, Iter 400 /703, loss: 0.764\n",
      "Epoch 62, Iter 500 /703, loss: 0.734\n",
      "Epoch 62, Iter 600 /703, loss: 0.751\n",
      "Epoch 62, Iter 700 /703, loss: 0.697\n",
      "-------- train controller --------\n",
      "0 1.1241588592529297\n",
      "1 1.016323208808899\n",
      "2 0.4302138388156891\n",
      "3 1.8570222854614258\n",
      "4 1.6967371702194214\n",
      "5 1.3780449628829956\n",
      "6 1.1181925535202026\n",
      "7 0.19783644378185272\n",
      "8 1.6481469869613647\n",
      "9 0.555541455745697\n",
      "epoch: 62\n",
      "-------- train child --------\n",
      "ops [1, 4, 4, 3, 2, 2] skips [[], [1], [0, 0], [0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 63, Iter 100 /703, loss: 1.331\n",
      "Epoch 63, Iter 200 /703, loss: 1.032\n",
      "Epoch 63, Iter 300 /703, loss: 0.952\n",
      "Epoch 63, Iter 400 /703, loss: 0.892\n",
      "Epoch 63, Iter 500 /703, loss: 0.822\n",
      "Epoch 63, Iter 600 /703, loss: 0.829\n",
      "Epoch 63, Iter 700 /703, loss: 0.772\n",
      "-------- train controller --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.673848032951355\n",
      "1 -0.6092326045036316\n",
      "2 1.3084759712219238\n",
      "3 0.794847309589386\n",
      "4 2.3030173778533936\n",
      "5 0.6458421945571899\n",
      "6 1.2553131580352783\n",
      "7 1.8796303272247314\n",
      "8 1.2603249549865723\n",
      "9 -0.3435123860836029\n",
      "epoch: 63\n",
      "-------- train child --------\n",
      "ops [3, 3, 0, 2, 0, 2] skips [[], [1], [0, 0], [0, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 64, Iter 100 /703, loss: 0.875\n",
      "Epoch 64, Iter 200 /703, loss: 0.771\n",
      "Epoch 64, Iter 300 /703, loss: 0.755\n",
      "Epoch 64, Iter 400 /703, loss: 0.732\n",
      "Epoch 64, Iter 500 /703, loss: 0.694\n",
      "Epoch 64, Iter 600 /703, loss: 0.718\n",
      "Epoch 64, Iter 700 /703, loss: 0.683\n",
      "-------- train controller --------\n",
      "0 0.9141645431518555\n",
      "1 -1.3834741115570068\n",
      "2 0.52494215965271\n",
      "3 1.7904014587402344\n",
      "4 0.03290524706244469\n",
      "5 1.6404606103897095\n",
      "6 1.9648771286010742\n",
      "7 0.9355465173721313\n",
      "8 0.7637934684753418\n",
      "9 0.4281182289123535\n",
      "epoch: 64\n",
      "-------- train child --------\n",
      "ops [1, 1, 2, 1, 0, 2] skips [[], [0], [0, 1], [1, 1, 1], [0, 1, 1, 0], [1, 1, 1, 1, 1]]\n",
      "lr= [0.04999999999999992]\n",
      "Epoch 65, Iter 100 /703, loss: 0.773\n",
      "Epoch 65, Iter 200 /703, loss: 0.695\n",
      "Epoch 65, Iter 300 /703, loss: 0.685\n",
      "Epoch 65, Iter 400 /703, loss: 0.672\n",
      "Epoch 65, Iter 500 /703, loss: 0.651\n",
      "Epoch 65, Iter 600 /703, loss: 0.674\n",
      "Epoch 65, Iter 700 /703, loss: 0.642\n",
      "-------- train controller --------\n",
      "0 0.11313023418188095\n",
      "1 -0.2675751745700836\n",
      "2 -0.10658679157495499\n",
      "3 1.4928038120269775\n",
      "4 1.1245766878128052\n",
      "5 0.09016680717468262\n",
      "6 0.7261885404586792\n",
      "7 1.3565752506256104\n",
      "8 1.031988501548767\n",
      "9 -0.4485486149787903\n",
      "epoch: 65\n",
      "-------- train child --------\n",
      "ops [0, 2, 0, 1, 1, 3] skips [[], [1], [1, 1], [0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 0, 1]]\n",
      "lr= [0.02550000000000019]\n",
      "Epoch 66, Iter 100 /703, loss: 0.887\n",
      "Epoch 66, Iter 200 /703, loss: 0.762\n",
      "Epoch 66, Iter 300 /703, loss: 0.746\n",
      "Epoch 66, Iter 400 /703, loss: 0.717\n",
      "Epoch 66, Iter 500 /703, loss: 0.694\n",
      "Epoch 66, Iter 600 /703, loss: 0.706\n",
      "Epoch 66, Iter 700 /703, loss: 0.660\n",
      "-------- train controller --------\n",
      "0 2.6952197551727295\n",
      "1 1.5331147909164429\n",
      "2 0.3115539848804474\n",
      "3 2.065599203109741\n",
      "4 0.36133113503456116\n",
      "5 0.33005234599113464\n",
      "6 2.7290234565734863\n",
      "7 1.5563373565673828\n",
      "8 0.46101340651512146\n",
      "9 1.7177947759628296\n",
      "epoch: 66\n",
      "-------- train child --------\n",
      "ops [3, 4, 2, 1, 4, 4] skips [[], [0], [0, 1], [1, 0, 0], [1, 0, 1, 1], [1, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 67, Iter 100 /703, loss: 1.478\n",
      "Epoch 67, Iter 200 /703, loss: 1.189\n",
      "Epoch 67, Iter 300 /703, loss: 1.098\n",
      "Epoch 67, Iter 400 /703, loss: 1.044\n",
      "Epoch 67, Iter 500 /703, loss: 0.988\n",
      "Epoch 67, Iter 600 /703, loss: 0.982\n",
      "Epoch 67, Iter 700 /703, loss: 0.938\n",
      "-------- train controller --------\n",
      "0 0.865102231502533\n",
      "1 0.7281913757324219\n",
      "2 1.6871395111083984\n",
      "3 1.2001484632492065\n",
      "4 2.8648903369903564\n",
      "5 -0.922955334186554\n",
      "6 1.3146039247512817\n",
      "7 1.4485721588134766\n",
      "8 0.2520785331726074\n",
      "9 0.9098612666130066\n",
      "epoch: 67\n",
      "-------- train child --------\n",
      "ops [2, 3, 0, 3, 0, 3] skips [[], [1], [1, 0], [0, 1, 0], [1, 0, 0, 0], [0, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 68, Iter 100 /703, loss: 0.716\n",
      "Epoch 68, Iter 200 /703, loss: 0.679\n",
      "Epoch 68, Iter 300 /703, loss: 0.670\n",
      "Epoch 68, Iter 400 /703, loss: 0.653\n",
      "Epoch 68, Iter 500 /703, loss: 0.635\n",
      "Epoch 68, Iter 600 /703, loss: 0.665\n",
      "Epoch 68, Iter 700 /703, loss: 0.627\n",
      "-------- train controller --------\n",
      "0 1.1902722120285034\n",
      "1 2.330134630203247\n",
      "2 1.635604739189148\n",
      "3 0.585832417011261\n",
      "4 1.1746654510498047\n",
      "5 1.5538510084152222\n",
      "6 0.4039747714996338\n",
      "7 0.2024674415588379\n",
      "8 1.9007072448730469\n",
      "9 1.1084553003311157\n",
      "epoch: 68\n",
      "-------- train child --------\n",
      "ops [3, 0, 2, 3, 4, 0] skips [[], [1], [0, 0], [0, 1, 1], [1, 0, 0, 0], [0, 1, 1, 1, 1]]\n",
      "lr= [0.05000000000000029]\n",
      "Epoch 69, Iter 100 /703, loss: 0.913\n",
      "Epoch 69, Iter 200 /703, loss: 0.807\n",
      "Epoch 69, Iter 300 /703, loss: 0.798\n",
      "Epoch 69, Iter 400 /703, loss: 0.781\n",
      "Epoch 69, Iter 500 /703, loss: 0.757\n",
      "Epoch 69, Iter 600 /703, loss: 0.759\n",
      "Epoch 69, Iter 700 /703, loss: 0.744\n",
      "-------- train controller --------\n",
      "0 1.3931466341018677\n",
      "1 1.3584024906158447\n",
      "2 0.7990253567695618\n",
      "3 1.4211775064468384\n",
      "4 -0.11688127368688583\n",
      "5 0.7511964440345764\n",
      "6 1.7362987995147705\n",
      "7 0.17018833756446838\n",
      "8 0.3005731403827667\n",
      "9 1.450241208076477\n",
      "epoch: 69\n",
      "-------- train child --------\n",
      "ops [3, 1, 1, 0, 1, 3] skips [[], [0], [0, 0], [1, 1, 1], [0, 0, 0, 1], [0, 0, 1, 0, 0]]\n",
      "lr= [0.025500000000000203]\n",
      "Epoch 70, Iter 100 /703, loss: 1.027\n",
      "Epoch 70, Iter 200 /703, loss: 0.809\n",
      "Epoch 70, Iter 300 /703, loss: 0.789\n",
      "Epoch 70, Iter 400 /703, loss: 0.765\n",
      "Epoch 70, Iter 500 /703, loss: 0.737\n",
      "Epoch 70, Iter 600 /703, loss: 0.740\n",
      "Epoch 70, Iter 700 /703, loss: 0.705\n",
      "-------- train controller --------\n",
      "0 2.1389710903167725\n",
      "1 1.4280847311019897\n",
      "2 0.650350034236908\n",
      "3 1.610573172569275\n",
      "4 1.2535454034805298\n",
      "5 -0.4996972978115082\n",
      "6 0.38827428221702576\n",
      "7 2.223043203353882\n",
      "8 1.66110360622406\n",
      "9 -0.36404749751091003\n",
      "epoch: 70\n",
      "-------- train child --------\n",
      "ops [3, 3, 4, 2, 0, 0] skips [[], [0], [0, 0], [0, 0, 0], [1, 1, 1, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 71, Iter 100 /703, loss: 1.180\n",
      "Epoch 71, Iter 200 /703, loss: 1.017\n",
      "Epoch 71, Iter 300 /703, loss: 0.986\n",
      "Epoch 71, Iter 400 /703, loss: 0.938\n",
      "Epoch 71, Iter 500 /703, loss: 0.893\n",
      "Epoch 71, Iter 600 /703, loss: 0.884\n",
      "Epoch 71, Iter 700 /703, loss: 0.855\n",
      "-------- train controller --------\n",
      "0 2.2472052574157715\n",
      "1 2.531562089920044\n",
      "2 -0.38984405994415283\n",
      "3 1.272269606590271\n",
      "4 0.375929057598114\n",
      "5 0.2447725534439087\n",
      "6 1.2214984893798828\n",
      "7 2.49031138420105\n",
      "8 1.1410585641860962\n",
      "9 1.412749171257019\n",
      "epoch: 71\n",
      "-------- train child --------\n",
      "ops [2, 2, 0, 4, 0, 4] skips [[], [1], [1, 0], [0, 0, 1], [1, 0, 0, 0], [0, 1, 1, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 72, Iter 100 /703, loss: 0.701\n",
      "Epoch 72, Iter 200 /703, loss: 0.662\n",
      "Epoch 72, Iter 300 /703, loss: 0.648\n",
      "Epoch 72, Iter 400 /703, loss: 0.640\n",
      "Epoch 72, Iter 500 /703, loss: 0.609\n",
      "Epoch 72, Iter 600 /703, loss: 0.643\n",
      "Epoch 72, Iter 700 /703, loss: 0.602\n",
      "-------- train controller --------\n",
      "0 0.3018546998500824\n",
      "1 0.9244979023933411\n",
      "2 0.013587904162704945\n",
      "3 0.7138957977294922\n",
      "4 0.6119378209114075\n",
      "5 0.5837251543998718\n",
      "6 0.5431010127067566\n",
      "7 1.9343732595443726\n",
      "8 1.539576530456543\n",
      "9 2.7952423095703125\n",
      "epoch: 72\n",
      "-------- train child --------\n",
      "ops [1, 1, 3, 1, 4, 0] skips [[], [0], [0, 0], [1, 1, 0], [1, 0, 1, 1], [0, 1, 0, 0, 0]]\n",
      "lr= [0.049999999999999954]\n",
      "Epoch 73, Iter 100 /703, loss: 0.794\n",
      "Epoch 73, Iter 200 /703, loss: 0.726\n",
      "Epoch 73, Iter 300 /703, loss: 0.723\n",
      "Epoch 73, Iter 400 /703, loss: 0.709\n",
      "Epoch 73, Iter 500 /703, loss: 0.686\n",
      "Epoch 73, Iter 600 /703, loss: 0.704\n",
      "Epoch 73, Iter 700 /703, loss: 0.682\n",
      "-------- train controller --------\n",
      "0 -0.40362802147865295\n",
      "1 -0.3582709729671478\n",
      "2 -0.11694707721471786\n",
      "3 1.864375352859497\n",
      "4 -0.36325594782829285\n",
      "5 1.160385012626648\n",
      "6 -0.06009349972009659\n",
      "7 0.12810854613780975\n",
      "8 -0.42412710189819336\n",
      "9 0.8820104598999023\n",
      "epoch: 73\n",
      "-------- train child --------\n",
      "ops [1, 2, 0, 4, 2, 2] skips [[], [1], [0, 0], [1, 0, 0], [1, 1, 0, 1], [0, 1, 1, 1, 0]]\n",
      "lr= [0.025500000000000217]\n",
      "Epoch 74, Iter 100 /703, loss: 0.673\n",
      "Epoch 74, Iter 200 /703, loss: 0.617\n",
      "Epoch 74, Iter 300 /703, loss: 0.603\n",
      "Epoch 74, Iter 400 /703, loss: 0.591\n",
      "Epoch 74, Iter 500 /703, loss: 0.565\n",
      "Epoch 74, Iter 600 /703, loss: 0.587\n",
      "Epoch 74, Iter 700 /703, loss: 0.545\n",
      "-------- train controller --------\n",
      "0 1.9662185907363892\n",
      "1 0.738559901714325\n",
      "2 0.7727689743041992\n",
      "3 1.578843355178833\n",
      "4 0.6854775547981262\n",
      "5 -1.6284284591674805\n",
      "6 -0.011463427916169167\n",
      "7 1.1271833181381226\n",
      "8 0.2498379498720169\n",
      "9 0.35416388511657715\n",
      "epoch: 74\n",
      "-------- train child --------\n",
      "ops [0, 0, 2, 0, 1, 1] skips [[], [0], [0, 0], [0, 0, 0], [1, 1, 0, 0], [1, 0, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 75, Iter 100 /703, loss: 1.092\n",
      "Epoch 75, Iter 200 /703, loss: 0.889\n",
      "Epoch 75, Iter 300 /703, loss: 0.827\n",
      "Epoch 75, Iter 400 /703, loss: 0.772\n",
      "Epoch 75, Iter 500 /703, loss: 0.726\n",
      "Epoch 75, Iter 600 /703, loss: 0.730\n",
      "Epoch 75, Iter 700 /703, loss: 0.682\n",
      "-------- train controller --------\n",
      "0 3.3837451934814453\n",
      "1 -0.2395258992910385\n",
      "2 1.327478051185608\n",
      "3 1.9973148107528687\n",
      "4 0.19813178479671478\n",
      "5 1.4788169860839844\n",
      "6 1.776901125907898\n",
      "7 0.7320910692214966\n",
      "8 -0.015130233950912952\n",
      "9 0.8939372897148132\n",
      "epoch: 75\n",
      "-------- train child --------\n",
      "ops [1, 0, 1, 0, 1, 0] skips [[], [0], [0, 1], [1, 0, 0], [0, 1, 1, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.0255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Iter 100 /703, loss: 0.764\n",
      "Epoch 76, Iter 200 /703, loss: 0.676\n",
      "Epoch 76, Iter 300 /703, loss: 0.680\n",
      "Epoch 76, Iter 400 /703, loss: 0.660\n",
      "Epoch 76, Iter 500 /703, loss: 0.633\n",
      "Epoch 76, Iter 600 /703, loss: 0.654\n",
      "Epoch 76, Iter 700 /703, loss: 0.623\n",
      "-------- train controller --------\n",
      "0 0.36815568804740906\n",
      "1 -0.04733681678771973\n",
      "2 2.2121732234954834\n",
      "3 1.0700873136520386\n",
      "4 1.0275686979293823\n",
      "5 -0.2751164436340332\n",
      "6 0.8712366223335266\n",
      "7 -0.10570140182971954\n",
      "8 1.4463050365447998\n",
      "9 -0.6260887980461121\n",
      "epoch: 76\n",
      "-------- train child --------\n",
      "ops [4, 4, 2, 2, 4, 2] skips [[], [0], [1, 0], [0, 0, 0], [1, 0, 0, 0], [1, 1, 1, 1, 1]]\n",
      "lr= [0.05000000000000031]\n",
      "Epoch 77, Iter 100 /703, loss: 0.951\n",
      "Epoch 77, Iter 200 /703, loss: 0.830\n",
      "Epoch 77, Iter 300 /703, loss: 0.810\n",
      "Epoch 77, Iter 400 /703, loss: 0.782\n",
      "Epoch 77, Iter 500 /703, loss: 0.749\n",
      "Epoch 77, Iter 600 /703, loss: 0.779\n",
      "Epoch 77, Iter 700 /703, loss: 0.741\n",
      "-------- train controller --------\n",
      "0 0.08655672520399094\n",
      "1 0.49678388237953186\n",
      "2 -0.017880726605653763\n",
      "3 0.02823023870587349\n",
      "4 0.5420195460319519\n",
      "5 1.0952423810958862\n",
      "6 -0.5822036862373352\n",
      "7 0.7139550447463989\n",
      "8 -0.8998327255249023\n",
      "9 0.3975915014743805\n",
      "epoch: 77\n",
      "-------- train child --------\n",
      "ops [1, 0, 4, 0, 1, 3] skips [[], [0], [1, 0], [1, 0, 1], [0, 0, 0, 0], [1, 1, 0, 1, 0]]\n",
      "lr= [0.025500000000000224]\n",
      "Epoch 78, Iter 100 /703, loss: 0.753\n",
      "Epoch 78, Iter 200 /703, loss: 0.673\n",
      "Epoch 78, Iter 300 /703, loss: 0.660\n",
      "Epoch 78, Iter 400 /703, loss: 0.650\n",
      "Epoch 78, Iter 500 /703, loss: 0.623\n",
      "Epoch 78, Iter 600 /703, loss: 0.637\n",
      "Epoch 78, Iter 700 /703, loss: 0.610\n",
      "-------- train controller --------\n",
      "0 -0.06826295703649521\n",
      "1 -0.05625021457672119\n",
      "2 0.6567168235778809\n",
      "3 0.24336262047290802\n",
      "4 -0.9303116798400879\n",
      "5 2.3027706146240234\n",
      "6 0.45392081141471863\n",
      "7 -0.034599196165800095\n",
      "8 -0.7411286234855652\n",
      "9 0.8770284652709961\n",
      "epoch: 78\n",
      "-------- train child --------\n",
      "ops [3, 2, 2, 1, 2, 0] skips [[], [0], [0, 1], [0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 79, Iter 100 /703, loss: 1.124\n",
      "Epoch 79, Iter 200 /703, loss: 0.906\n",
      "Epoch 79, Iter 300 /703, loss: 0.863\n",
      "Epoch 79, Iter 400 /703, loss: 0.820\n",
      "Epoch 79, Iter 500 /703, loss: 0.774\n",
      "Epoch 79, Iter 600 /703, loss: 0.774\n",
      "Epoch 79, Iter 700 /703, loss: 0.725\n",
      "-------- train controller --------\n",
      "0 0.5782880783081055\n",
      "1 0.4882826805114746\n",
      "2 1.5898505449295044\n",
      "3 1.4970024824142456\n",
      "4 2.2813491821289062\n",
      "5 0.8740003705024719\n",
      "6 1.2623733282089233\n",
      "7 0.862882137298584\n",
      "8 1.6197757720947266\n",
      "9 -0.9250456094741821\n",
      "epoch: 79\n",
      "-------- train child --------\n",
      "ops [3, 4, 1, 0, 0, 2] skips [[], [1], [0, 0], [0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 1, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 80, Iter 100 /703, loss: 1.130\n",
      "Epoch 80, Iter 200 /703, loss: 0.847\n",
      "Epoch 80, Iter 300 /703, loss: 0.818\n",
      "Epoch 80, Iter 400 /703, loss: 0.778\n",
      "Epoch 80, Iter 500 /703, loss: 0.737\n",
      "Epoch 80, Iter 600 /703, loss: 0.756\n",
      "Epoch 80, Iter 700 /703, loss: 0.722\n",
      "-------- train controller --------\n",
      "0 0.006080913823097944\n",
      "1 -0.20747266709804535\n",
      "2 1.1941478252410889\n",
      "3 0.6100452542304993\n",
      "4 -0.37709736824035645\n",
      "5 0.36100655794143677\n",
      "6 0.8651847243309021\n",
      "7 0.7363607287406921\n",
      "8 -0.4536378085613251\n",
      "9 -0.10757417976856232\n",
      "epoch: 80\n",
      "-------- train child --------\n",
      "ops [2, 4, 4, 2, 4, 4] skips [[], [0], [0, 0], [0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0, 0]]\n",
      "lr= [0.049999999999999975]\n",
      "Epoch 81, Iter 100 /703, loss: 0.821\n",
      "Epoch 81, Iter 200 /703, loss: 0.764\n",
      "Epoch 81, Iter 300 /703, loss: 0.760\n",
      "Epoch 81, Iter 400 /703, loss: 0.729\n",
      "Epoch 81, Iter 500 /703, loss: 0.716\n",
      "Epoch 81, Iter 600 /703, loss: 0.752\n",
      "Epoch 81, Iter 700 /703, loss: 0.703\n",
      "-------- train controller --------\n",
      "0 0.8381046652793884\n",
      "1 -0.1102510467171669\n",
      "2 1.5601822137832642\n",
      "3 0.3650428354740143\n",
      "4 1.3168407678604126\n",
      "5 -0.1418888121843338\n",
      "6 -0.08895356953144073\n",
      "7 -0.45738720893859863\n",
      "8 0.7391646504402161\n",
      "9 0.8868071436882019\n",
      "epoch: 81\n",
      "-------- train child --------\n",
      "ops [2, 2, 3, 3, 3, 4] skips [[], [1], [1, 0], [0, 0, 0], [0, 0, 0, 0], [1, 0, 0, 1, 1]]\n",
      "lr= [0.025500000000000238]\n",
      "Epoch 82, Iter 100 /703, loss: 0.702\n",
      "Epoch 82, Iter 200 /703, loss: 0.651\n",
      "Epoch 82, Iter 300 /703, loss: 0.635\n",
      "Epoch 82, Iter 400 /703, loss: 0.628\n",
      "Epoch 82, Iter 500 /703, loss: 0.603\n",
      "Epoch 82, Iter 600 /703, loss: 0.624\n",
      "Epoch 82, Iter 700 /703, loss: 0.584\n",
      "-------- train controller --------\n",
      "0 0.9230079650878906\n",
      "1 0.34699803590774536\n",
      "2 0.8020801544189453\n",
      "3 0.6658726930618286\n",
      "4 -1.1989072561264038\n",
      "5 1.1705073118209839\n",
      "6 0.8827709555625916\n",
      "7 1.3230352401733398\n",
      "8 1.452622413635254\n",
      "9 0.5807464718818665\n",
      "epoch: 82\n",
      "-------- train child --------\n",
      "ops [3, 3, 1, 4, 2, 2] skips [[], [0], [0, 0], [1, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 83, Iter 100 /703, loss: 1.461\n",
      "Epoch 83, Iter 200 /703, loss: 1.049\n",
      "Epoch 83, Iter 300 /703, loss: 0.974\n",
      "Epoch 83, Iter 400 /703, loss: 0.895\n",
      "Epoch 83, Iter 500 /703, loss: 0.844\n",
      "Epoch 83, Iter 600 /703, loss: 0.827\n",
      "Epoch 83, Iter 700 /703, loss: 0.775\n",
      "-------- train controller --------\n",
      "0 0.5958472490310669\n",
      "1 1.3053802251815796\n",
      "2 0.5669968128204346\n",
      "3 0.6624774932861328\n",
      "4 1.9122806787490845\n",
      "5 -0.31971752643585205\n",
      "6 -0.39333269000053406\n",
      "7 2.3942337036132812\n",
      "8 1.5121294260025024\n",
      "9 1.9187564849853516\n",
      "epoch: 83\n",
      "-------- train child --------\n",
      "ops [4, 0, 2, 0, 2, 2] skips [[], [0], [1, 0], [0, 0, 1], [1, 0, 0, 0], [1, 1, 0, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 84, Iter 100 /703, loss: 0.827\n",
      "Epoch 84, Iter 200 /703, loss: 0.757\n",
      "Epoch 84, Iter 300 /703, loss: 0.742\n",
      "Epoch 84, Iter 400 /703, loss: 0.712\n",
      "Epoch 84, Iter 500 /703, loss: 0.679\n",
      "Epoch 84, Iter 600 /703, loss: 0.707\n",
      "Epoch 84, Iter 700 /703, loss: 0.677\n",
      "-------- train controller --------\n",
      "0 2.1774771213531494\n",
      "1 -0.29069992899894714\n",
      "2 0.18372252583503723\n",
      "3 1.5246027708053589\n",
      "4 1.635752558708191\n",
      "5 1.4141005277633667\n",
      "6 0.26562246680259705\n",
      "7 0.9378564953804016\n",
      "8 -0.19975042343139648\n",
      "9 0.7194448709487915\n",
      "epoch: 84\n",
      "-------- train child --------\n",
      "ops [0, 3, 1, 3, 0, 1] skips [[], [0], [1, 1], [1, 1, 1], [0, 0, 0, 0], [0, 0, 1, 1, 0]]\n",
      "lr= [0.049999999999999635]\n",
      "Epoch 85, Iter 100 /703, loss: 0.927\n",
      "Epoch 85, Iter 200 /703, loss: 0.826\n",
      "Epoch 85, Iter 300 /703, loss: 0.800\n",
      "Epoch 85, Iter 400 /703, loss: 0.769\n",
      "Epoch 85, Iter 500 /703, loss: 0.734\n",
      "Epoch 85, Iter 600 /703, loss: 0.759\n",
      "Epoch 85, Iter 700 /703, loss: 0.726\n",
      "-------- train controller --------\n",
      "0 -0.0908195748925209\n",
      "1 1.60405433177948\n",
      "2 1.0945689678192139\n",
      "3 -0.3217250406742096\n",
      "4 -0.14467251300811768\n",
      "5 0.8205111622810364\n",
      "6 0.8705536127090454\n",
      "7 0.879203736782074\n",
      "8 0.750623345375061\n",
      "9 0.7248668074607849\n",
      "epoch: 85\n",
      "-------- train child --------\n",
      "ops [1, 0, 4, 1, 1, 1] skips [[], [0], [1, 1], [1, 0, 0], [0, 1, 1, 0], [1, 0, 0, 0, 1]]\n",
      "lr= [0.0254999999999999]\n",
      "Epoch 86, Iter 100 /703, loss: 0.675\n",
      "Epoch 86, Iter 200 /703, loss: 0.637\n",
      "Epoch 86, Iter 300 /703, loss: 0.621\n",
      "Epoch 86, Iter 400 /703, loss: 0.614\n",
      "Epoch 86, Iter 500 /703, loss: 0.593\n",
      "Epoch 86, Iter 600 /703, loss: 0.609\n",
      "Epoch 86, Iter 700 /703, loss: 0.580\n",
      "-------- train controller --------\n",
      "0 0.6925097703933716\n",
      "1 -0.3707883954048157\n",
      "2 1.5855388641357422\n",
      "3 -0.6788662075996399\n",
      "4 0.8833465576171875\n",
      "5 0.42582160234451294\n",
      "6 0.5453649759292603\n",
      "7 0.4762274920940399\n",
      "8 1.4454501867294312\n",
      "9 0.3445168137550354\n",
      "epoch: 86\n",
      "-------- train child --------\n",
      "ops [3, 1, 3, 0, 2, 1] skips [[], [1], [0, 0], [1, 0, 1], [1, 1, 0, 0], [0, 0, 0, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 87, Iter 100 /703, loss: 1.039\n",
      "Epoch 87, Iter 200 /703, loss: 0.840\n",
      "Epoch 87, Iter 300 /703, loss: 0.811\n",
      "Epoch 87, Iter 400 /703, loss: 0.758\n",
      "Epoch 87, Iter 500 /703, loss: 0.730\n",
      "Epoch 87, Iter 600 /703, loss: 0.726\n",
      "Epoch 87, Iter 700 /703, loss: 0.673\n",
      "-------- train controller --------\n",
      "0 0.792086124420166\n",
      "1 1.0357311964035034\n",
      "2 1.1746015548706055\n",
      "3 1.2580945491790771\n",
      "4 0.4583685100078583\n",
      "5 0.5586895942687988\n",
      "6 1.0184252262115479\n",
      "7 2.422657012939453\n",
      "8 -0.39655038714408875\n",
      "9 1.9538862705230713\n",
      "epoch: 87\n",
      "-------- train child --------\n",
      "ops [1, 1, 2, 4, 4, 0] skips [[], [1], [1, 1], [1, 0, 1], [1, 0, 0, 1], [0, 0, 1, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 88, Iter 100 /703, loss: 0.665\n",
      "Epoch 88, Iter 200 /703, loss: 0.626\n",
      "Epoch 88, Iter 300 /703, loss: 0.627\n",
      "Epoch 88, Iter 400 /703, loss: 0.610\n",
      "Epoch 88, Iter 500 /703, loss: 0.587\n",
      "Epoch 88, Iter 600 /703, loss: 0.608\n",
      "Epoch 88, Iter 700 /703, loss: 0.578\n",
      "-------- train controller --------\n",
      "0 1.119417667388916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.49866676330566406\n",
      "2 2.13791823387146\n",
      "3 2.31473708152771\n",
      "4 1.4980701208114624\n",
      "5 -0.36563196778297424\n",
      "6 1.9286867380142212\n",
      "7 0.19664669036865234\n",
      "8 1.2737441062927246\n",
      "9 0.735497236251831\n",
      "epoch: 88\n",
      "-------- train child --------\n",
      "ops [4, 3, 1, 4, 2, 1] skips [[], [0], [0, 0], [0, 0, 0], [1, 1, 0, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.05000000000000069]\n",
      "Epoch 89, Iter 100 /703, loss: 1.028\n",
      "Epoch 89, Iter 200 /703, loss: 0.866\n",
      "Epoch 89, Iter 300 /703, loss: 0.835\n",
      "Epoch 89, Iter 400 /703, loss: 0.820\n",
      "Epoch 89, Iter 500 /703, loss: 0.775\n",
      "Epoch 89, Iter 600 /703, loss: 0.797\n",
      "Epoch 89, Iter 700 /703, loss: 0.756\n",
      "-------- train controller --------\n",
      "0 0.8970404863357544\n",
      "1 0.24854981899261475\n",
      "2 0.3365926444530487\n",
      "3 0.1687115728855133\n",
      "4 1.14104425907135\n",
      "5 -1.8611043691635132\n",
      "6 -0.07821585983037949\n",
      "7 -0.8824300169944763\n",
      "8 -0.3203905522823334\n",
      "9 1.2530691623687744\n",
      "epoch: 89\n",
      "-------- train child --------\n",
      "ops [4, 0, 3, 0, 2, 2] skips [[], [1], [1, 1], [1, 1, 1], [1, 0, 0, 0], [1, 0, 1, 1, 0]]\n",
      "lr= [0.025500000000000262]\n",
      "Epoch 90, Iter 100 /703, loss: 0.804\n",
      "Epoch 90, Iter 200 /703, loss: 0.723\n",
      "Epoch 90, Iter 300 /703, loss: 0.704\n",
      "Epoch 90, Iter 400 /703, loss: 0.691\n",
      "Epoch 90, Iter 500 /703, loss: 0.658\n",
      "Epoch 90, Iter 600 /703, loss: 0.689\n",
      "Epoch 90, Iter 700 /703, loss: 0.657\n",
      "-------- train controller --------\n",
      "0 -0.21741099655628204\n",
      "1 0.5899764895439148\n",
      "2 -0.6667577624320984\n",
      "3 0.1273314505815506\n",
      "4 0.15480172634124756\n",
      "5 0.38061919808387756\n",
      "6 1.683115005493164\n",
      "7 0.8937172293663025\n",
      "8 1.0440844297409058\n",
      "9 -0.5789408683776855\n",
      "epoch: 90\n",
      "-------- train child --------\n",
      "ops [2, 4, 0, 1, 1, 2] skips [[], [1], [1, 0], [1, 0, 1], [0, 1, 0, 0], [0, 1, 1, 0, 1]]\n",
      "lr= [0.001]\n",
      "Epoch 91, Iter 100 /703, loss: 0.831\n",
      "Epoch 91, Iter 200 /703, loss: 0.720\n",
      "Epoch 91, Iter 300 /703, loss: 0.690\n",
      "Epoch 91, Iter 400 /703, loss: 0.646\n",
      "Epoch 91, Iter 500 /703, loss: 0.613\n",
      "Epoch 91, Iter 600 /703, loss: 0.617\n",
      "Epoch 91, Iter 700 /703, loss: 0.558\n",
      "-------- train controller --------\n",
      "0 0.40677833557128906\n",
      "1 2.551863431930542\n",
      "2 0.491677850484848\n",
      "3 0.08342745155096054\n",
      "4 1.1231788396835327\n",
      "5 0.025995081290602684\n",
      "6 -0.5595642924308777\n",
      "7 1.0781396627426147\n",
      "8 2.0821657180786133\n",
      "9 1.2003841400146484\n",
      "epoch: 91\n",
      "-------- train child --------\n",
      "ops [2, 2, 3, 2, 1, 3] skips [[], [0], [0, 0], [1, 1, 1], [1, 0, 0, 0], [1, 0, 0, 1, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 92, Iter 100 /703, loss: 0.664\n",
      "Epoch 92, Iter 200 /703, loss: 0.630\n",
      "Epoch 92, Iter 300 /703, loss: 0.618\n",
      "Epoch 92, Iter 400 /703, loss: 0.609\n",
      "Epoch 92, Iter 500 /703, loss: 0.587\n",
      "Epoch 92, Iter 600 /703, loss: 0.610\n",
      "Epoch 92, Iter 700 /703, loss: 0.569\n",
      "-------- train controller --------\n",
      "0 0.061628129333257675\n",
      "1 0.44121742248535156\n",
      "2 2.0819242000579834\n",
      "3 -0.26450401544570923\n",
      "4 2.246546983718872\n",
      "5 0.7775475382804871\n",
      "6 0.26268574595451355\n",
      "7 0.1188076063990593\n",
      "8 0.7312476634979248\n",
      "9 0.9925773739814758\n",
      "epoch: 92\n",
      "-------- train child --------\n",
      "ops [4, 2, 2, 3, 1, 2] skips [[], [0], [0, 1], [1, 1, 0], [0, 0, 1, 0], [1, 0, 0, 0, 1]]\n",
      "lr= [0.05000000000000036]\n",
      "Epoch 93, Iter 100 /703, loss: 0.920\n",
      "Epoch 93, Iter 200 /703, loss: 0.798\n",
      "Epoch 93, Iter 300 /703, loss: 0.781\n",
      "Epoch 93, Iter 400 /703, loss: 0.752\n",
      "Epoch 93, Iter 500 /703, loss: 0.723\n",
      "Epoch 93, Iter 600 /703, loss: 0.747\n",
      "Epoch 93, Iter 700 /703, loss: 0.711\n",
      "-------- train controller --------\n",
      "0 -0.1346743106842041\n",
      "1 -0.8165647387504578\n",
      "2 -0.01365573424845934\n",
      "3 0.18217897415161133\n",
      "4 0.7290388345718384\n",
      "5 0.3634984791278839\n",
      "6 -0.3027486205101013\n",
      "7 -0.4384582042694092\n",
      "8 0.15317316353321075\n",
      "9 0.496592253446579\n",
      "epoch: 93\n",
      "-------- train child --------\n",
      "ops [4, 4, 1, 3, 1, 0] skips [[], [1], [0, 0], [0, 0, 0], [1, 1, 0, 1], [0, 1, 0, 0, 1]]\n",
      "lr= [0.025500000000000626]\n",
      "Epoch 94, Iter 100 /703, loss: 1.021\n",
      "Epoch 94, Iter 200 /703, loss: 0.876\n",
      "Epoch 94, Iter 300 /703, loss: 0.847\n",
      "Epoch 94, Iter 400 /703, loss: 0.818\n",
      "Epoch 94, Iter 500 /703, loss: 0.787\n",
      "Epoch 94, Iter 600 /703, loss: 0.807\n",
      "Epoch 94, Iter 700 /703, loss: 0.771\n",
      "-------- train controller --------\n",
      "0 1.0147711038589478\n",
      "1 0.5929009914398193\n",
      "2 -0.2870943248271942\n",
      "3 1.3563958406448364\n",
      "4 -0.37402546405792236\n",
      "5 0.8824106454849243\n",
      "6 -0.7164730429649353\n",
      "7 0.6029431223869324\n",
      "8 0.7545806765556335\n",
      "9 -0.09425429254770279\n",
      "epoch: 94\n",
      "-------- train child --------\n",
      "ops [3, 0, 3, 2, 3, 2] skips [[], [0], [0, 1], [1, 1, 1], [0, 1, 1, 0], [1, 0, 1, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 95, Iter 100 /703, loss: 0.928\n",
      "Epoch 95, Iter 200 /703, loss: 0.817\n",
      "Epoch 95, Iter 300 /703, loss: 0.791\n",
      "Epoch 95, Iter 400 /703, loss: 0.765\n",
      "Epoch 95, Iter 500 /703, loss: 0.716\n",
      "Epoch 95, Iter 600 /703, loss: 0.711\n",
      "Epoch 95, Iter 700 /703, loss: 0.653\n",
      "-------- train controller --------\n",
      "0 -0.1169985756278038\n",
      "1 2.2261388301849365\n",
      "2 1.6323566436767578\n",
      "3 0.7871162295341492\n",
      "4 0.007669401355087757\n",
      "5 1.574910044670105\n",
      "6 0.9393168687820435\n",
      "7 -0.31037312746047974\n",
      "8 0.7563060522079468\n",
      "9 1.1861408948898315\n",
      "epoch: 95\n",
      "-------- train child --------\n",
      "ops [4, 4, 2, 3, 4, 1] skips [[], [1], [0, 1], [1, 0, 0], [1, 1, 0, 1], [0, 0, 1, 0, 0]]\n",
      "lr= [0.0255]\n",
      "Epoch 96, Iter 100 /703, loss: 0.821\n",
      "Epoch 96, Iter 200 /703, loss: 0.772\n",
      "Epoch 96, Iter 300 /703, loss: 0.753\n",
      "Epoch 96, Iter 400 /703, loss: 0.741\n",
      "Epoch 96, Iter 500 /703, loss: 0.711\n",
      "Epoch 96, Iter 600 /703, loss: 0.731\n",
      "Epoch 96, Iter 700 /703, loss: 0.697\n",
      "-------- train controller --------\n",
      "0 0.7649897933006287\n",
      "1 0.601533830165863\n",
      "2 -0.24298648536205292\n",
      "3 0.7495004534721375\n",
      "4 0.0002028942108154297\n",
      "5 -1.0097922086715698\n",
      "6 -0.13742071390151978\n",
      "7 1.2538328170776367\n",
      "8 -0.6442328095436096\n",
      "9 0.5691666007041931\n",
      "epoch: 96\n",
      "-------- train child --------\n",
      "ops [2, 0, 0, 2, 1, 2] skips [[], [0], [1, 1], [1, 1, 1], [1, 0, 1, 0], [1, 1, 0, 0, 1]]\n",
      "lr= [0.05000000000000002]\n",
      "Epoch 97, Iter 100 /703, loss: 0.732\n",
      "Epoch 97, Iter 200 /703, loss: 0.674\n",
      "Epoch 97, Iter 300 /703, loss: 0.667\n",
      "Epoch 97, Iter 400 /703, loss: 0.652\n",
      "Epoch 97, Iter 500 /703, loss: 0.614\n",
      "Epoch 97, Iter 600 /703, loss: 0.651\n",
      "Epoch 97, Iter 700 /703, loss: 0.607\n",
      "-------- train controller --------\n",
      "0 0.5676438212394714\n",
      "1 -0.19760465621948242\n",
      "2 0.11305301636457443\n",
      "3 0.6176225543022156\n",
      "4 -0.5276658535003662\n",
      "5 0.30061110854148865\n",
      "6 0.8407206535339355\n",
      "7 0.7153018712997437\n",
      "8 1.6661847829818726\n",
      "9 0.6712659001350403\n",
      "epoch: 97\n",
      "-------- train child --------\n",
      "ops [1, 0, 2, 4, 1, 3] skips [[], [0], [0, 1], [0, 0, 0], [1, 1, 1, 1], [0, 0, 0, 1, 0]]\n",
      "lr= [0.025500000000000286]\n",
      "Epoch 98, Iter 100 /703, loss: 0.766\n",
      "Epoch 98, Iter 200 /703, loss: 0.675\n",
      "Epoch 98, Iter 300 /703, loss: 0.657\n",
      "Epoch 98, Iter 400 /703, loss: 0.642\n",
      "Epoch 98, Iter 500 /703, loss: 0.612\n",
      "Epoch 98, Iter 600 /703, loss: 0.643\n",
      "Epoch 98, Iter 700 /703, loss: 0.607\n",
      "-------- train controller --------\n",
      "0 0.041837967932224274\n",
      "1 -0.7319060564041138\n",
      "2 -1.3422586917877197\n",
      "3 1.3212112188339233\n",
      "4 -0.006806802935898304\n",
      "5 -0.7760634422302246\n",
      "6 -1.0034205913543701\n",
      "7 0.20317716896533966\n",
      "8 0.8158079385757446\n",
      "9 -1.3690117597579956\n",
      "epoch: 98\n",
      "-------- train child --------\n",
      "ops [0, 0, 4, 1, 4, 2] skips [[], [0], [1, 0], [0, 0, 1], [0, 1, 0, 1], [0, 0, 0, 0, 0]]\n",
      "lr= [0.001]\n",
      "Epoch 99, Iter 100 /703, loss: 1.102\n",
      "Epoch 99, Iter 200 /703, loss: 0.867\n",
      "Epoch 99, Iter 300 /703, loss: 0.810\n",
      "Epoch 99, Iter 400 /703, loss: 0.757\n",
      "Epoch 99, Iter 500 /703, loss: 0.698\n",
      "Epoch 99, Iter 600 /703, loss: 0.708\n",
      "Epoch 99, Iter 700 /703, loss: 0.646\n",
      "-------- train controller --------\n",
      "0 -0.44187435507774353\n",
      "1 0.037006426602602005\n",
      "2 1.1150003671646118\n",
      "3 1.4469273090362549\n",
      "4 -0.4526841342449188\n",
      "5 0.799269437789917\n",
      "6 2.4257607460021973\n",
      "7 -0.21111802756786346\n",
      "8 1.2772612571716309\n",
      "9 -1.5331865549087524\n",
      "epoch: 99\n",
      "-------- train child --------\n",
      "ops [1, 4, 2, 0, 1, 4] skips [[], [0], [1, 1], [1, 0, 0], [1, 0, 1, 0], [1, 0, 1, 0, 1]]\n",
      "lr= [0.0255]\n",
      "Epoch 100, Iter 100 /703, loss: 0.660\n",
      "Epoch 100, Iter 200 /703, loss: 0.627\n",
      "Epoch 100, Iter 300 /703, loss: 0.613\n",
      "Epoch 100, Iter 400 /703, loss: 0.615\n",
      "Epoch 100, Iter 500 /703, loss: 0.590\n",
      "Epoch 100, Iter 600 /703, loss: 0.617\n",
      "Epoch 100, Iter 700 /703, loss: 0.582\n",
      "-------- train controller --------\n",
      "0 1.3916645050048828\n",
      "1 1.3268043994903564\n",
      "2 1.2994409799575806\n",
      "3 0.455339640378952\n",
      "4 0.3878802955150604\n",
      "5 2.090265989303589\n",
      "6 1.0437730550765991\n",
      "7 -0.06931667774915695\n",
      "8 1.7956756353378296\n",
      "9 -0.7870703339576721\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50,100):\n",
    "    print(\"epoch:\",epoch)\n",
    "    print('-------- train child --------')\n",
    "    #sample an arch\n",
    "    ctrl.ctrl.net_sample()\n",
    "    ops = ctrl.ctrl.ops\n",
    "    skips = ctrl.ctrl.skips\n",
    "    print('ops',ops,'skips',skips)\n",
    "    child_loss.append(child.train_epoch(ops,skips, train_imgs, train_labels, epoch, train_step))\n",
    "    \n",
    "    if (epoch + 1) % config.ctrl_train_every_epochs == 0:\n",
    "        print('-------- train controller --------')\n",
    "        ctrl_loss.append(ctrl.train_epoch(child, valid_imgs, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "rc('mathtext', default='regular')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "lns1 = ax.plot(np.arange(len(ctrl_loss)),ctrl_loss, '-', linewidth=0.7,color='green',label = 'Controller')\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(np.arange(len(child_loss)),child_loss, '-', linewidth=0.7,color='red',label = 'CNN')\n",
    "\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=0)\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildChildModel(nn.Module):\n",
    "    def __init__(self,\n",
    "               class_num,\n",
    "               ops,\n",
    "               skips,\n",
    "               num_layers=6,\n",
    "               out_channels=24,\n",
    "               batch_size=32\n",
    "              ):\n",
    "        super(BuildChildModel, self).__init__() \n",
    "        self.class_num = class_num \n",
    "        self.num_layers = num_layers \n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.ops  = ops\n",
    "        self.skips = skips\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layers = self._build_layer(self.class_num)\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size = 3, padding=(1,1),stride=1),\n",
    "            nn.BatchNorm2d(num_features=self.out_channels),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(self.out_channels, class_num, bias=True) \n",
    "        \n",
    "\n",
    "    def _build_layer(self, class_num):\n",
    "        layer = []\n",
    "        \n",
    "        # major part of the graph consisting of all NasLayers\n",
    "        for i in range(self.num_layers):\n",
    "            if self.ops[i] == 0:\n",
    "                kernel = nn.Conv2d(\n",
    "                in_channels=self.out_channels, \n",
    "                out_channels=self.out_channels, \n",
    "                kernel_size = 3, \n",
    "                padding=(1,1), # (3-1)/2\n",
    "                stride=1)\n",
    "            elif self.ops[i] == 1:\n",
    "                kernel = nn.Conv2d(\n",
    "                    in_channels=self.out_channels, \n",
    "                    out_channels=self.out_channels, \n",
    "                    kernel_size = 5, \n",
    "                    padding=(2,2), # (5-1)/2\n",
    "                    stride=1)\n",
    "            elif self.ops[i] == 2:\n",
    "                kernel = nn.AvgPool2d(\n",
    "                    kernel_size=3, \n",
    "                    padding=(1,1),\n",
    "                    stride=1)\n",
    "            elif self.ops[i] == 3:\n",
    "                kernel = nn.MaxPool2d(\n",
    "                    kernel_size=3, \n",
    "                    padding=(1,1),\n",
    "                    stride=1)\n",
    "            layer.append(kernel)\n",
    "            # bn_out\n",
    "            if (self.ops[i] == 0) or (self.ops[i] == 1):\n",
    "                layer.append(nn.ReLU(inplace = True))\n",
    "                layer.append()\n",
    "        # create a ModuleList, or the parameters cannot be added\n",
    "        layer = nn.ModuleList(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "    # TODO: no reduction !\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        run (like forward) a child model determined by sample_arch\n",
    "        Args:\n",
    "            sample_arch: a list consisting of 2 * num_layers elements\n",
    "                op_id = sample_arch[2k]: operation id\n",
    "                skip = sample_arch[2k + 1]: element i of such binary vector \n",
    "                    is used to describe whether the previous layer i is used \n",
    "                    as an input\n",
    "            x: input of the child model\n",
    "        Return:\n",
    "            x: output of the child model\n",
    "        \"\"\"\n",
    "        # layers\n",
    "        prev_layers = []\n",
    "        # stem_conv\n",
    "        x = self.head(x)\n",
    "        # nas_layers\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.layers[i](x)\n",
    "            for j in range(i):\n",
    "                if self.skips[i][j]:\n",
    "                    x = x + prev_layers[j]\n",
    "            prev_layers.append(x)\n",
    "        # global_avgpool\n",
    "        x = self.tail(x)\n",
    "        x = global_avgpool(x)\n",
    "        # fc\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
